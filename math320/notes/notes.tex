\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{geometry}
\geometry{letterpaper, margin=2.0cm, includefoot, footskip=30pt}

\usepackage{fancyhdr}
\pagestyle{fancy}

\lhead{Math 320}
\chead{Notes}
\rhead{Nicholas Rees, 11848363}
\cfoot{Page \thepage}

\newtheorem*{problem}{Problem}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\theoremstyle{remark}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\ep}{{\varepsilon}}
\newcommand{\SR}{{\mathcal R}}

\renewcommand{\theenumi}{(\alph{enumi})}

\begin{document}
\section{September 20}
\subsection{Sequences and Limits}
A sequence in a given set $X$ is simply a function $x \colon \N \to X$.
It is often writen $x_n$ instead of $x(n)$, and list just the values
\[x = (x_1,x_2,x_3,\dots)\]
Order matters: the sequence is different from the set $\{x_1,x_2,\dots\}$,
e.g. $x_n = (-1)^{n+1}$ for $n \in \N$,
but $\{(-1)^{n+1} \colon n\in\N\} = \{-1,1\}$.

\begin{definition}[Converges to $\hat{x}$]
Given a sequence $(x_n)_{n\in\N}$ and a point $\hat{x}$,
all in $X = \R$, saying the sequence $(x_n)$ converges to $\hat{x}$ means
\[\forall \ep>0, \exists N \in \N \colon \forall n > N, \lvert x_n - \hat{x} \rvert < \ep\]
\end{definition}
You should very familiar with this definition.
"Not familiar like yeah I know them,
but familiar like your mom."

Notation: When this happens, write
\[\hat{x} = \lim_{n\to\infty} x_n\]
or $x_n \to \hat{x}$ as $n \to \infty$.

What is this definition saying?
Somebody walks in the door with some epislon,
some error tolerance beyond our control,
and we need to be ready to give an $N$ for anything,
typically depending on $\ep$.
$N$ depicts a point on the number line where stuff starts to go right,
all the other points are $\ep$ close to $\hat{x}$.
Close, because absolute value is how we measure distance.
\[\lvert x_n - \hat{x} \rvert < \ep
\iff -\ep < x_n - \hat{x} < \ep
\iff x_n \in (\hat{x}-\ep, \hat{x}+\ep)\]
"When I go to the tailor, I think of this.
For any ribbon width,
you have to cover all but finite terms ($N(\ep)$) of $x_n$
with your fabric."

In the world of computer science, you might care about efficiency of picking $N$,
but here, we don't, just need to find an $N$.
So technically, we can always find a prime number where after everything goes right.

\begin{definition}[Converges/Diverges]
A real-valued sequence $(x_n)_{n\in\N}$ {\it converges} when
\[\exists \hat{x} \in \R \colon \lim_{n\to\infty} x_n = \hat{x}\]
It {\it diverges} when it does not converge.
\end{definition}
$(x_n)$ diverges $\iff \forall \hat{x} \in \R, \exists \ep>0, \forall N\in\N
\colon \lvert x_n - \hat{x} \rvert \geq \ep$.
In other words,
every candidate for the limit $\hat{x}$,
comes with some tolerance
so that no matter how far to the right we start out ("political joke"),
a point further right will escape tolerance band.

If we want to reimagine this as story of closeness for vectors,
complex numbers, or something else with a distance,
the idea of a limit is the same (just have to reinterpret distance).
If you were wondering why we jumped over metric spaces in chapter 2,
this gives us some examples to motivate the metrics.

Some simple examples
\begin{enumerate}
\item $x_n = \frac{1}{n}$ conberges to $\hat{x} = 0$.
\begin{proof}
	Given $\ep > 0$, note $\frac{1}{\ep} \in \R$ and Archimedes
	says $\exists N \in \N$ such that $N>\frac{1}{\ep}$.
	Use that one: any $n > N$ will have $\lvert x_n - \hat{x}\rvert
	= \lvert \frac{1}{n} - 0\rvert = \frac{1}{n} < \frac{1}{N} < \ep$.
\end{proof}
\item $x_n = 1$ converges to $\hat{x} = 1$.
\begin{proof}
	Given $\ep > 0$, pick $N = 320$.
	Now every $n > N$ makes $\lvert x_n - \hat{x} \rvert = 0 < \ep$.
	This works... but many smaller choices for $N$ would be okay too.
\end{proof}
\end{enumerate}

Less simple examples:
\begin{enumerate}
\item $x_n = \frac{\sin{n}}{1+n+n^2+n^3+n^4+n^5}$ converges to $\hat{x} = 0$.
Preview: for every $n \in \N$,
\[\lvert x_n - \hat{x} \rvert = \frac{\lvert\sin{n}\rvert}{1+n+n^2+n^3+n^4+n^5}
< \frac{1}{0+n+0+0+0+0}\]
and $\frac{1}{n} < \ep$ whenever $n > \frac{1}{\ep}$.
So pick any integer $N \geq \frac{1}{\ep}$ and every $n>N$
will have $\frac{1}{n} < \ep$ and make $\lvert x_n - \hat{x}\rvert < \ep$.

Efficiency note: I chose the worst possible thing to outgrow $\sin{n}$ fast,
but it does not matter.
\end{enumerate}

\section{September 22}
Here is an example of a convergence proof,
as it would be written in homework / from the book:
the sequence $x = \frac{n^2 - 320n^{3/2}}{2n^2 - 80}$ converges to $\hat{x} = \frac12$.
(guessing the value for rationals, just look for dominating power).
\begin{proof}
	Given $\ep > 0$, choose integer $N \geq \max\{30,\left(\frac{750}{\ep}\right)^2\}$.
	This works. Indeed, every $n > N$ obeys $n > 30$ so $n^2 > 900$,
	giving
	\begin{equation}
		2n^2 - 801 = n^2 + (n^2 - 801) > n^2
	\end{equation}
	\begin{equation}
		\sqrt{n} > \frac{750}{\ep}
	\end{equation}
	Thus, if $n > N$,
	\begin{align*}
		\lvert x_n - \hat{x} \rvert
		&= \lvert \frac{n^2 - 320n^{3/2}}{2n^2 - 801} - \frac{1}{2} \rvert\\
		&= \lvert \frac{2(n^2 - 320n^{3/2}) - (2n^2 - 801)}{2(2n^2-801)}\rvert\\
		& \leq \frac{640n^{3/2} + 801}{2(2n^2 - 801)}\\
		& \leq \frac{640n^{3/2} + 801n^{3/2}}{2n^2}\\
		& < \frac{1500n^{3/2}}{2n^2}\\
		&= \frac{750}{\sqrt{n}} < \frac{750}{750/\ep} = \ep
	\end{align*}
\end{proof}
How do we come up with this?
We start with coming up with the limit.
Then, we try to make $\lvert x_n - \hat{x}\rvert$ small,
or a simpler version small (and then hopefully simpler thing is slightly larger).

Another example: $\lim_{n\to\infty} n^{1/n} = 1$ (from the book itself).
\begin{proof}
	Define $x_n = n^{1/n} - 1$; each $x_n > 0$.
	Recall $(1 + a) = 1 + na + \frac{n(n-1)}{2}a^2 + \cdots + na^{n-1} + a^n$ (fact of algebra).
	So $(1 + a)^n \geq \frac{n(n-1)}{2}a^2$ for all $a > 0$.
	Thus $n = (1 + x_n)^2 \geq \frac{n(n-1)}{2}x_n^2$.
	Insist $n \geq 2$ to get $x_n > 0$.
	Reorganize $0 < x_n^2 \leq \frac{2}{n(n-1)}n \iff x_n \leq \sqrt{\frac{2}{n-1}}$.
	Solve $\sqrt{2}{n-1} < \ep$ for $\frac{2}{n-1} < \ep^2 \iff \frac{2}{\ep^2} < n-1$.
	So choosing $N \geq \max\{2,1+\frac{2}{\ep^2}\}$ does the job.
\end{proof}
This kind of is more accurate what he might expect on homework,
folding the explanation job and the proof.

Let's look at a divergent example now:
$x_n = (-1)^n$.
\begin{proof}
	Fix any $\hat{x} \in \R$.
	Pick $\ep = 1$.
	Fix $N \in \N$.
	Now consider $n_e$ even, $n_o$ odd, with $n_e > N, n_o > N$.
	Then $x_{n_e} = (-1)^{n_e} = 1$ and $x_{n_e} = (-1)^{n_e}$.
	So $2 = |x_{n_2} - x_{n_o}| = |(x_{n_e} - \hat{x}) + (\hat{x} - x_{n_o})|
	\leq |x_{n_e} - \hat{x}| + |x_{n_o} - \hat{x}|$
	(Typical $\pm$ trick to ``uncancel").
	One of the terms on the RHS is $\geq 1$.
	One of $n=n_e$ or $n=n_o$ completes the logical statement above.
\end{proof}
\subsection{Theory / Properties}
(More in the notes)
\begin{theorem}[Squeeze Theorem]
	Let $(a_n),(x_n),(b_n$ be real-valued sequences, and $L \in \R$.
	Assume
	\begin{enumerate}
		\item $a_n \to L$
		\item $b_n \to L$
		\item $a_n \leq x_n \leq b_n, \exists N \in \N, \forall n>N$
	\end{enumerate}
	Then $x_n \to L$ as $n \to \infty$.
\end{theorem}
\begin{proof}
	Given $\ep > 0$, use (a) to get $N_a \in N$ such that
	\[
		|a_n - L| < \ep, \forall n > N_a
	\]
	So $L - \ep < a_n$.
	Use (b) to get $N_b \in \N$ such that
	\[
		|b_n -L| < \ep, \forall n > N_b
	\]
	So $b_n < L+\ep$.
	Use (c) to get $N_c \in \N$ such that
	\[
		a_n \leq x_n \leq b_n, \forall n > N_c
	\]
	Now if $N = \max\{N_a, N_b, N_c\}$,
	every $n > N$ does all three jobs:
	\[
		L - \ep < a_n \leq x_n \leq b_n < L + \ep
	\]
	Thus $|x_n - L| < \ep$.
\end{proof}

\section{September 27}
\subsection{Completeness}
Completeness is the property that makes $\R$ better than $\Q$.
There are three ways to look at it
(three properties $\R$ has $\Q$ doesn't, that kind of capture the same thing):
\subsubsection{Cauchy Sequences}
\begin{definition}
	A sequence $(x_n)$ is call {\it Cauchy} when
	\begin{enumerate}
		\item $\forall \ep > 0, \exists N \in \N$ such that
		$\forall m,n \geq N$, $|x_m - x_n| \leq \ep$
		(last bit is what makes different than limit),
		OR EQUIVALENT DEFINITION:
		\item $\forall \ep > 0$, $\exists N \in \N$ such that
			$\forall n \geq N$, $\forall p \in \N$, $|x_{n+p}-x_n| < \ep$
	\end{enumerate}
\end{definition}
\begin{proposition}
	Every convergent sequence is Cauchy.
\end{proposition}
\begin{proof}
	Pick one: let $(x_n)$ converge to $\hat{x}$.
	Estimate:
	\[
		|x_n - x_m| = |(x_n - \hat{x}) + (\hat{x} - x_m)|
		\leq |x_n - \hat{x}| +|x_m - \hat{x}|
	\]
	To get definition (a), let $\ep>0$ be given and use definition
	of $x_n \to \hat{x}$ with $\ep' = \frac{\ep}{2}$ to get $N \in \N$
	such that $|x_k - \hat{x}| < \ep'$ whenever $k > N$.
	This $N$ works in (a), since $m,n\geq\N \implies |x_m - x_n| < \ep'+\ep' = \ep$.
\end{proof}
\begin{corollary}
	Any sequence that is not Cauchy must diverge.
\end{corollary}
This is the contrapositive of the statement above.
This corollary is useful for proving a sequence diverges.

\begin{theorem}[Metric Completeness]
	Every Cauchy sequence converges (to a real limit) in $\R$.
\end{theorem}

\subsubsection{Bounded Sets}
\begin{theorem}[Order Completeness]
	Given any nonempty $S \subseteq \R$, let
	\begin{align*}
		A &= \{a\in\R \colon \forall x \in S, a \leq x\}\\
		B &= \{b\in\R \colon \forall x \in S, b \geq x\}
	\end{align*}
	Then
	\begin{enumerate}
		\item Either $A = \emptyset$ or $A = (-\infty, \alpha]$ for some $\alpha \in \R$
		\item Either $B = \emptyset$ or $B = [\beta,+\infty)$ for some $\beta \in \R$.
	\end{enumerate}
\end{theorem}
Terminology: Say $S$ is {\it bounded above} when $B \neq \emptyset$
and call each $b \in B$ an {\it upper bound for $S$}.
$S$ is {\it bounded below} when $A \neq \emptyset$;
each $a \in A$ is a {\it lower bound for $S$}.
{\it Bounded} means bounded above and bounded below.

You can see this property breaking with the rationals with $S = \{p \in \Q \colon p^2 < 2\}$.

When $B \neq \emptyset$, call $\beta$ the \emph{supremum} of $S$: $\beta = \sup(S)$.
Useful characterization:
\begin{enumerate}
	\item $\forall x \in S$, $x \leq \beta$ ($\beta$ is an upper bound for $S$)
	\item $\forall \gamma < \beta$, $\exists x \in S \colon \gamma < x$
		(nothing less than $\beta$ is an upper bound).
	\item Some would say $\beta$ is the Least Upper Bound for $S$.
\end{enumerate}

When $A \neq \emptyset$, $\alpha = \inf(S)$ is the \emph{infininum}
or the \emph{greatest lower bound} for $S$.

\subsubsection{Monotonic Sequences}
\begin{theorem}
	Given any sequence $(x_n)$ with
	\[
		x_1 \leq x_2 \leq x_3 \leq \cdots
	\]
	either $x_n \to \infty$ or $x_n$ converges to a real limit.
\end{theorem}
When we are saying goes to $\infty$, similar to $\ep-N$ (definition in Rudin).

We will see all of these theorem's are logically equivalent,
and then will prove one.

\subsubsection{Linkages}
(He's going fast to prove one of these theorems;
wants to give Zahl informed students in 321,
but also Zahl is teaching next class because Louwen is out of town)

These 3 viewpoints on completeness contain equivalent information.
Each one implies the others.
We will show Metric Completeness implies Order Completeness.
Zahl can do what he wants, but Louwen wants him to show the other two linkages.

\begin{theorem}
	Metric completeness (Cauchy sequences must converge)
	implies order completeness (if $S \neq \emptyset$ bounded above, $\sup(S)$ exists).
\end{theorem}
\begin{proof}
	Let $S \subseteq \R$ be nonempty;
	define $B = \{ b \in \R \colon \forall s \in S, s \leq b\}$.
	Assume $B \neq \emptyset$ and define sequence
	\[
		b_n = \min\left(B \cap \left\{\frac{k}{2^n} \colon k \in \Z\right\}\right)
	\]
	We will see this will be a Cauchy sequence;
	$\beta = \lim_{n\to\infty} b_n$ will have the properties defining $\sup(S)$.
	To show this, for each fixed $n$,
	\begin{enumerate}
		\item $b_n - \frac{1}{2_n} \not \in B \implies
			\exists s_n \in S \colon s_n > b_n - \frac{1}{2^n}$
		\item $b_{n+1} \leq b_n$ ($\min$ over a larger set of points)
		\item Using $b_{n+1} \in B$, $b_{n+1} \geq s_n$ for $s_n$ above.
			Using (b), $b_{n+1} \geq s_n > b_n - \frac{1}{2^n}
			\iff 0 \leq b_n - b_{n+1} < \frac{1}{2^n}$
	\end{enumerate}
	Now estimate
	\begin{align*}
		|b_{n+p} - b_n|
		&= b_n - b_{n+p}\\
		&= (b_n - b_{n+1} + (b_{n+1} - b_{n+2}) + \cdots + (b_{n+p-1} - b_{n+p})\\
		&< \frac{1}{2^n} + \frac{1}{2^{n+1}} + \cdots +\frac{1}{2^{n+p-1}} < \frac{2}{2^n}
	\end{align*}
	This is the key to showing $(b_n)$ is Cauchy,
	and then we use previous stuff to imply order.
	Can't finish within class, so check lecture notes.
\end{proof}

\section{September 29 (Zahl)}
\subsection{Subsequences}
We will look at subsequences today,
since will use it in the heart of a proof today.

Recall the definition of a sequence of real numbers
(but also works for general sequences).
A seqeunce of real numbers is a function $f \colon \N \to \R$
(or also notation $x\colon \N \to \R$),
so the sequence is $f(1),f(2),\dots$ (or $x(1), x(2),\dots$).
But we more often write this as $x_1,x_2,x_3,\dots$, ($x_n$).

A subsequence will be created by jumping over elements in a sequence.
The important thing is that the indices do need to strictly increase.

\begin{definition}[Subsequence]
	If $x \colon \N \to \R$ is a sequence,
	then a {\it subsequence} of $x$ is a sequence of the form
	\[
		x \circ g\colon \N \to \R
	\]
	where $g \colon \N \to \N$ is strictly increasing.
\end{definition}
But no one uses this notation in practice.
We more often write $x_{n_1}, x_{n_2}, \dots$, ($x_{n_k}$),
where $n_1 < n_2 < n_3 < \cdots$.
We can continually take subsequences,
and since naturals are infinte,
our new subsequences can be infinite in size.

\subsection{Completeness}
Recall the three slightly related notions of completeness.
These properties help distinguish the reals from the rationals,
or higher dimensional Euclidean space,
where some of these properties do not hold.
\begin{enumerate}
	\item Metric completeness property:
		every Cauchy sequence of real numbers converges.
		\begin{itemize}
			\item Recall that we showed that every convergent is Cauchy,
				and we are showing that the converse is true,
				but there exists other settings where this does not hold
		\end{itemize}
	\item Order completeness (Least Upper Bound property):
		If $S \subset \R$ is non-empty,
		we define the set $B$ to be the upper bounds of $S$:
		$B = \{b\in\R \colon \forall s \in S, s \leq b\}$.
		Then either $B = \emptyset$
		or $B = [\beta,\infty)$ for some $\beta \in \R$.
		\begin{itemize}
			\item We making use of the canonical ordering of $\R$,
				we don't have such an obvious ordering for other spaces
			\item If $S$ is any set that is unbounded above,
				then $B$ is empty.
			\item Switch to Greek letters ($\beta$) for historic reasons
				(same with Dedekind cuts)
			\item We know order is transitive, but this is
				entirely consistent with $B = (\beta, \infty)$.
				It is not hard to imagine a world where we don't get $\beta$:
				the set of positive numbers whose square is less than $2$.
				Can describe this entirely with the rationals,
				but there is no rational $\beta$ for this set,
				will always be able to find a smaller rational.
		\end{itemize}
	\item Monotone converge property:
		If $(x_n)$ is monotone increasing,
		either $x_n \to \infty$, or $(x_n)$ converges.
		(Could also have monotone decreasing and $-\infty$).
\end{enumerate}
\begin{theorem}
	Properties (a),(b),(c) are equivalent.
\end{theorem}
(Whenever setting up a result of this type,
make it $(a) \implies (b)$, $(b) \implies (c)$, $(c) \implies (a)$
(reorder) otherwise people will be confused).
We will eventually see that this holds for the reals, but not today.

We will prove that $(b) \implies (c)$ today.
\begin{proof}
	Let $(x_n)$ be a monotone increasing sequence.
	(Note on notation, we mean $x_1 \leq x_2 \leq \cdots$;
	some people will call this a weakly monotone increasing sequence,
	and with strict inequalities is strictly increasing).
	Let $S = \{x_n \colon n \in \N\}$ (the range of values taken by $x$).
	If $S$ is not bounded above,
	then we claim that $x_n \to \infty$.
	(At the level of this course,
	expect us to fill in the details;
	a little fill-in: we know monotone increasing and attains
	the full range above, $B = \emptyset$)
	Otherwise, the set of upper bounds $B$ is of the form
	$B = [\beta,\infty)$
	($B$ is not empty because $S$ is bounded above, the set of upper bounds).

	Want to show $x_n$ converges
	(the way to show convergence is just to come up with clever guess,
	and then show it converges to it).
	Lets show $x_n \to \beta$.
	We know $x_n \leq \beta$ for every $n$.
	Let $\ep>0$. Then $\exists x_n \in S$ such that $x_n > \beta - \ep$.
	(If this were not true, $\beta - \ep$ is an upper bound less than $\beta$.)
	So for all $n \geq N$, $x_n \geq x_N > \beta - \ep$.
	i.e. $|x_n - \beta| < \ep$.
	But then $x_n$ converges to $\beta$.
\end{proof}

We will now show that (c) $\implies$ (a).
This is complicated enough to break into three pieces;
each of these three pieces are useful for other places in the future
(so will call a Lemma).
\begin{lemma}
	Every sequence of real numbers (doesn't have to be real?)
	has a (weakly) monotone subsequence.
\end{lemma}
Won't try to prove, but let's understand.
Note that monotone here can both be for increasing or decreasing.
See that for $x_n = (-1)^nn$, we can find uncountably many monotone subsequences;
think the power set of the natural numbers.
But we just need the existence of a single monotone subsequence.
\begin{lemma}
	Every Cauchy sequence is bounded
\end{lemma}
Complete confidence can prove in a line or two.
Lemma 1 required some cleverness to prove, this is just definitions.
\begin{lemma}
	If $(x_n)$ is Cauchy, $(x_{n_k})$ is a subsequence,
	and $x_{n_k} \to \hat{x}$
	($\forall \ep > 0, \exists K$ such that $\forall k \geq K$, $|x_n-\hat{x} < \ep$).
	Then $x_n \to \hat{x}$.
\end{lemma}
We do need Cauchy: take the sequence $x_n = (-1)^n$.

Because every Cauchy sequence is bounded,
and we have a monotone increasing subsequence,
and we know it converges by monotone convergence property,
we have that the Cauchy sequence converges.

\section{October 4}
\subsection{Wrapping up Completeness Structure}
We will prove one of the theorem's later,
and will use the properties freely for now.

We will prove the third lemma from last class:
	If sequence $(x_n)$ is Cauchy, and some subsequence $(x_{n_k})$
	converges to $L \in \R$,
	then the full sequence obeys $\lim_{n\to\infty}x_n = L$.
\begin{proof}
	Key:
	\[
		|x_n - L| \leq |x_n - x_{n_k}| + |x_{n_k} - L|
	\]
	Let $\ep>0$ be given. Invent $\ep' = \frac{\ep}{2}$ and use Cauchy:
	\[
		\exists N\colon \forall m,n \geq N, |x_m - x_n| < \ep'
	\]
	Because the subsequence convergence gives $K \in \N$ so big that
	$\forall k > K$ makes
	\[
		|x_{n_k} - L| < \ep'
	\]
	Pick any $k^* > K$ (with $n_{k^*}$) and $n>N$ and use the first inequality from above:
	\[
		|x_n - L| < |x_n - x_{n_{k^*}}| + |x_{n_{k^*}} - L| < \ep' + \ep' = \ep
	\]
\end{proof}
The key thing is that we are going far enough down where our second term is under control,
and our first is under control by Cauchy.

Hunting License (called because each property
says there exists a real number that does stuff...):
it is ok to use completeness in any of these three forms to solve HW and exams.

\begin{corollary}[Bolzano-Weierstrass Theorem]
	Every bounded sequence of real numbers is
	gauranteed to have a convergent subsequence.
\end{corollary}

Some students showed him Tao,
and Tao uses the word $\ep$-steady to replace a sequence being Cauchy...
just a name that is useful to remember though, like Bolzano-Weierstrass.

"Completeness will let you down if you're a rational person,
but will ff if you're a real person."

\subsection{Supremum and Infinum}
Recall that the supremum is the least upper bound,
and the infinum is the greatest lower bound.
Giiven any set $S \subseteq \R$, we let
\[
	B = \{b \in \R \colon \forall x \in S, x \leq b\}
\]
What can happen?
\begin{itemize}
	\item $B = \emptyset$, if $S$ has no upper bound (e.g. $S = \Z$)
	\item $B = [\beta, +\infty)$, if $S$ has an upper bound (hence $\beta = \sup(S)$)
	\item $B = (-\infty,\infty)$, if $S = \emptyset$
\end{itemize}
We can extend the definition of ``sup" to cover all $3$ cases:
$\sup(S) = +\infty$ if $S$ has no upper bound and $\sup(S) = -\infty$ if $S = \emptyset$.

There is also a symmetric extension for inf:
$\inf(S) = -\infty$ if $S$ has no lower bound
and $\inf(S) = +\infty$ if $S = \emptyset$.

This also allows $\sup$ and $\inf$ to operate on sets that
includes $+\infty,-\infty$.

\subsubsection{Upper and Lower Limits}
\begin{definition}[Limit Superior]
	Given a real sequence $(x_n)$, define
	\[
		\limsup_{n\to\infty} x_n = \inf_{n\in\N}\left(\sup_{k\geq n} x_k\right)
	\]
\end{definition}
\begin{definition}[Limit Inferior]
	Given a real sequence $(x_n)$, define
	\[
		\liminf_{n\to\infty} x_n = \sup_{n\in\N}\left(\inf_{k\geq n} x_k\right)
	\]
\end{definition}
Halloween joke: ``limb soup" is pronounciation of the first.
"I'm not playing the age card, but I got dad jokes plus!"
\begin{itemize}
	\item $x_n = \frac{1}{n}$, then
		\[
			\limsup_{n\to\infty} \frac{1}{n} =
			\inf_{n\in\N}\left(\sup\{\frac{1}{k} \colon k \geq n\}\right)
			= \inf_{n\in\N} \{\frac{1}{n}\} = 0
		\]
		If you have a convergent sequence to start with,
		limsup is going to match your idea what the limit is.
	\item $x_n = (-1)^n + \frac{1}{n}$, then
		\begin{align*}
			\limsup_{k\to\infty} (x_n)
			&= \inf_n \sup\{(-1)^k + \frac{1}{k} \colon k \geq n\}\\
			&= \inf_n \{1+2j \text{ if }k=2j \text{ even, OR}
			-1+\frac{1}{2j} \text{ if }k=2j-1 \text{ odd}\}\\
			&= \inf_n\{1+\frac{1}{\lceil n/2 \rceil}\}\\
			&= 1
		\end{align*}
		or something like that... fix at home.
		\begin{align*}
			\liminf_{n\to\infty}(x_n)
			&= \sup_n[\inf_{k\geq n} (-1)^k + \frac{1}{k}]\\
			&= \sup_n\{-1\}\\
			&= -1
		\end{align*}
		This thing is bouncing around $1$ and $-1$ with a negligiblle pertrubation.
		We can also tell that it has two subsequences that do converge
		($n$ even will converge to $1$ and $n$ odd will converge to $-1$).
		This is what is being captured with $\limsup$ and $\liminf$.
\end{itemize}

\section{October 6}
\subsection{Upper and Lower Limits (continued)}
Recall, for any real sequeunce, $x = (x_n)$,
\[
	\overline{\lim_{n\to\infty}} = \limsup_{n \to \infty} x_n = \inf_{n\in\N}\left(\sup_{k\geq n}\right)
\]
\[
	\underline{\lim_{n\to\infty}} = \liminf_{n \to \infty} x_n = \sup_{n\in\N}\left(\inf_{k\geq n}\right)
\]
gives values in $\R \cup \{\pm\infty\}$
ff missing proposition.

\begin{proof}
	For each $n \in \N$, consider ``tail",
	\[
		T_n = \{x_n,x_{n+1},x_{n+2}, \dots \}
	\]
	and let $i_n = \inf(T_n), s_n = \sup(T_n)$.
	Clearly $i_n \leq s_n$, and $i_n \leq i_{n+1}$, $s_n \geq s_{n+1}$.
	For any $m,n\in\N$, pick any $N> \max\{\}$.
	ehh... I showed up late, now I'm lost
\end{proof}
``Don't look at a Cauchy sequence after 9pm... I lost sleep."


\begin{proposition}
	Let $x = (x_n)$ be a real sequence.
	Define $\mu = \liminf_{n\to\infty} x_n$, $M = \limsup_{n\to\infty} x_n$.
	\begin{enumerate}
		\item If $l = \lim_{k\to\infty}x_{n_k}$ for some subsequence $(x_{n_k})$ of $x$,
			then $\mu \leq l \leq M$.
		\item There exist subsequences $(x_{n_j})$ and $(x_{n_k})$
			of $x$ along which $\mu = \lim_{j\to\infty} x_{n_j}$ and $M = \lim_{k\to\infty} x_{n_k}$.
	\end{enumerate}
\end{proposition}
This is actually how Rudin defines the $\limsup$ and $\liminf$.
But we will show the two defintions are equivalent.

Read notes for proof.

Next time: we are doing half the job of constructing the reals. The other half is on HW5.

\section{October 11}
\subsection{Homework Tips}
Make $\liminf_{n\to\infty}x_n$ and $\liminf_{n\to\infty}x_n$ your main tools.
Just writing $\lim_{n\to\infty} x_n$ requires prior work to show it exists.
But $\limsup$ $\liminf$ always take on a value in $\R \cup\{\pm \infty\}$.
And exactly when $\limsup_{n\to\infty}x_n = \liminf_{n\to\infty} x_n$
(then they are equal).

\begin{lemma}
	If real sequences $(x_n),(y_n)$ obey
	$\forall N \in \N \colon x_n \leq y_n, \forall n \geq N,$
	then
	\[
		\liminf_{n\to\infty}x_n \leq \liminf_{n\to\infty}y_n
	\]
	\[
		\limsup_{n\to\infty}x_n \leq \limsup_{n\to\infty}y_n
	\]
\end{lemma}
Proof in Canvas notes.
[But note: knowing $x_n < y_n$ is NOT ENOUGH to get ``$<$" in results above,
e.g. $x_n = -\frac{1}{n}$, $y_n = 0$.]
We can use this to supercharge Squeeze Theorem ideas.
Imagine trying to show $\lim_{n\to\infty} x_n = L$
for a given sequence $x_n$, some $L \in \R$.
It would suffice to show
\[
	L \leq \liminf_{n\to\infty} x_n \leq \limsup_{n\to\infty}x_n \leq L
\]
(the middle inequality is by definition).
This is equivalent to
\[
	\forall \ep > 0, L-\ep \leq \liminf_{n\to\infty} x_n \leq \limsup_{n\to\infty}x_n \leq L + \ep\quad (**)
\]

With squeeze idea,
can cook up your own sequences $a_n \to L$ and $b_n \to L$ and show
\[
	\forall \ep > 0, \exists N \in \N \colon a_n -\ep \leq x_n \leq b_n + \ep, \forall n \geq N
\]
If you can arrange this inequality,
say ``fix $\ep > 0$,
take $\limsup$,$\liminf$ on $n$ to get $(**)$.
Works for each $\ep > 0$, so done."
Loewen was making homework problems,
and when he was doing solves to make sure it was possible,
this trick came up again and again to show something about the $\lim_{n\to\infty}x_n$.

\subsection{Constructing $\R$}
Before next class, make sure to have read the homework problems.

We want some mathematical structure that does what we want the real numbers to do.
We propose sets of Cauchy sequeunces.
We will tell when they're equal, how to do arithmetic on them,
when one is bigger than the other,
and they get us the least upper bound property.
Will probably take us all of today and tomorrow.

\subsubsection{Notation}
$CS(\Q)\colon$ the set of all Cauchy sequences with entries in $\Q$\newline
$x,y,z\colon$ typical sequence names, e.g., $x = (x_1,x_2,\dots)$.\newline
$R[x] = \left\{x' \in CS(\Q) \colon
\lim_{n\to\infty}\lvert x'_n - x_n \rvert = 0\right\}$.\newline
$\mathcal{R} = \{R[x] \colon x \in CS(\Q)\}$, our model for $\R$\newline
$\Phi \colon \Q \to \mathcal{R}$, defined by $\Phi(q) = R[(q,q,\dots)]$

\subsubsection{Equality}
For $x,x' \in CS(\Q)$ define a relation ``$\sim$" by
$x' \sim x \iff \lim_{n\to\infty}|x'_n - x_n| = 0$.
This is an ``equivalence relation" (RST)
and the sets $R[x]$ are its the equivalence classes.

\subsubsection{Addition}
We already did a homework that we handed in last weekend,
where we could extend the idea of adding two Cauchy sequences.
For $x,y \in CS(\Q)$, we defined
\[
	x + y = (x_1 + y_1, x_2 + y_2, \dots)
\]
The result is in $CS(\Q)$ by HW04, Question 6(a).

Now boost the definition to $\mathcal{R}$, hoping for
\[
	R[x] + R[y] = R[x+y], \quad x,y\in CS(\Q)
\]
\begin{proposition}
	This ``$+$" is well-defined (between Cauchy sequences??).
	That is, if $x,x',y,y' \in CS(\Q)$ obey $R[x]=R[x'], R[y]=R[y']$,
	then
	\[
		R[x+y] = R[x'+y']
	\]
\end{proposition}
\begin{proof}
	With $x,x',y,y'$ a above,
	pick any $z' \in R[x'+y']$.
	To show $z' \in R[x+y]$, use the properties
	\begin{align*}
		z' \in R[x'+y'] &\iff z'_n-(x'_n+y'_n) \to 0\\
		x' \in R[x'] = R[x] &\iff x'_n - x_n \to 0\\
		y' \in R[y'] = R[y] &\iff y'_n - y_n \to 0\\
	\end{align*}
	thus $z'_n - (x_n + y_n) = z'_n - (x'_n+y'_n) + (x'_n-x_n) + (y'_n-y_n)$,
	So (sum rule for limites) $z'_n - (x_n + y_n) \to 0$,
	i.e. $z' \in R[x+y]$.
	But $z'$ ff photo
\end{proof}

\section{October 12}
\subsection{Constructing Reals (cont.)}
\subsubsection{Addition (cont.)}
\begin{proposition}
	$(\mathcal{R},+)$ is an Abelian group, i.e. $\forall x,y,z \in CS(\Q)$
	\begin{enumerate}
		\item $\mathcal{R}[x] + \mathcal{R}[y]$ is a well-defineed element of $\mathcal{R}$
		\item $\mathcal{R}[x] + \mathcal{R}[y] = \mathcal{R}[y] + \mathcal{R}[x]$
		\item $(\mathcal{R}[x] + \mathcal{R}[y]) + \mathcal{R}[z]
			= \mathcal{R}[x] + (\mathcal{R}[y]) + \mathcal{R}[z])$
		\item $\mathcal{R}[x] + \Phi(0) = \mathcal{R}[x]$
		\item $\mathcal{R}$ contains an element ``$-\mathcal{R}[x]$"
			satisfying $\mathcal{R}[x] + (-\mathcal{R}[x]) = \Phi(0)$
	\end{enumerate}
\end{proposition}
\begin{proof}
	\begin{enumerate}
		\item Did yesterday
		\item Follows from commutativity of rationals
		\item Follows from associativity of rationals
		\item Adding $0$ to each term does not change the sequence.
			(Uniqueness of identity... homework
			$\mathcal{R}[(1,\frac{1}{2},\frac{1}{3},\dots)] = \Phi(0)$)
		\item Given $x = (x_1,x_2,\dots)$.
			Define $-x = (-x_1,-x_2,\dots) \in CS(\Q)$
			and note $\mathcal{R}[x] + \mathcal{R}[-x] =
			\mathcal{R}[x+(-x)] = \mathcal{R}[(0,0,\dots)] = \Phi(0)$.
	\end{enumerate}
\end{proof}

\subsubsection{Multiplication}
Extend ``$\cdot$" to $\mathcal{R}$ by lifting ``$\cdot$" defined in $CS(\Q)$.
\[
	\mathcal{R}\cdot\mathcal{R}[y] := \mathcal{R}[x\cdot y] =
	\mathcal{R}[(x_1y_1, x_2y_2, \dots)]
\]
Recall HW04: $x\cdot y$ truly belongs to $CS(\Q)$.
\begin{proposition}
	This $\cdot$ is well-defined.
	I.e., if $x,x',y,y' \in CS(\Q)$ with $\mathcal{R}[x] = \mathcal{R}[x']$
	and $\mathcal{R}[y] = \mathcal{R}[y']$ then
	$R[x] \cdot \mathcal{R}[y] = R[x'] \cdot \mathcal{R}[y']$.
\end{proposition}
Will actually do a better proof than we did with addition...
Loewen is going to update Canvas notes to make addition look like this too.
\begin{proof}
	Given $x,x',y,y'$ as in setup,
	$x' \in \mathcal{R}[x]$ and $y' \in \mathcal{R}[y]$,
	give $x'_n - x_n \to 0$ and $y'_n - y_n \to 0$.
	Now rearrange (add and subtract trick)
	\[
		x'_ny'_n - x_ny_n = [(x'_n-x_n) + x_n]y'_n - x_ny_n
		= (x'_n - x_n)y'_n + x_n(y'_n - y_n)
	\]
	Every Cauchy sequence is bounded, so $\exists M_0,M$ such that
	\[
		|x'_ny'_n - x_ny_n| \leq M_0|x'_n - x_n| + M_1|y'_n - y_n|
	\]
	Squeeze theorem $\implies$ LHS $\to 0$, i.e., $x'\cdot y' \in \mathcal{R}[x\cdot y]$.
	But $x'\cdot y' \in \mathcal{R}[x' \cdot y']$.
	A non-empty intersection implies equality (HW05, Q3...).
\end{proof}
(Make sure to know add and subtract trick,
multiply and divide trick,
head and tail trick... and something else?)

\begin{proposition}
	$(\mathcal{R}\setminus \Phi(0), \cdot)$ is an Abelian group
	(same as with addition, but identity is $\Phi(1)$,
	and denote inverse as ``$1/\mathcal{R}[x]$").
\end{proposition}
\begin{proof}
	First four axioms done easy same as before.
	Inverse axiom is done in HW05, Q6.
\end{proof}

\subsubsection{Distribution}
\begin{proposition}
	Given any $a,b,c \in CS(\Q)$
	\[
		\mathcal{R}[a]\cdot(\mathcal{R}[b]+\mathcal{R}[c])
		=(\mathcal{R}[a] \cdot \mathcal{R}[b]) +
		(\mathcal{R}[a] \cdot \mathcal{R}[c])
	\]
\end{proposition}
\begin{proof}
	Expand definitions on both sides:
	\[
		LHS = \mathcal{R}[a]\cdot\mathcal{R}[b+c] = \mathcal{R}[a\cdot(b+c)]
	\]
	\[
		RHS = \mathcal{R}[a\cdot b] + \mathcal{R}[a\cdot c]
		= \mathcal{R}[a\cdot b + a\cdot c]
	\]
	Inside Cauchy sequences have $n$th terms.
	\begin{align*}
		[a\cdot(b+c)]_n &= a_n(b_n+c_n)\\
		[a\cdot b + a\cdot c]_n &= a_nb_n + a_nc_n
	\end{align*}
	Each term is the same, so both the LHS and RHS share
	this representative sequence, thus they must be equal.
\end{proof}

This fulfills the axioms for the mathematical object we call a field.
We now want to show $\mathcal{R}$ is an ordered field.

\subsubsection{Order}
\begin{definition}
	Given $x,y \in CS(\Q)$, define $\mathcal{R}[x] < \mathcal{R}[y]$
	when $\exists r > 0 (r \in \Q), \exists N \in \N$ such that
	\[
		x_n + r < y_n, \forall n \geq N
	\]
\end{definition}
\begin{proposition}
	This definition for ``$<$" is unambiguous.
	(i.e., independent of representatives selected from $\mathcal{R}[x], \mathcal{R}[y]$).
	and
	\begin{enumerate}
		\item Every $x \in CS(\Q)$ obeys exactly one of:
			$\mathcal{R}[x] < \Phi(0)$ or $\mathcal{R}[x] > \Phi(0)$
			$\mathcal{R}[x] = \Phi(0)$
		\item Transitivity of order ff
	\end{enumerate}
\end{proposition}
\begin{proof}
	HW05
\end{proof}

\section{October 13}
\begin{proposition}[Order Components -- OC]
	Let $a,b \in CS(\Q)$.
	\begin{enumerate}
		\item If $\SR[a] > \SR[b]$, then $\exists N \in\N$ such that
			\[
				\SR[a] > \Phi(b_N)
			\]
		\item If $\SR[a] \leq \Phi[b_k]$ for all $k \in \N$, then
			\[
				\SR[a] \leq \SR[b]
			\]
	\end{enumerate}
\end{proposition}
\begin{proof}
	Note (b) $\iff$ (a) by contraposition, so we just prove (a).
	Given $\SR[a] > \SR[b]$, $\exists N_0 \in \N$ and $r>0$ ($r \in \Q$)
	such that $a_n > b_n + r$ for all $n\geq N_0$.
	Also, $a,b \in CS(\Q)$ give $N_a,N_b \in \N$ such that
	$|a_m - a_n| < r/10$ for $m,n \geq N_a$ and
	$|b_m - b_n| < r/10$ for $m,n \geq N_b$.
	Let $N = \max\{N_0,N_a,N_b\}$.
	For any $m \geq N$,
	\begin{align*}
		b_N &= b_m + (b_N - b_m)\\
			&\leq b_m + \frac{r}{10}\\
			&\leq (a_m - r) + \frac{r}{10}\\
			&= a_m - \frac{9}{10} < a_m -\frac{1}{2}r
	\end{align*}
	Thus $b_N < a_m - \frac{1}{2}r$ $\forall m \geq N$,
	so we are done.
\end{proof}
Hmm... the Cauchy property of $a$ never got used in the proof.
This is mildly interesting, we could throw out some of the stuff we wrote down.
Why is it important than? Probably because it is baked into the definition
of the inequality in this context.

\subsection{Completeness of $\SR$}
Notation shift: use $\alpha = \SR[a]$, $\beta = \SR[b]$, and $\gamma = \SR[c]$.

Reflection:
We are now at the base camp of Everest, ready for our final ascent.
$\Phi \colon \Q \to \SR$ embeds a ``working copy of $\Q$" in $\SR$.
We confirmed $\Phi(p+q) = \Phi(p) + \Phi(q)$, $\Phi(p\cdot q) = \Phi(p)\cdot\Phi(q)$,
$p < q \iff \Phi(p) < \Phi(q)$;
everything we want from the rationals is mirrored in $CS(\Q)$.
Now, realize that there are some equivalence classes of
Cauchy sequences not in here:
$\SR$ includes many elements not of the form $\Phi(q), q\in\Q$.
E.g., here's a membr of $SR$: $\pi = R[(3,3.1,3.141,3.1415,\dots)]$
is outside $\Phi(\Q)$.

``Outside the Sauder school of business... don't get me started, but outside Sauder,
professors only wear ties on special occassions.
Today is a special occasion." [he is wearing a tie]
\begin{theorem}[Completeness of $\SR$]
	Let $A$ be a nonempty subset of $\SR$ with an upper bound,
	i.e., $\exists \mu \in \SR$ such that $\forall \alpha \in A$, $\alpha \leq \mu$.
	then there exists $\beta \in \SR$ such that
	\begin{enumerate}
		\item $\forall \alpha \in A$, $\alpha \leq \beta$
			($\beta$ is \emph{an} upper bound)
		\item $\forall \gamma \in \SR$ with $\gamma < \beta$,
			$\exists \alpha \in A$ such that $\gamma < \alpha$.
			($\beta$ is the \emph{least} upper bound)
	\end{enumerate}
\end{theorem}
\begin{proof}
	For each $n \in \N$, define $b_n = \min(S_n)$ where
	$S_n = \{\frac{k}{2^n} \colon k \in \Z, \forall \alpha \in A,
	\Phi(\frac{k}{2^n}) \geq \alpha\}$.
	By hypothesis, $\mu$ is an upper bound for $A$.
	And HW05 gives some integer $K$ such that $\mu < \Phi(K)$.
	Every $\frac{k}{2^n} \geq K$ will belong to $S_n$.
	Also (exercise) $S_n$ has a lower bound.
	Also, for each $n$, $S_{n+1} \supseteq S_n$.
	So $b_n \geq b_{n+1} \geq b_n - \frac{1}{2^{n+1}}$,
	or $0 \leq b_n - b_{n+1} \leq \frac{1}{2^{n+1}}$.
	So if $p \in \N$, $0 \leq b_n - b_{n+p} \leq (b_n - b_{n+1})
	+ \cdots + (b_{n+p-1} - b_{n+p})$.
	So
	\begin{align*}
		0 &\leq b_n - b_{n+p}\\
		  &\leq \frac{1}{2^{n+1}} + \frac{1}{2^{n+2}} + \cdots + \frac{1}{2^{n-p}}\\
		  &\leq \frac{1}{2^n}\left(\frac12 + \frac14 + \cdots + \frac{1}{2^p}\right)\\
		  &< \frac{1}{2^n}
	\end{align*}
	Hence $b = (b_n)$ is a Cauchy sequence.
	Let $\beta = \SR[b]$.
	From the definition of $b_n \in S_n$, $\Phi(b_n) \geq \alpha, \forall \alpha \in A$.
	From OC (b) above,
	$\beta \geq \alpha, \forall \alpha \in A$.
	That is conclusion (a).

	For (b), let $\gamma < \beta$ and say $\gamma = \SR[c]$.
	This comes with $r>0 (r\in\Q), N \in \N$ where for all $n \geq N$,
	\[
		c_n + r < b_n
	\]
	Increase $N$ if needed to get $\frac{1}{2^N} < \frac{r}{2}$.
	Then
	\[
		c_n + \frac{r}{2} = (c_n + r) - \frac{r}{2} < b_n - \frac{r}{2}
		< b_n - \frac{1}{2^N} \leq b_N - \frac{1}{2^N}
	\]
	(where the last step is because $b_n \geq b_{n+1}$).
	Thus $\Phi(c) < \Phi(b_N)$.

	ff he forgot about $\alpha$... check Canvas.
\end{proof}
Thus every monotone bounded sequence of these converge.
Every Cauchy sequence of these converges.

Archimedean property follows (in Canvas notes).

\section{October 25}
Recall we were trying to prove
\begin{proposition}[Cauchy Condensation Test]
	For $a_n \geq a_{n+1} \geq 0$, the following are equivalent:
	\begin{enumerate}
		\item $S = \sum_{n=1}^\infty a_n < + \infty$
		\item $T = \sum_{k=0}^\infty 2^ka_{2^k} < + \infty$
	\end{enumerate}
\end{proposition}
Last class, proved (b) $\implies$ (a). Now we prove the other direction.
\begin{proof}
	If $S$ converges,
	consider a partial sum for $T$:
	\begin{align*}
		t_n
		&= \sum_{k=0}^n 2^ka_{2^k}\\
		&= a_1 + 2a_2 + 4a_4 + \cdots + 2^na_{2^n}\\
		&= 2\left[\frac{1}{2}a_1 + a_2 + 2a_4 + \cdots + 2^{n-1}a_{2^n}\right]\\
		&\leq 2\left[a_1 + a_2 + (a_3 + a_4) + (a_5 + a_6 + a_7 + a_8)
		+ \cdots + (a^{2^{n-1}+1} + \cdots + a_{2^n})\right]\\
		&\leq 2s_{2^n} \leq 2S
	\end{align*}
	Partial sums for $T$ are bounded $\implies T < + \infty$.
\end{proof}
\subsection{$p$-series}
For fixed $p$, let
\[
	\zeta(p) = \sum_{n=1}^\infty \frac{1}{n^p}
\]
(Related to the $\zeta$ function from Riemann Hypothesis,
but that's over complex values.)
Which $p$ makes this converge?
Reject all $p \leq 0$: crude test.
For $p > 0$, summand decreases,
so Cauchy Condensation Test (CCT) applies,
so consider $\sum_{k = 0}^\infty \frac{2^k}{(2^k)^p}
= \sum_{k=0}^\infty \frac{1}{(2^{p-1})^k}$.
This is a gemotric series with ratio $r = 1/(2^{p-1})$.
So the series converges if and only if $r < 1$, or $p > 1$.

Digression: This is a glimpse through a keyhoel to a big room of mathematics.
If you extended $\zeta(p)$ to complex inputs $p$,
this will converge when the real part of $\mathrm{Re}(p)$...
then extended to more $p \in \C$.
We have $\sum_{k=1}^\infty p^k = \frac{1}{1-p}$ when $|p|<1$:
we have a function defined through a series,
but has an alternative form and can define the convergence to that form.
He admittedly doesn't know a lot about the subject, but would like to point out.

Note that this is new information:
the ratio and root test will not help with $p$-series.
The series converges to slowly to detect geometrically,
and ratio and root test are not sharp enough to detect it.
Indeed, $\alpha = \limsup_{n\to \infty}|a_n|^{1/n} = \limsup_{n\to\infty}\frac{1}{(n^p)^{1/n}}
= \limsup_{n\to\infty} \frac{1}{(n^{1/n})^p} = 1$.
And we already talked about how ratio test is even coarser than
the ratio test, so it also lets us down (but have worked through the details on canvas).

\subsection{Kummer's Test}
(From TBB book, pp 115.)
\begin{theorem}
	Consider $S = \sum_{n=1}^\infty a_n$,
	with all $a_n > 0$.
	Let $(D_n)$ be a sequence with $D_n > 0, \forall n$.
	Define
	\[
		\underline{L} = \liminf_{k\to\infty} \frac{D_ka_k - D_{k+1}a_{k+1}}{a_{k+1}}
	\]
	\[
		\overline{L} = \limsup_{k\to\infty} \frac{D_ka_k - D_{k+1}a_{k+1}}{a_{k+1}}
	\]
	\begin{enumerate}
		\item If $\underline{L} > 0$, then $S$ converges.
		\item If $\overline{L} < 0$ and $\sum_{n=1}^\infty \frac{1}{D_n} = +\infty$
			then $S$ diverges.
	\end{enumerate}
\end{theorem}
\begin{proof}
\begin{enumerate}
	\item If $\underline{L} > 0$, then pick $r \in (o,\underline{L})$.
		By definition of $\liminf$, $\exists N \in \N$ such that
		$\forall k \geq N$, $r < \frac{D_ka_k - D_{k+1}a_{k+1}}{a_{k+1}}$,
		i.e. $ra_{k+1} < D_ka_k - D_{k+1}a_{k+1}$.
		Telescope these:
		\begin{align*}
			ra_{N+1} &< D_Na_N - D_{N+1}a_{N+1}\\
			ra_{N+2} &< D_{N+1}a_{N+1} - D_{N+2}a_{N+2}\\
					 &\vdots\\
			ra_{N+p} &< D_{N+p-1}a_{N+p-1} - D_{N+p}a_{N+p}\\
		\end{align*}
		Thus $r\left[a_{N+1} + \cdots + a_{N+p}\right] < D_Na_N - D_{N+p}a_{N+p} < D_Na_N$.
		Since $N$ fixed and $p \in \N$ arbtrary,
		partial sums for $\sum_n a_n$ are bounded, so that series converges.
	\item See Canvas notes (similar ideas)
\end{enumerate}
\end{proof}
``I love WebWork [sarcastically]. I wrote some of those questions,
and some of them really lit up Piazza."

Example: Take constant $D_n = 1$ in Kummer's Test:
\[
	\underline{L} = \liminf_{k \to \infty} \frac{a_k - a_{k+1}}{a_{k+1}}
	= \liminf \left(\frac{a_k}{a_{k+1}} - 1\right)
\]
\[
	\overline{L} = \limsup \left(\frac{a_k}{a_{k+1}} - 1\right)
	= \left( \frac{1}{\liminf\left(\frac{a_{k+1}}{a_k}\right)} - 1\right)
\]
as we have shown on some homework.
So $\underline{L} = \frac{1}{\overline{\alpha}} - 1$,
$\overline{L} = \frac{1}{\underline{\alpha}} - 1$,
and we see convergence if $\overline{alpha} < 1$ and diverges if $\underline{\alpha} > 1$...
that's the ratio test!
So Kummer's test covers the ratio test.

Example: Take $D_n = n$ in Kummer's test.
(Note $\sum\frac{1}{D_n} = \sum{1}{n} = +\infty$.)
Hence
\begin{align*}
	\frac{D_ka_k - D_{k+1}a_{k+1}}{a_{k+1}}
	&= \frac{ka_k - (k+1)a_{k+1}}{a_{k+1}}\\
	&= \frac{k(a_k - a_{k+1}) - a_{k+1}}{a_{k+1}}\\
	&= k\left(\frac{a_k}{a_{k+1}} - 1\right) - 1
\end{align*}
Testing limits of this leads to the refined test named for Raabe.
Details on Canvas notes and in HW07.

\section{October 27}
``Let's give series one last shot." (Yoooo, last series lecture).
\subsection{Alternating Series}
Most of the series we've been looking at have had all
positive terms, now we have ones that include negatives.
\begin{theorem}
	If $S = \sum_{n=0}^\infty (-1)^n a_n$ and
	\begin{enumerate}
		\item $a_0 \geq a_1 \geq a_2 \geq \cdots$
		\item $\lim_{k \to \infty} a_k = 0$
	\end{enumerate}
	Then $S$ converges.
\end{theorem}
\begin{proof}
	Let $s_n = \sum_{k=0}^n (-1)^k a_k$ for $n \geq 0$.
	Can envision this as the partial sums going back and forth,
	but shrinking in magnitude:
	\[
		s_1 \leq s_3 \leq s_5 \leq \cdots \leq s_6 \leq s_4 \leq s_2 \leq s_0
	\]
	Note that the odd partial sums form a monotonically increasing sequence,
	and the even partial sums form a monotonically decreasing sequence,
	and clearly both sequences are bounded.
	Thus, they both converge.
	But
	\[
		0 \leq s_{2n} - s_{2n+1} = a_{2n+1}
	\]
	which has limmit $0$,
	so Squeeze theorem (with $a_k \to 0$) shows both have the same limit.
\end{proof}
Note from Math 101:
any partial sum gives a lower or upper bound on the final value
that $S$ converges to (depending on if even or odd);
strategy for calculation (but we're not in this game for that).

\subsection{Summation by Parts}
``We love our add and subtract trick, we plan to use it for HW07 Q4.
We love our telescoping series.
What if we did both?"

Consider $\sum_{k=0}^n A_k b_k$.
Define $A'_k = A_k - A_{k-1}$, $B_n = b_0 + b_1 +\cdots + b_n$,
and $b_k = B'_k = B_k - B_{k-1}$.
Now
\begin{align*}
	\sum_{k=0}^n A_k b_k
	&= \sum_{k=0}^n A_k B'_k\\
	&= A_0b_0 + A_1b_1 + A_2b_2 + \cdots + A_nb_n\\
	&= A_0B_0 + A_1(B_1 - B_0) + A_2(B_2 - B_1) + \cdots + A_n(B_n - B_{n-1})\\
	&= (A_0 - A_1)B_0 + (A_1 - A_2)B_1 + \cdots + (A_{n-1} - A_n)B_{n-1} + A_nB_n\\
	&= (-A'_1)B_0 + (-A'_2)B_1 + \cdots + (-A'_n)B_{n-1} + A_nB_n\\
	&= A_nB_n - \sum_{k=1}^n A'_kB_{k-1}
\end{align*}
Now recall our analogue: $\int_a^b uv' dx = uv\vert_a^b - \int_a^bu'vdx$...
looks similar enough to deserve the name.
Our work will be useful in proving the following theorem.
\begin{theorem}[Dirichlet's Test]
	Consider $S = \sum_{n=0}^\infty a_nb_n$. If
	\begin{enumerate}
		\item $a_n \geq a_{n+1}$ for all $n$, and $a_n \to 0$ as $n \to \infty$.
		\item $B_n = b_0 + b_1 + \cdots + b_n$ form a bounded sequence
	\end{enumerate}
	Then $S$ converges also.
\end{theorem}
\begin{remark}
	If $b_n = (-1)^n$, this will give us the alternating series test.
\end{remark}
\begin{proof}
	Use $A_k = a_k$ in the summation by parts formula.
	Look at the partial sums, so
	\[
		S_n =  \sum_{k=0}^n a_kb_k
		= a_nb_n - \sum_{k=1}^n\underbrace{(a_k - a_{k-1})}_{a'_k}B_{k-1}
	\]
	Both the right hand side pieces converge as $n \to \infty$.
	Prove this using assumptiion (b) first.
	Let $C = \sup_k |B_k|$... a number.
	Then $|a_nB_n| \leq C|a_n| \to 0$ by (a).
	For the second piece, use monotonicity
	\[
		\sum_{k=1}^n \left\lvert \left( a_k - a_{k-1}\right)B_{k-1}\right\rvert
		\leq C \sum_{k=1}^n\left(a_{k-1} - a_k\right)
		= C(a_0 - a_n) \leq Ca_0
	\]
	where the equality is because this is a telescoping series.
	So the series $\sum_{k=1}^\infty(a_k - a_{k-1})B_{k-1}$ converges absolutely,
	so it must converge.
\end{proof}
I asked if we could modify hypothesis to say for any monotone sequence;
he says it's the same, we just multiply $a_n$ by a sign,
and Dirichlet's still applies.

Will try to come up with a question on HW08 that makes Dirichlet's
test better than alternating series.
Mostly, using convergence tests is a homework activity
(people were complaining about memorizing all these tests);
proving these tests can show up on exams.

\subsection{Absolute Convergence vs Conditional Convergence}
Recall: if $\underbrace{\sum_{n=1}^\infty |a_n| \leq +\infty}_{
\text{absolute convergence for }\sum a_n}$,
then $\sum_{n=1}^\infty a_n$ converges
(and we say it is absolutely convergent).
The converse is not true.
The alternating series test shows $\sum_{n=1}^\infty \frac{(-1)^n}{n}$ converges,
yet $\sum_{n=1}^\infty \left\lvert\frac{(-1)^n}{n}\right\rvert$
is known to divierge.
Any series where $\sum a_n$ converges but $\sum|a_n| = +\infty$
are called conditionally convergent.

\subsubsection{Rearrangement}
Re-ordering terms is okay for absolutely convergent series,
but strange for conditionally convergent ones, e.g.
let $S = \sum\frac{(-1)^{n+1}}{n} = 1 - \frac{1}{2} + \frac{1}{3} -+ \cdots$.
Build $\tilde{S}$ using the same pieces, but shuffle the order:
\begin{align*}
	\tilde{S}
	&= 1 - \frac{1}{2} - \frac{1}{4} + \frac{1}{3} - \frac{1}{6}
	-\frac{1}{8} + \frac{1}{5}-\frac{1}{10} - \cdots\\
	&= \left(1 - \frac{1}{2}\right) - \frac{1}{4} + \left(\frac{1}{3} - \frac{1}{6}\right)
	-\frac{1}{8} + \left(\frac{1}{5}-\frac{1}{10}\right) - \cdots\\
	&= \frac12\left[1 - \frac{1}{2} + \frac{1}{3} - \frac{1}{4} +- \cdots\right]\\
	&= \frac{1}{2}S
\end{align*}
Funny...
\newline We start on the much awaited Chapter 2 next week.

\section{October 30}
ff

\section{November 1}
\begin{definition}[Metric Space]
	A \emph{metric space} is a pair $(X,d)$ combining a
	nonempty set$X$ and a function $d \colon X\times X \to \R$
	obeying conditions above.
\end{definition}
Using $d$ for ``distance" extends ideas/notations beyond Euclidean case
E.g. $B[x;r) = \{x' \in X \colon d(x',x)< r\}$ defines a ``ball"
with centre $x$ and radius $r>0$...
now an idea not a literal shape but ok...
Then declare a set $U \subseteq X$ to be \emph{open} exactly when
\[
	\forall x \in U, \exists \ep > 0 \colon B[x;r) \subseteq U
\]
Let $\mathcal{T}$ denote the set of all open sets in $X$:
that's the 9metric) topology.
As before, $B[x;r)$ is an open set for any $x \in X$,
any $\ep > 0$, and the set $\mathcal{T}$ has properties (HTS1) - (HTS4).

Why bother? to capture convergence of sequence ideas.
\begin{definition}[Convergence]
	Given sequence $x_1,x_2, \dots$ in $X$
	and point $\hat{x} \in X$, to say
	\[
		\lim_{n\to\infty} x_n = \hat{x} \quad \text{or}
		\quad x_n \to \hat{x} \text{ as } n \to \infty
	\]
	$\forall \ep > 0, \exists N \in \N$ such that $\forall n > N$, $d(x_n,\hat{x})<\ep$.
\end{definition}
``[After someone sneezed] I used to teach Engineers.
When someone sneezed, the whole class would say `bless you`."

\begin{proposition}
	In metric space $(X,d)$, a set $U$ is open iff every point $x \in U$ does this:
	whenver any $x_n$ has $x = \lim_{n\to\infty} x_n$ then
	$x_n \in U$ for all $n$ sufficiently large.
\end{proposition}
\begin{proof}
	$(\Rightarrow)$ Given $U \in \mathcal{T}$ and point $x \in U$,
	definition of open gives some $\ep > 0$ such that $B[x;\ep) \subseteq U$.
	So if $(x_n)$ is any sequence with $x_n \to x$, definition of
	convergence gives $N \in \N$ such that $\forall n > N$,
	$\underbrace{d(x_n,x)}_{x_n \in B[x;\ep) \subseteq U}< \ep$.

	$(\Leftarrow)$ Work on the contrapositive, so assume set $U$ is not open.
	Then some $x \in U$ breaks the defining property:
	$\exists \ep > 0, B[x;\ep) \subseteq U$.
	So for each $n \in \N$, $\ep=\frac{1}{n}$ here makes $B[x;\frac{1}{n}) \not\subseteq U$.
	That is, some point $x_n \in B[x;\frac{1}{n})$ obeys $x_n \not\in U$.
	Now $d(x_n,x) < \frac{1}{n}$ (some version of Squeeze in $\R$),
	so sequence $x_n$ obeys $x_n \to x$ and $x \not\in U$.
\end{proof}
He got a lot of push back at the start of the course because
was doing Rudin out of order;
but now we have this hands-on example of dealing with convergence
that we know a little how to play with metric spaces.

\subsection{Ball shapes in $\R^2$}
$\R^2 = \{x = (x_1,x_2) \colon x_1,x_2 \in \R\}$ can be given sseveral metrics, including:
\begin{itemize}
	\item $d_2(x,y) = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2}$.
		We recover everyone's favourite ball $B[0;1)$.
	\item $d_1(x,y) = |x_1 - y_1| + |x_2 - y_2|$.
		Our ball is now shaped like a diamond.
	\item $d_\infty(x,y) = \max\{|x_1-y_1|, |x_2-y_2|\}$.
		Our ball is now a square.
	\item Can interpolate: $d_p(x,y) = \left[|x_1-y_1|^p + |x_2 - y_2|\right]^{1/p}$,
		a valid metric for any $p \geq 1$ (also $p = +\infty$).
\end{itemize}

In principle, each different metric gives a different family of open sets,
and different story with convergence.
But amazingly, these define a compatible idea of convergence:
the set of open sets are the same regardless of these metrics.

\subsection{Hausdorff Topological Spaces}
``As is the norm in mathematics, I know the name [Hausdorff]
and nothing else."
\begin{definition}
	A HTS is an ordered pair $(X,\mathcal{T})$ where $X$ is a
	nonempety set, and $\mathcal{T} \subseteq \mathcal{P}(x)$
	with these $4$ properties:
	\begin{enumerate}
		\item[(HTS 1)] $\emptyset \in \mathcal{T}$, $X \in \mathcal{T}$.
		\item[(HTS 2)] For any collection $\mathcal{J} \subseteq \mathcal{T}$
			one has $\bigcup \mathcal{J} \in \mathcal{T}$
			(any union of open sets is open).
		\item[(HTS 3)] If $U_1,\dots,U_N \in \mathcal{T}$ (and $N \in \N$)
			then $\bigcap_{k=1}^N U_k \in \mathcal{T}$
			(any finite intersection of open sets is open).
		\item[(HTS 4)] Whenever $x \neq y$ in $X$,
			there exists $U,V \in \mathcal{T}$ obeying
			$x \in U, y \in V, U \cap V = \emptyset$
			(enough open sets to separate points).
	\end{enumerate}
\end{definition}
Examples
\begin{itemize}
	\item Any metric topology
	\item Discrete topology: $\mathcal{T} = \mathcal{P}(X)$
		(``every set is open.")
\end{itemize}

\section{December 4}
So normally in this last week, we do Calculus.
To do Calculus, we need derivatives.
To do derivatives, we need limits.

\subsection{Limits}
Limits are near the idea of continuity.
\begin{definition}[Metric Space Limit]
	Suppose $X<Y$ are metric spaces with $f \colon X \to Y$ and $x_0 \in X$.
	We will say
	\[
		\lim_{x \to x_0} f(x) = y
	\]
	exactly when
	\begin{enumerate}
		\item[(i)] $x_0 \in X'$
		\item[(ii)] $\phi(x) = \begin{cases} f(x), &\text{if }x\neq x_0\\
			y_0, &\text{if }x_0=x\end{cases}$
			is continuous at $x_0$.
	\end{enumerate}
\end{definition}
Intuition: $y_0$ is the ``best constant approximation for $f(x)$-values when $x$ is near $x_0$."
The function doesn't even need to be defined at $x_0$,
just crucially, we never look at $f(x_0)$.
And then Loewen draws the canonical picture of a graph with a hole.
Equivalently, we can write the conditions out explicitly:
\begin{enumerate}
	\item[(i)] $x_0 \in X'$
	\item[(ii)] $\forall \ep > 0$, $\exists \delta > 0 \colon
		\forall x \in \mathbb{B}(x_0;\delta), d_Y(f(x),y_0) < \ep$
		(note the parantheses in the ball!).
\end{enumerate}
For (ii),  a sequential alternative exists, namely,
$f(x_n) \to y_0$ for any sequence $x_n \to x_0$ along which $x_n \neq x_0$ $\forall n$.

Now that we have that, we can move on to the star of the show.

\subsection{Derivatives}
\subsubsection{Basics}
We've all seen this before, just now limit means a little more when we say it.
Really, he just wants to show us cool functions for the next two days.
\begin{definition}[Derivative]
	Given interval $[a,b] \subseteq \R$ and point $c \in [a,b]$,
	and $f \colon [a,b] \to \R$, let
	\[
		f'(c) = \lim_{x \to c}\frac{f(x)-f(c)}{x-c}
	\]
	if the limit exists.
\end{definition}
This is ``the derivative of $f$ at $c$" when the RHS conerges.
Say ``$f$ is differentiable at $c$" when this happens,
``non-differentiable" when it doesn't.
Now he draws the canonical picture of a slope of a graph.

Bro is complaining about Goldilocks.
``Goldilocks is the story of a juvenile deliqiuent who goes and invades
some peoples homes when they are out on a walk,
and then goes and complains about things!
Too hot! Too hard! Too hoo-haa!"
He couldn't decide no the next lemma, between Goldilocks and butterfly,
but butterfly requires less cultural context.

Instantenous Slope:
When $f'(c)$ exists, it tells the slope of the ``best linear approximation"
for $f(x)$ near $x = c$.
Compare with
\[
	l(x) = f(c) + m(x-c)
\]
for generic constant $m$.

\begin{lemma}[Goldilocks/Butterfly Lemma]
	In the setup from above:
	\begin{enumerate}
		\item[(i)] If $m < f'(c)$ then $\exists \delta > 0$ such that
			$l(x) > f(x)$ $\forall x \in (c-\delta, c)$ and
			$l(x) < f(x)$ $\forall x \in (c,c+\delta)$.
		\item[(ii)] If $m > f'(c)$ then $\exists \delta > 0$ such that
			$l(x) < f(x)$ $\forall x \in (c-\delta, c)$ and
			$l(x) > f(x)$ $\forall x \in (c,c+\delta)$.
	\end{enumerate}
\end{lemma}
Someone from Caltech wrote a book that basically invented the tangent line using this.
And then draws a picture of a concave down graph...
and looks like a butterfly.
Butterfly shape captures $y = f(x)$ on some interval $(c-\delta,c+\delta)$.

\begin{proof}
	\begin{enumerate}
		\item[(i)] Suppose $m < f'(c)$.
			Define $\ep = \frac12[f'(c)-m] > 0$.
			Use the limit definition to get $\delta > 0$ such that
			\[
				\left\lvert \frac{f(x) - f(c)}{x-c} - f'(c)\right\rvert < \ep,
				\forall x \in \mathbb{B}(c;\delta)
			\]
			\[
				-\ep|x-c| < f(x) - f(c) - f'(c)(x-c) < \ep |x-c|
			\]
			\[
				-\ep|x-c| < f(x) - [l(x) + (f'(c) - m)(x-c)] < \ep|x-c|
			\]
			\[
				\underbrace{(f'(c) - m)}_{2\ep}[x-c] - \ep|x-c|
				< f(x) - l(x) < \underbrace{(f'(c) - m)}_{2\ep}[x-c] + \ep|x-c|
			\]
			Now if $x > c$, $|x-c| = (x-c)$, and the left inequality says
			\[
				0 < \ep(x-c) < f(x) - l(x), \; \forall x \in (c,c+\delta)
			\]
			And for $x < c$, $|x-c| = -(x-c)$, so the right inequality says
			\[
				f(x) - l(x) < -2\ep|x-c|+\ep|x-c| = -\ep|x-c|,
				\; \forall x \in (c-\delta,c)
			\]
		\item[(ii)] Similar.
	\end{enumerate}
\end{proof}

\begin{corollary}
	If $f$ is differentiable at $c$, then $f$ must be continuous at $c$
\end{corollary}

\begin{proof}
	Use Butterfly Lemma + Squeeze Theorem (more in Canvas).
\end{proof}

\begin{remark}
	Converse is false: consider $f(x) = |x|$ and $c = 0$.
	The function is continuous at $c$, but $f'(c)$ does not exist.
\end{remark}
Our $f'(x)$ is a jump function, and there is no way to define a value at $c=0$.
Some derivatives are discontinuous -- but in ways more interesting than this. E.g.
\[
	f(x) = \begin{cases}
		x^2 \sin\frac{1}{x}, &x \neq 0\\
		0, &x=0
	\end{cases}
\]
`` Everyone's favourite really.
No one came for the chain rule."
Here, $f'9x) = 2x\sin{\frac{1}{x}} + x^2\cos{\frac{1}{x}} \left[\frac{-1}{x^2}\right]
= - \cos{\frac{1}{x}} + 2x\sin{\frac{1}{x}}$ for $x \neq 0$,
and $f'(0) = \lim_{x\to0} \frac{f(x) - f(0)}{x-0} = \lim_{x \to 0} x \sin{\frac{1}{x}}$.
So $f'(x)$ exists for all $x \in \R$ and it is NOT CONTINUOUS at $0$.
Then he drew the pictures...
121 looking function.

He'll show us more weird functions next time.

Q from after class: we can just assume our math 100 derivative rules on the final.

\section{December 6}
He does the butterfly lemma again.
Maybe instead of making a statement about what happens
when we chose the slope just right,
we describe what happens if we choose wrong.

Wrong ideas:
The tangent line touches the graph at one point only. No!
Tangent line touches the curve but it doesn't cross it. No!
If $f'(c)>0$ then $f$ is increasing on some open interval with mid-point $c$. No!
The thing that breaks evrything: $f(x) = \begin{cases} \frac12 x + x^2\sin(\frac{1}{x}) & x\neq 0 \\ 0 & x=0\end{cases}$.
$f'(x) = \frac12 - \cos{\frac{1}{x}} + 2x\sin{\frac{1}{x}}$ has values
positive and negative in every interval interval $(-\delta,\delta)$.

\subsection{Optimization}
\begin{theorem}
	Suppose $f \colon (a,b) \to \R$ has a minimum
	at some point $c \in (a,b)$ i.e. $f(c) \leq f(x)$ $\forall x \in (a,b)$.
	Then $f'(c) = 0$.
\end{theorem}
\begin{proof}
	Contrapositive and use the Butterfly.
	Suppose $f'(c) \neq 0$. WLOG assume $f'(c) > 0$.
	Pick $m = 0$ and use Butterfly Lemma to get $\delta > 0$ such that
	\[
		f(x) < f(c) + m(x-c), \forall x \in (c-\delta, c)
	\]
	Thus $f(c)$ is NOT a minimizer on $(a,b)$.
\end{proof}
``This is simple enough that they show you this in first year calculus,
and then they give you two weeks of problems."
The typical story is finding critical points, etc.
but very unlikely he will test us on it.

\begin{theorem}[Darboux]
	Derivatives have the intermediate value property,
	i.e., if $f$ is differentiable at all points of ff
\end{theorem}
\begin{proof}
	Result is obvious if $\mu = f'(a)$ or $\mu = f'(b)$.
	So suppose $f'(a) < \mu < f'(b)$ WLOG.
	Consder few function $g(x) = f(x) - \mu x$:
	this is continuous on $[a,b]$ with $g'(a) = f'(a) - \mu < 0$
	and $g'(b) = f'(b) - \mu > 0$.

	So (butterfly lemma) tho absolute minimum of $g$
	on set $[a,b]$ can't occur at either end.
	So it must be ff
\end{proof}

\begin{theorem}[Mean Value Theorem -- MVT]
	Given $f \colon [a,b] \to \R$ continuous,
	suppose $f'(x)$ exists $\forall x \in (a,b)$.
	Then $\exists c \in (a,b)$ such that
	\[
		\frac{f(b) - f(a)}{b-a} = f'(c)
	\]
\end{theorem}
Insert image.
``I think MVT is my favourite theorem."
``My g".
\begin{proof}
	Define $m = \frac{f(b) - f(a)}{b-a}$.
	$g(x) = f(x) - mx$.
	Evaluate: $g(a) = f(a) - ma$ and $g(b) = f(b) - mb$.
	Note $g(b) - g(a) = [f(b) - f(a)] - m(b-a) = 0$.
	Now $g$ is continuous with compact domain,
	so it has absolute max and absolute min in $[a,b]$.
	If $g$ is CONSTANT those are equal but $0 = g'(c) = f'(c) - m$ $\forall c \in (a,b)$.
	If $g$ is NOT CONSTANT then some $c \in (a,b)$
	will provide a local extremum and give $0 = g'(c) = f'(c) - m$.
\end{proof}
This gives what seems like is given in first-year calculus:
If a function is increasing, the derivative is going to be greater than or equal to zero.
But also, nontrivial, if the derivative is increasing, then so is the function.
\begin{corollary}
	If $f'(x) > 0$ $\forall x \in (a,b)$ then $f$ is increasing on $(a,b)$
\end{corollary}
\begin{proof}
	If $a' < b'$ lie in $(a,b)$, MVT says
	\[
		\frac{f(b') - f(a')}{b'-a'} = f'(c) > 0
	\]
	for some $c \in (a',b')$, so $f(b') > f(a')$.
	``Job done."
\end{proof}
``Review is for the weak." (I think he meant the day).
``The final exam, it's worth a lot, 50\%, so you probably want to study."
Only spend 20\% of your time reviewing. Do problems.
Think about 6 questions each day and write up 3, every day from now to the exam.
\end{document}
