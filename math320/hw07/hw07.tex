\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{geometry}
\geometry{letterpaper, margin=2.0cm, includefoot, footskip=30pt}

\usepackage{fancyhdr}
\pagestyle{fancy}

\lhead{Math 320}
\chead{Homework 7}
\rhead{Nicholas Rees, 11848363}
\cfoot{Page \thepage}

\newtheorem*{problem}{Problem}

\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\ep}{{\varepsilon}}
\newcommand{\SR}{{\mathcal R}}

\renewcommand{\theenumi}{(\alph{enumi})}

\begin{document}
\subsection*{Problem 1}
{\it Use $\sum_{n=1}^\infty \frac{1}{n^4} = \frac{\pi^4}{90}$ and
a splitting argument to evaluate
$S = 1 + \frac{1}{3^4} + \frac{1}{5^4} + \frac{1}{7^4} + \cdots.$}

\begin{proof}[Solution]\let\qed\relax
	We have
	\[
		\frac{\pi^4}{90}
		= \sum_{n=1}^\infty \frac{1}{n^4}
		= S + \sum_{\substack{n=1\\n\text{ even}}}^\infty \frac{1}{n^4}
		= S + \sum_{n=1}^\infty \frac{1}{(2n)^4}
		= S + \frac{1}{16}\sum_{n=1}^\infty\frac{1}{n^4}
		= S + \frac{\pi^4}{16\cdot90}
	\]
	Thus $S = \frac{\pi^4}{90} - \frac{\pi^4}{16\cdot90} = \frac{\pi^4}{96}$.
\end{proof}
\clearpage
~\clearpage

\subsection*{Problem 2}
{\it Test the following series for convergence.
Treat all real values of the constant parameter $p$.
\begin{enumerate}
	\item $\sum_{n=2}^\infty \frac{1}{(\log{n})^p}$
	\item $\sum_{n=2}^\infty \frac{1}{(\log{n})^n}$
	\item $\sum_{n=2}^\infty \frac{1}{n^p(\log{n})}$
	\item $\sum_{n=2}^\infty \frac{1}{n(\log{n})^p}$
\end{enumerate}}

\begin{enumerate}
	\item \begin{proof}[Solution]\let\qed\relax
		Note that $\log{n}$ is monotonically increasing
		and $\log{n} > 0$ when $n \geq 2$.
		Let $p \leq 0$.
		Then $(\log{n})^p$ is monotonically decreasing,
		so $a_n = \frac{1}{(\log{n})^p}$ is monotonically increasing.
		Note that $a_2 > 0$ for all $p$,
		and $a_n \geq a_2$ for all $n$,
		thus $(a_n)$ does not converge to zero
		because it is bounded below away from zero
		(one can use $\ep = a_2$ to show failiure to converge).
		Thus, by the crude divergence,
		$\sum_{n=2}^\infty \frac{1}{(\log{n})^p}$
		does not converge.

		Let $p > 0$.
		Since $\log{n}$ is monotonically increasing,
		$(\log{n})^p$ is also monotonically increasing.
		Furthermore, for $n \geq 2$, $(\log{n})^p > 0$.
		Thus, $(a_n)$, where $a_n = \frac{1}{(\log{n})^p}$,
		is a monotonically decreasing series.
		
		Also note that
		\[
			\sum_{k=1}^\infty 2^k a_{2^k}
			= \sum_{k=1}^\infty 2^k \frac{1}{(\log{2^k})^p}
			= \frac{1}{(\log{2})^p}\sum_{k=1}^\infty \frac{2^k}{k^p}
		\]
		Note that $\left(\frac{2^k}{k^p}\right)^{-1} = \frac{k^p}{2^k}$.
		Thus by Theorem 3.20 (d) in Rudin
		(where $\alpha = p$ and $p = 1$)
		we know that $\lim_{n\to\infty}\frac{k^p}{2^k} = 0$.
		But by question 6 (b) from homework 6,
		which states that if $x_n \to 0$ then $1/x_n$ cannot converge,
		we have that $\frac{2^k}{k^p}$ diverges.
		Thus, $\sum_{k=1}^\infty 2^k a_{2^k}$
		diverges as well
		(crude divergence test).
		Then, by the Cauchy Condensation Test,
		$\sum_{n=2}^\infty a_n$ must diverge as well
		(since $(a_n)$ is monotonically decreasing
		and bounded below by $0$).
		Hence, regardless of $p$, the series fails to converge.
	\end{proof}
	\item \begin{proof}[Solution]\let\qed\relax
		Let $a_n = \frac{1}{(\log{n})^n}$
		and
		\[
			b_n = \begin{cases}
				\frac{1}{(\log{2})^2} & n = 2\\
				\frac{1}{(\log{3})^n} & n \geq 3
			\end{cases}
		\]
		Consider the series $\sum_{n=2}^\infty b_n =
		\frac{1}{(\log{2})^n} + \sum_{n=3}^\infty \frac{1}{(\log{3})^n}$.
		Note that our series is a geometric series,
		specifically $\log{3} > 1$ so $0 < \frac{1}{\log{3}} < 1$,
		which is the common ratio $r$,
		and so we know that the series $\sum_{n=2}^\infty b_n$ converges.

		Now, since $0 < \log{3} \leq \log{n}$ for $n \geq 3$,
		we have $0 < \frac{1}{\log{n}} < \frac{1}{\log{3}}
		\implies 0 < \frac{1}{(\log{n})^n} < \frac{1}{(\log{3})^n}$,
		thus $b_n \geq a_n = |a_n| \geq 0$ for all $n$,
		and thus by the comparison test,
		$\sum_{n=2}^\infty a_n$ must converge as well.
		(This is true regardless of $p$; it was not used.)
	\end{proof}
	\item \begin{proof}[Solution]\let\qed\relax
		Let $a_n = \frac{1}{n^p(\log{n})}$.
		Let $p \leq 0$.
		Then $n > 1 \implies 0 < \frac{1}{n} < 1
		\implies \frac{1}{n^p} \geq 1$.
		Furthermore, $0 < \log{n} < n \implies \frac{1}{\log{n}} > \frac{1}{n} > 0$.
		Thus $\frac{1}{n^p}\frac{1}{\log{n}} > \frac{1}{n}
		= \left\lvert \frac{1}{n}\right\rvert > 0$.
		Recall that $\sum_n \frac{1}{n} = +\infty$,
		so by the comparison test,
		$\sum_{n=2}^\infty a_n = +\infty$ as well,
		i.e. it fails to converge.

		Now let $0 < p \leq 1$.
		Note that $(n+1)^p > n^p > 0$ and $\log{n+1} > \log{n} > 0$
		for all $n$,
		thus $(n+1)^p\log{n+1} > n^p\log{n} > 0$,
		and taking the reciprocal, we get $a_n > a_{n+1} > 0$.
		Now see
		\[
			\sum_{k=1}^\infty 2^k\frac{1}{(2^k)^p(\log{2^k})}
			= \frac{1}{\log{2}}\sum_{k=1}^\infty\frac{(2^k)^{1-p}}{k}
		\]
		First, if $p = 1$, then our sum becomes
		$\frac{1}{\log{2}}\sum_{k=1}^\infty\frac{1}{k}$,
		which diverges, so by the Cauchy condensation test,
		$\sum_n a_n$ must diverge as well.
		Now let $p \neq 1$.
		Note that $\left(\frac{(2^k)^{1-p}}{k}\right)^{-1}
		= \frac{k}{(2^{1-p})^k}$.
		By Theorem 3.20 (d) in Rudin (where $\alpha = 1$ and $p = 2^{1-p}-1>0$),
		we know that $\lim_{k\to\infty} \frac{k}{(2^{1-p})^k} = 0$.
		But then by question 6 (b) from homework 6,
		we have that $\frac{(2^k)^{1-p}}{k}$ must diverge as well.
		Thus $\sum_{k=1}^\infty 2^ka_{2^k}$ diverges as well
		(crude divergence test).
		Then, by the Cauchy condensation test,
		$\sum_{n=2}^\infty a_n$ must diverge as well.
		Thus if $0 < p \leq 1$, the series fails to converge.

		Finally, let $p > 1$.
		Note that for all $n \geq 3$,
		$\log{n} > 1$,
		so $0 < n^p < n^p \log{n}$, thus $0 < \frac{1}{n^p\log{n}}
		= \left\lvert \frac{1}{n^p\log{n}}\right\rvert < \frac{1}{n^p}$.
		Since $p > 1$,
		we know that $\sum_{n=3}^\infty \frac{1}{n^p}$ converges,
		thus by the comparison test,
		$\sum_{n=3}^\infty \frac{1}{n^p\log{n}}$ converges as well.
		Therefore, when $p > 1$,
		$\sum_{n=2}^\infty \frac{1}{n^p(\log{n})}$ converges.
	\end{proof}
	\item \begin{proof}[Solution]\let\qed\relax
		Let $a_n = \frac{1}{n(\log{n})^p}$.
		Let $p \leq 0$.
		Note that since $0 < \frac{1}{\log{n}} < 1$ for $n \geq 3$,
		we have $1 \leq \frac{1}{(\log{n})^p}$.
		Thus, $\frac{1}{n(\log{n})^p} \geq \frac{1}{n} = \left\lvert \frac{1}{n}\right\rvert > 0$.
		Furthermore, $\sum_n \frac{1}{n}$ diverges,
		thus by the comparison test,
		$\sum_{n=3}^\infty a_n$ diverges as well.
		Thus, when $p \leq 0$, $\sum_{n=2}^\infty a_n$ does not converge.

		Now let $p > 0$.
		Note that $(n+1) > n > 0$ and $\log{n+1} > \log{n} > 0
		\implies (\log{n+1})^p > (\log{n})^p > 0$ for all $n \geq 2$.
		thus $(n+1)^p\log{n+1} > n^p\log{n} > 0$,
		and taking the reciprocal, we get $a_n > a_{n+1} > 0$.
		Consider
		\[
			\sum_{k=1}^\infty 2^k\frac{1}{2^k(\log{2^k})^p}
			= \frac{1}{(\log{2})^p} \sum_{k=1}^\infty \frac{1}{k^p}
		\]
		But the sum is just the $p$-series.
		Thus if $p \leq 1$, this sum diverges,
		and if $p > 1$, the sum converges.
		Thus, by the Cauchy condensation test,
		when $0 < p \leq 1$, $\sum_n a_n$ fails to converge;
		when $p > 1$, $\sum_n a_n$ converges.
	\end{proof}
\end{enumerate}
\clearpage

\subsection*{Problem 3}
{\it Consider the set $\ell^2$ consisting of all real sequences
$x = (x_1, x_2, \dots)$ enjoying the special property that
$\sum_n |x_n|^2$ converges.
Define an inner product on $\ell^2$ as follows:
\[
	\forall x,y \in \ell^2, \; \langle x, y \rangle := \sum_{n=1}^\infty x_ny_n
\]
\begin{enumerate}
	\item Prove that the series in this definition converges.
\end{enumerate}
Informally, this is the natural generalization of Euclidean $k$-space
to the case $k = \aleph_0$;
the inner product $\langle x,y \rangle$ in $\ell^2$ is
analogous to the dot product $x \bullet y$ in $\R^k$.
It's only a small stretch to call the elements of $\ell^2$ ``vectors".
Add further credibility to this interpretation by defining
$\lVert x \rVert = \sqrt{\langle x , x\rangle}$
for each $x \in \ell^2$, and then proving
\begin{enumerate}
	\item[(b)] $\lvert \langle x,y \rangle| \leq \lVert x \rVert\lVert y \rVert$
		for all $x,y \in \ell^2$.
	\item[(c)] $\lVert x + y \rVert \leq \lVert x \rVert + \lVert y \rVert$ for all $x,y \in \ell^2$.
\end{enumerate}
This generalization has some limitations, however.
In $\R^k$, any sequence of vectors $x^{(1)},x^{(2)},x^{(3)},\dots$,
whose compoenent sequences converge must be a convergent sequence of vectors,
and its limit can be identified by taking the limit in each component separetly.
Show that this fails in $\ell^2$, as follows:
\begin{enumerate}
	\item[(d)] Construct a sequence $x^{(1)},x^{(2)},\dots$, of vectors in $\ell^2$
		such that $\lVert x^{(n)} \rVert = 1$ for all $n$,
		and yet every $p \in \N$ the `$p$-th component sequence'
		$\langle \mathbf{e}_p, x^{(n)}\rangle$ converges to $0$ as $n \to \infty$.
		Here, just as in $\R^k$, $\mathbf{e}_p$ denotes the ``standard unit vector"
		with exactly one nonzero entry,
		which is a $1$ in position $p$.
\end{enumerate}}

\begin{enumerate}
	\item \begin{proof}[Solution]\let\qed\relax
		For all $n$, we have $(x_n + y_n)^2 \geq 0$,
		so $x_n^2 + y_n^2 \geq -2x_ny_n$,
		and $(x_n - y_n)^2 \geq 0$,
		so $x_n^2 + y_n^2 \geq 2x_ny_n$,
		hence $x_n^2 + y_n^2 \geq 2|x_ny_n| \geq |x_ny_n|$.

		Let $b_n = x_n^2 + y_n^2 = |x_n|^2 + |y_n|^2$.
		We now show that $\sum_n b_n$ converges.
		Let $X_n = \sum_{k=0}^n |x_k|^2$ and
		$Y_n = \sum_{k=0}^n|y_k|^2$.
		Thus $X_n + Y_n = \sum_{k=0}^n (|x_k|^2 + |y_k|^2) = \sum_{k=0}^n b_k$.
		Since $\sum_n |x_n|^2$ and $\sum_n |y_n|^2$ converge,
		denote their limits as
		$X = \sum_n|x_n|^2 = \lim_{n\to\infty}X_n$
		and $Y = \sum_n|y_n|^2 = \lim_{n\to\infty}Y_n$.
		So by the addition limit law, we have
		$\sum_n b_n = \lim_{n\to\infty} \sum_{k=0}^n b_k
		= \lim_n\to\infty(X_n + Y_n) = X + Y$,
		thus $\sum_n b_n$ converges.

		Finally, by the comparison test, since $\sum_n (x_n^2 + b_n^2)$,
		we have that $\sum_n (x_ny_n)$ converges as well.
	\end{proof}
	\item \begin{proof}[Solution]\let\qed\relax
		Note that if we consider the first $k$ terms of $x_n$ and $y_n$
		as entries of a $k$-tuple,
		we have shown in class the Cauchy Schwartz inequality:
		\[
			L_k = \left\lvert\sum_{n=1}^k x_ny_n\right\rvert \leq
			\sqrt{\sum_{n=1}^k x^2_n}\sqrt{\sum_{n=1}^k y^2_n}
			= \sqrt{\sum_{n=1}^k x^2_n\sum_{n=1}^k y^2_n} = R_k
		\]
		Note that $L_k$ and $R_k$ are sequences that satisfy $L_k \leq R_k$
		for all $k$.
		Thus by the lemma from October 11 from the course notes,
		we also have
		\[
			\liminf_{k\to\infty} L_k \leq \liminf_{k\to\infty} R_k
			\quad \text{and} \quad
			\limsup_{k\to\infty}L_n \leq \limsup_{k\to\infty}R_k
		\]
		Furthermore, both $\sum_n x_n^2$ and $\sum_n y_n^2$
		converge,
		so our limit laws tell us that their product must also converge,
		and $R_k$ is just the square of a convergent sequence,
		and so $R_k$ must also converge.
		Additionally, we proved in part (a) that $L_k$ converges.
		Thus, both of their $\limsup$s and $\liminf$s
		must equal each other, so we get
		\[
			\lim_{k\to\infty} L_k \leq \lim_{k\to\infty} R_k
		\]
		But extracting the definitions of $L_k$ and $R_k$,
		this is just
		\[
			\lim_{k\to\infty} \left\lvert\sum_{n=1}^k x_ny_n\right\rvert \leq
			\lim_{k\to\infty} \sqrt{\sum_{n=1}^k x^2_n}\sqrt{\sum_{n=1}^k y^2_n}
		\]
		\[
			\implies |\langle x,y \rangle| =
			\left\lvert\sum_{n=1}^\infty x_ny_n\right\rvert \leq
			\sqrt{\sum_{n=1}^\infty x^2_n}\sqrt{\sum_{n=1}^\infty y^2_n}
			= \lVert x \rVert \lVert y \rVert
		\]
	\end{proof}
	\item \begin{proof}[Solution]\let\qed\relax
		ff idk something to do with previous one.
		Note that if we consider the first $k$ terms of $x_n$ and $y_n$
		as entries of a $k$-tuple,
		we have shown in class the triangle inequality:
		\[
			L_k = \sqrt{\sum_{n=1}^k x_n + y_n} \leq
			\sqrt{\sum_{n=1}^k x^2_n} + \sqrt{\sum_{n=1}^k y^2_n} = R_k
		\]
		Note that $L_k$ and $R_k$ are sequences that satisfy $L_k \leq R_k$
		for all $k$.
		Thus by the lemma from October 11 from the course notes,
		we also have
		\[
			\liminf_{k\to\infty} L_k \leq \liminf_{k\to\infty} R_k
			\quad \text{and} \quad
			\limsup_{k\to\infty}L_n \leq \limsup_{k\to\infty}R_k
		\]
		Furthermore, both $\sum_n x_n^2$ and $\sum_n y_n^2$
		converge,
		so our limit laws tell us that their sum must also converge,
		and $R_k$ is just the square of a convergent sequence,
		and so $R_k$ must also converge.
		Additionally, in order for $x,y \in \ell^2$,
		we must have that $L_k$ converges.
		Thus, both of their $\limsup$s and $\liminf$s
		must equal each other, so we get
		\[
			\lim_{k\to\infty} L_k \leq \lim_{k\to\infty} R_k
		\]
		But extracting the definitions of $L_k$ and $R_k$,
		this is just
		\[
			\lim_{k\to\infty}\sqrt{\sum_{n=1}^k x_n + y_n} \leq
			\lim_{k\to\infty}\sqrt{\sum_{n=1}^k x^2_n} +
			\lim_{k\to\infty}\sqrt{\sum_{n=1}^k y^2_n}
		\]
		\[
			\implies \lVert x + y\rVert = \sqrt{\sum_{n=1}^\infty x_n + y_n}
			\leq \sqrt{\sum_{n=1}^\infty x^2_n} + \sqrt{\sum_{n=1}^\infty y^2_n}
			= \lVert x \rVert + \lVert y \rVert
		\]
	\end{proof}
	\item \begin{proof}[Solution]\let\qed\relax
		Consider the sequence of vectors:
		\begin{align*}
			x^{(1)} &= 1,0,0\dots\\
			x^{(2)} &= \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}},0,0,\dots\\
					&\vdots\\
			x^{(n)} &= \underbrace{\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{n}},
			\dots, \frac{1}{\sqrt{n}}}_{n \text{ times}}
			0,0,\dots
		\end{align*}
	\end{proof}
	All of these vectors lie in $\ell^2$,
	since they are just the sum of a finite sequence of terms.
	Note that $\lVert x^{(n)}\rVert = \sum_{k=0}^n \frac{1}{n} = 1$.

	However, $\langle \mathbf{e}_p, x^{(n)} \rangle = \frac{1}{\sqrt{n}}$
	if $p \leq n$ or $= 0$ if $p > n$.
	Regardless,
	as $n \to \infty$,
	$\frac{1}{\sqrt{n}} \to 0$ as well,
	thus $\langle \mathbf{e}_p, x^{(n)} \rangle \to 0$ as $n \to \infty$.
\end{enumerate}
\clearpage

\subsection*{Problem 4}
{\it Given that the sequence $(s_n + 2s_{n+1})$ converges,
prove that the sequence $(s_n)$ converges.}
\begin{proof}[Solution]\let\qed\relax
	Since $(s_n + 2s_{n+1})$ converges,
	then for any $\ep'$, there exists some $N' \in \N$
	such that for for all $n \geq N$ and for all $p \in \N$,
	we have $|2s_{n+p+1} + s_{n+p} - 2s_{n+1} - s_n| < \ep'$.

	We want to show that if $\ep > 0$,
	there exists some $N \in \N$ such that
	for all $n \geq N$ and for all $p \in \N$,
	we have $|s_{n+p} - s_n| < \ep$.
	To do this then, we want some bound on $|2s_{n+p+1} - 2s_{n+1}|$.
	This kinda makes me feel like induction.
	If we let $p = 1$, then we have $|2s_{n+2} - 2s_{n+1}|$.

	Maybe something about how 

	Let $\ep > 0$ be arbitrary.
	Let $N = N'$.
	We will show that $(s_n)$ is Cauchy.
	Let $N = N'$ (where $N'$ is so that other series bounded by $\ep$ too).
	Let $n \geq N$. We now induct on $p$.
	Base case ($p = 1$):
	we have $|s_{n+1} - s_n|$.
	But note that $\ep > |2s_{n+2} + s_{n+1} - 2s_{n+1} - s_n|
	= |2s_{n+2} - s_{n+1} - s_n|$.

	Hmm, what if showing $s_n$ gets arbitrarily close to $s_n + 2s_{n+1}$.

	\textbf{If the above is still gibberish, it's because I had to submit
	in a rush and didn't end up solving/fixing this question.}
\end{proof}
\clearpage
~\clearpage

\subsection*{Problem 5}
{\it Prove that if $\sum^\infty_{n=1}a^2_n$ converges,
then $\sum_{n=1}^\infty \frac{a_n}{n^q}$
converges for any constant $q > \frac{1}{2}$.}

\begin{proof}[Solution]\let\qed\relax
	First, note that $\sum_{n}|\frac{1}{n^q}|^2 = \sum_n \frac{1}{n^{2q}}$.
	Since $2q > 1$, this is a $p$-series that converges.
	Thus, by problem 3(a), we have that
	$\sum_{n=1}^\infty \frac{a_n}{n^q}$ will converge as well
	(where  we substitute $x_n = a_n$ and $y_n = \frac{1}{n^q}$).
\end{proof} 
\clearpage
~\clearpage

\subsection*{Problem 6}
{\it In parts (a)-(c) below, suppose $a_n > 0$ and $b_n > 0$ for all $n$, and define
\[
	A = \sum_{n=1}^\infty a_n, \quad B = \sum_{n=1}^\infty b_n
\]
\begin{enumerate}
	\item Prove the Limit Comparison Test:
		If $b_n/a_n$ converges to a real number $L > 0$,
		then series $A$ converges if and only if series $B$ converges.
	\item Prove the Ratio Comparison Test:
		If $a_{n+1}/a_n \leq b_{n+1}/b_n$, convergence of series $B$
		implies convergence of series $A$.
		What if $a_{n+1}/a_n \leq b_n/b_{n-1}$ instead?
		[Clue: Start by finding upper and lower bounds for the sequence $r_n = a_n/b_n$.]
	\item Use (b) with $\zeta(p)$ to prove Raabe's Test:
		if $p > 1$ and $a_{n+1}/a_n \leq 1 - p/n$ for all $n$ sufficiently large,
		then series $A$ converges.
		[Clue: First show that $1-px < (1-x)^p$ for all $x \leq 1$. Just use calculus.]
	\item Test $\sum_n a_n$ for convergence,
		where $a_n = \frac{1\cdot 4 \cdots (3n+1)}{n^23^nn!}$.
\end{enumerate}

\begin{enumerate}
	\item \begin{proof}[Solution]\let\qed\relax
		Let $b_n/a_n \to L > 0$ as $n \to \infty$.
		Then for any $\ep = L$,
		there exists some $N \in \N$ such that for $n \geq N$,
		we have $|b_n/a_n - L| < L$.
		Furthermore, since $a_n,b_n,L > 0$, we have
		\[
			\frac{b_n}{a_n} - L = \left\lvert\frac{b_n}{a_n}\right\rvert - |L|
			\leq \left\lvert\left\lvert\frac{b_n}{a_n}\right\rvert - |L|\right\rvert
			\leq \left\lvert \frac{b_n}{a_n} - L \right\rvert  < L
		\]
		Thus $0 < |b_n| < 2La_n$.
		Assume that series $A$ converges.
		Then $\lim_{n \to \infty} \sum_{k=N}^n (2La_k) = 2L\lim_{n \to \infty} \sum_{k=N}^n a_k$
		converges as well.
		Thus by the comparison test,
		$\sum_{k=N}^\infty b_k$ converges,
		thus $\sum_{k=1}^{N-1}b_k + \sum_{k=N}^\infty b_k = B$
		converges as well.

		Now, note that if $b_n/a_n$ converges to some positive value,
		$a_n/b_n$ must converge to some positive value as well,
		specifically, $L^{-1}$
		(by Rudin Theorem 3.3 (d)),
		since $b_n/a_n \neq0$ for all $n$ and $L\neq0$.
		Thus, we can repeat an identical argument to the one above
		to show that $B$ converging implies $A$ converging
		(where we just swap the $a_n$'s and $b_n$'s around).
	\end{proof}
	\item \begin{proof}[Solution]\let\qed\relax
		Let $a_{n+1}/a_n \leq b_{n+1}/b_n$.
		Then since $a_n,b_n > 0$, we have
		\[
			\frac{a_n}{b_n} \geq \frac{a_{n+1}}{b_{n+1}}
		\]
		This clearly has an upper bound,
		namely $a_1/b_1$,
		and since both $a_n,b_n > 0$ for all $n$, we must have
		$a_n/b_n > 0$ for all $n$,
		hence $0$ is a lower bound for $a_n/b_n$.
		Thus, by the Monotone Convergence property,
		we have that $a_n/b_n$ converges,
		which we'll denote to be a value, $L \geq 0$.
		If $L > 0$,
		then convergence of $B$ implies convergence of $A$
		by the Limit Comparison Test above.

		It remains to show this is still true when $L = 0$.
		If $a_n/b_n \to 0$ as $n \to \infty$,
		then if $\ep = 1$,
		there exists some $N \in \N$ such that for all $n \geq N$,
		we have $\left|\frac{a_n}{b_n}\right| < 1$.
		But $\frac{a_n}{b_n} = \left|\frac{a_n}{b_n}\right|$,
		thus $0 < a_n = |a_n| < b_n$.
		Assume that series $B$ converges.
		Then $\lim_{n\to\infty}\sum_{k=N}^nb_n$ converges as well
		(all subsequences of a convergent sequence converge).
		Thus by the comparison test, $\sum_{k=N}^\infty a_k$ converges,
		thus $\sum_{k=1}^{N-1}a_k + \sum_{k=N}^\infty a_k = A$
		converges as well.

		Note that $a_{n+1}/a_n \leq b_n/b_{n-1}$ is also a valid
		statement too,
		since we can just reindex $b_n$ by bumping everything along by $1$,
		(say set $b'_n = b_{n-1}$ and $b'_1 = 1$).
		This doesn't alter the convergence of $B$ (ie. $\sum_n b'_n$ converges
		if and only if $\sum_n b_n = B$ converges),
		since we are just adding a finite term,
		thus, the Ratio Comparison test still holds.
	\end{proof}
	\item \begin{proof}[Solution]\let\qed\relax
		We first invoke calculus to show that $1 - px \leq (1-x)^p$
		when $0 \leq x \leq 1$ and $p > 1$.
		Note that
		\[
			\frac{d}{dx}(1-x)^p\vert_{x=0} = -p(1-x)^{p-1}\vert_{x=0} = -p
		\]
		Now, at $x_0 = 0$, $(1-x_0)^p = 1 = y_0$.
		Thus the tangent line at $x_0$ is $y_0 - p(x-x_0) = 1-px$.
		Realize that $(1-x)^p$ is convex on $x \in [0,1]$
		since $p > 1$
		(one can confirm this with the second derivative, $p(p-1)(1-x)^{p-1}$).
		Thus, our tangent line lies below the curve on the interval,
		thus $1-px \leq (1-x)^p$.

		Thus, we have for all $n$ sufficiently large,
		\[
			a_{n+1}/a_n \leq 1-p/n \leq (1-\frac{1}{n})^p = \left(\frac{n-1}{n}\right)^p
		\]
		(which is valid, since $\frac{1}{n} \in [0,1]$ for all $n \in \N$).
		Thus, if $b_n = \frac{1}{n^p}$,
		then by what we just proved in part (b) for the ratio comparison test,
		since $a_{n+1}/a_n \leq b_n/b_{n-1}$,
		and $B = \sum_{n}b_n$ converges since $b_n$
		is a $p$-series with $p > 1$,
		we can conclude that $\sum_n a_n = A$ converges as well.
	\end{proof}
	\item \begin{proof}[Solution]\let\qed\relax
		Note that for all $n$.
		\begin{align*}
			a_{n+1}/a_n
			&= \frac{(3(n+1)+1)n^2}{(n+1)^23(n+1)}\\
			&= \frac{(3n+4)n^2}{3(n+1)^3}\\
			&\leq \frac{(3(n+1)n^2}{3(n+1)^3}\\
			&= \left(\frac{n}{n+1}\right)^2
		\end{align*}
		Then $a_{n+1}/a_n \leq \left(\frac{n}{n+1}\right)^2$.
		Note that when $n \geq 2$,
		we have that $1 - \frac{1.1}{n} \geq \frac{2}{n} \geq a_n$.
		Thus for sufficiently large $n$,
		there exists $p > 1$
		such that $a_{n+1}/a_n \leq 1-p/n$,
		so by Raabe's test, $\sum_na_n$ converges.
	\end{proof}
\end{enumerate}
\clearpage

\subsection*{Problem 7}
{\it Prove: If each $a_n \geq 0$ and $\sum_{n=1}^\infty a_n$ diverges,
then $\sum_{n=1}^\infty \frac{a_n}{1+a_n}$ also diverges.
Does the converse hold?}

\begin{proof}[Solution]\let\qed\relax
	We prove the contrapositive.
	Let each $a_n \geq 0$.
	Let $\sum_{n=1}^\infty \frac{a_n}{1+a_n}$ converge.
	By the Monotone Convergence criterion,
	we must have that the sequence of partial sums of $\frac{a_n}{1+a_n}$
	is bounded,
	say $0 \leq \frac{a_n}{1+a_n} < M$ and $M > 1$.
	ff wait this should be a partial sum
	say $0 \leq \sum_{n=1}^k \frac{a_n}{1+a_n} < M$.
	Then $a_n < M + Ma_n \implies -M < (M-1)a_n \implies -\frac{M}{M-1} < a_n$.
	The thing is... I doubt that $M$ can turn into a bound for $\sum_n a_n$.
	ff

	The converse of this is if $\sum_n a_n$ converges,
	then $\sum_n \frac{a_n}{1+a_n}$ converges as well.
	This is not true: we provide the counter-example
	$a_n = $
\end{proof}
\clearpage
~\clearpage

\subsection*{Problem 8}
{\it
\begin{enumerate}
	\item Prove: Given any $D \in \R$ and $\delta > 0$,
		there is a finite collection of numbers $a_1,a_2,\dots,a_N$
		such that $D = a_1 + a_2 + \cdots + a_N$ and
		\[
			\delta > |a_1| > |a_2| > \cdots > |a_N| > 0
		\]
	\item Let $(\sigma_n)_{n\in\N}$ be an arbitrary sequence of real numbers.
		Explain how to construct a sequence $(x_n)_{n\in\N}$ in $\R$ satisfying, simultaneously
		\begin{enumerate}
			\item[(i)] $|x_n| > |x_{n+1}|$ for all $n$, and $x_n \to 0$ as $n \to \infty$, and
			\item[(ii)] the sequence $(s_N)_{N\in\N}$ defined by
				$s_N = \sum_{n=1}^N x_n$ has $(\sigma_n)_n$ as a subsequence.
		\end{enumerate}
\end{enumerate}
\emph{Discussion}: This show badly the converse of the Crude Divergence Test can fail:
the series $\sigma_n x_n$ has terms tending to $0$,
yet its sequence of partial sums can be wild enough to
hit all elements of the preassigned $(\sigma_n)_n$.

\begin{enumerate}
	\item \begin{proof}[Solution]\let\qed\relax
		By the Archimedean property,
		there exists an integer $q$ such that $q\delta > |D|$.
		integer we can take).
		Then let $l = |D|/q < \delta$.
		Then assign $|a_1| = l\frac{2q+1-1}{2q+1}$,
		$|a_2| = l\frac{2q+1-2}{2q+1}$, etc. $|a_j| = l\frac{2q+1-j}{2q+1}$
		(we will assign signs later).
		Note that each of these are less than $l$,
		since the fraction is always less than $1$,
		but never $0$.
		Then letting $j$ vary from $1$ to $2q$, we have
		\[
			\sum_{j=1}^{2q}|a_j| = \sum_{j=1}^{2q}l\frac{2q+1-j}{2q+1}
			= l\sum_{j=1}^{q}\left(\frac{2q+1-j}{2q+1} + \frac{2q+1+j}{2q+1}\right)
			= lq = |D|
		\]
		Now, assign each $|a_n|$ the same sign as $D$,
		and one now sees that,
		$\delta > l > |a_1| > |a_2| > \cdots > |a_{2q}| > 0$;
		and $D = a_1 + a_2 + \cdots + a_{2q}$.
		Since $q$ was an integer, this is a finite collection.
	\end{proof}
	\item \begin{proof}[Solution]\let\qed\relax
		We construct our sequence $(x_n)$ inductively
		such that $(\sigma_n)_n$ is a subsequence of $s_N$.
		Let $x_1 = \sigma_1 \implies s_1 = \sigma_1$.
		Now, for any $k \in \N$,
		assume that $x_j$, for all $j \in \{1,\dots, N\}$,
		has been defined such that $s_N = \sigma_k$
		and $|x_n| > |x_{n+1}|$ for all $n$.
		By part (a), if $\delta = \min\{|x_N|, \frac{1}{N}\}$
		and $D = \sigma_{k+1} - \sigma_k$,
		there is some finite collection of $M$ terms
		such that $\min\{|x_N|,\frac{1}{N}\} > |x_{N+1}| > |x_{N+2}| > \cdots > |x_{N+M}| > 0$
		and $\sigma_{k+1} - \sigma_k = x_{N+1} + x_{N+2} + \cdots + x_{N+M}$.
		Thus, $s_{N+M} = s_N + x_{N+1} + x_{N+2} + \cdots + x_{N+M}
		= \sigma_k + \sigma_{k+1} - \sigma_{k} = \sigma_{k+1}$.
		Thus, our constructed sequence satisifes (ii).

		To show it satisifies (i),
		note that we have $|x_n| > |x_{n+1}|$ for all $n$, by construction.
		Finally, to show that $x_n \to 0$ as $n \to \infty$,
		let $\ep > 0$ be arbitrary.
		Then by Archimedean property, there exists some $k \in \N$
		such that $k > \frac{1}{\ep} > 0 \implies 0 < \frac{1}{k} < \ep$.
		But then we know that for all $n > k$,
		we have $|x_n| < \frac{1}{k} < \ep$ by the construction,
		thus $x_n \to 0$ as $n \to \infty$,
		giving us (i) as well.
	\end{proof} 
\end{enumerate}
\clearpage
~\clearpage

\end{document}
