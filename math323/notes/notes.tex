\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{geometry}
\geometry{letterpaper, margin=2.0cm, includefoot, footskip=30pt}

\usepackage{fancyhdr}
\pagestyle{fancy}

\lhead{Math 321}
\chead{Notes}
\rhead{Nicholas Rees}
\cfoot{Page \thepage}

\newtheorem*{problem}{Problem}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\theoremstyle{remark}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\ep}{{\varepsilon}}
\newcommand{\SR}{{\mathcal R}}

\renewcommand{\theenumi}{(\alph{enumi})}

\begin{document}
\section{January 9}
Going to do on chalkboard because he prefers the pacing;
so not going to be any notes produced by him.
No final exam, quizzes every two weeks, so keep on top of things;
the last quiz may be weighted more.
Good idea to review linear algebra, like eigenvectors/eigenvalues, etc.

Today's lecture will be off the top of his head, got into police incident last night.

\subsection{Rings Intro}
Can think of a generalization of $\Z$,
where you have an addition $+$ and a multiplication $\cdot$.
We assume that $(R,+)$ is an abelian group;
$\cdot$ is associative, there exists an identity $1_R$, but that's it;
and the distributive law $a(x+y) = ax + ay$, $(x+y)a = xa+ya$.
(Don't forget about closure of the opertions!!!).

These things are completely ubiquitious:
there are a lot more examples of rings than groups.

\noindent Examples:
\begin{itemize}
	\item $\Q$, $\C$, $\R$
	\item polynoimal with coefficients in $\C$, $\Q$, $\R$
	\item $n \times n$ matrices with entries in $\C$, $\Q$, $\R$
		(product is not commutative)
		\[
			1_R = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}, \quad
			0 = \begin{pmatrix} 0 & 0 \\ 0 & 0 \end{pmatrix}, \quad
			\begin{pmatrix} \vec{a} \\ \vec{b} \end{pmatrix}
			\begin{pmatrix} \vec{p} & \vec{q} \end{pmatrix}
			= \begin{pmatrix} \vec{a} \cdot \vec{p} & \vec{a} \cdot \vec{q}\\
			\vec{b} \cdot \vec{p} & \vec{b} \cdot \vec{q} \end{pmatrix}
		\]
\end{itemize}

In a ring, we can have $xy = 0$ even if $x,y \neq 0$,
e.g. $x = y = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}$, $R = 2\times 2$ matrices.
$x$ is a left zero divisor, $y$ is a right zero divisor.
$x^n = 0$ is possible for $x \neq 0$.
So very few things hold in all rings.
But rings can do much of what we want to do in a lot of contexts:
addition/subtraction, multiplication, but no division.

E.g. suppose $x^n = 0$, $x \in R$.
Then there exists a multiplicative inverse for $(1 - x)$.
\[
	\frac{1}{1-x} = 1 + x + x^2 + \cdots x^{n-1} +
	\underbrace{x^n}_{0} + \underbrace{x^{n+1}}_{0}
\]
\begin{align*}
	(1-x)(1 + x + x^2 + \cdots + x^{n-1})
	&= (1 + x + x^2 + \cdots + x^{n-1}) - x(1 + x + x^2 + \cdots + x^{n-1})\\
	&= (1+ x + \cdots + x^{n-1}) - x - x^2 - \cdots - x^{n-1} - x^n\\
	&= 1-x^n = 1
\end{align*}
So act similarly to what we expect, but have to be careful about commutative.
Note that this is like the approximation that analysts do,
where we are assuming $x^n$ is sufficiently small...
well, this is like ``infinitely small",
and some people in algebraic geometry actually do stuff like this.

See
\begin{align*}
	(x+y)^2
	&= (x+y)(x+y)\\
	&= x(x+y) + y(x+y)\\
	&= x^2 + \underbrace{xy + yx}_{\text{not same unless} xy = yx} + y^2
\end{align*}
So when our ring is commutative, we recover the binomial theorem we know and love.

\subsection{Types of Rings (lots!)}
\begin{enumerate}
	\item Commutative (multiplication is commutative).
		Algebra works as it should, but still have to deal with zero divisors.
		Huge field, ``commutative algebra".
	\item Domains: no zero divisors $xy = 0 \implies x = 0$ or $y = 0$.
		Usually applies to commutative rings.
	\item Division rings: $(R^*, \cdots)$ is a group (which may or may not be commutative).
		[Note $R^* := R \setminus \{0\}$]
	\item Fields: $(R^*, \cdot)$ is a commutative group
\end{enumerate}
\begin{remark}
	$0 \cdot a = a \cdot 0 = 0$, $\forall a \in R$
\end{remark}
\begin{proof}
	$a + 0 \cdot a = (1 + 0)\cdot a = 1\cdot a = a$
	and so $0 \cdot a = 0 $, $\forall a$,
	and works the same on the other side.
\end{proof}
Note then that $0 = 1 + (-1)$ and so $0 \cdot a = (1 + (-1))a = a + (-1)a = 0$,
hence the additive inverse of the multiplicative identity,
multiplied by $a$ gives $a$'s additive inverse as well.

Now let $R = \{a + b\sqrt{2} \mid a,b \in \Q\}$.
Claim: this is a field.
$(a + b \sqrt{2})(c + d\sqrt{2}) = ac + 2bd + (bc + ad) \sqrt{2} \in R$.
We now want to show $\frac{1}{a+b\sqrt{2}}$ exists in $R$.
See
\[
	\frac{1}{a+b\sqrt{2}} = \frac{a-b\sqrt{2}}{(a+b\sqrt{2})(a - b\sqrt{2})}
		= \frac{a - b\sqrt{2}}{a^2 - 2b^2} \in R
\]
provided $a^2 - 2b^2 \neq 0$.
Always true since $a^2 - 2b^2 = 0 \iff \frac{a^2}{b^2} = 2$ so $\frac{a}{b} = \pm \sqrt{2}$.
We have $a + b \sqrt{d}$ as long as $\sqrt{d}$ is irrational.

What about these funny noncommutative division rings.
Define $\mathbb{H}_\R = \{a + bi + cj + dk \mid a,b,c,d \in \R \text{ or } \Q\}$ where
$i^2 = j^2 = k^2 = -1$, $ij = k$, $jk = i$, $ki = j$
and $ji = -k$, $kj = i$, $ik = -j$.
We have division:
\[
	(a + bi + cj + dk)^{-1} = \frac{a - bi - cj - dk)}{a^2 + b^2 + c^2 = d^2}
\]
These are called the Quaternions.

A note on the axioms: some people actually define rings without the multiplicative identity,
but we will always assume it has one.

\begin{definition}[Nilpotent elements]
	$x^n = 0$ for some $n \in \Z^+$ (``infinitely small")
\end{definition}
``Something going off in my pocket doesn't sound that good, but it's been that kind of day."

There are a lot of pathologies in rings.
Something that holds for one might be really different in another.
For example, when we drop that division axiom,
things get really wonky.

\subsection{Matrix rings}
A matrix is an array with $m$ rows, $n$ columns,
with entries $a_{ij}$ in the $i$-th row and $j$-th column.
We now let $a_{ij} \in R$ where $R$ can be any ring (not just $\Q,\C,\R$).
We call this $M_{n\times m}(R)$.
The rules of algebra are the same as always, e.g.
\[
	\begin{pmatrix} a & b & c \\ d & e & f \end{pmatrix}
	\begin{pmatrix} \alpha \\ \beta \\ \gamma \end{pmatrix}
	:= \begin{pmatrix} a \alpha + b \beta + c\gamma \\ d\alpha + e\beta + f \gamma \end{pmatrix}
\]
where the multiplication and addition is in $R$.
This works because we don't need division in the entries of matrices,
unless perhaps we are taking inverse.
\begin{remark}
	$M_{1 \times 1}(R) = R$ and not neccesarily commutative
\end{remark}

It is surprising that we are able to say things about these matrices.
We have that $M_{n \times n}(R)$ is a ring, which we normally write as $M_n(R)$
(the product of $n \times n$ matrices is $n \times n$).

Scalar matrices $\begin{pmatrix} \alpha &0 &\cdots &0\\ 0 & \alpha &\cdots & 0 \\\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \alpha \end{pmatrix}$ where $\alpha \in R$.
This turns out to be a copy of $R$ (isomorphic),
where the identity is $I =
\begin{pmatrix} 1 &0 &\cdots &0\\ 0 & 1 &\cdots & 0 \\\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1 \end{pmatrix}$.

Assume that $R$ is commutative and $A \in M_n(R)$.
When does $A^{-1}$ exist in $M_n(R)$?
There is a formula for $A^{-1}$ when $R = \R,\C,\Q$.
If $A = (a_{ij})$ and $B = (b_{ij})$ where $b_{ij} = (-1)^{i+j}\det{A_{ij}}$
($A_{ij}$ is deleting the $i$th row and $j$-th column),
then $AB = BA = \det{A}\begin{pmatrix} 1 & \cdots & 0 \\ \vdots & \ddots & \vdots\\
0 &\cdots &1\end{pmatrix} =
\begin{pmatrix} \det{A} & \cdots & 0 \\ \vdots & \ddots & \vdots\\
0 &\cdots &\det{A}\end{pmatrix}$
so $A^{-1} = ff$.
Now, the trouble is that in linear algebra,
they don't tell you what a determinant is,
only how to compute it.
So we will use this definition of the determinant:
\begin{definition}[Determinant]
	if $R$ is commutative and $A$ is the $n \times n$ matrix with entries $a_{ij} \in R$, then
	\[
		\det{A} := \sum_{\sigma \in S_n} (-1)^{\mathrm{sgn}(\sigma)}
		\prod_{i=1}^n a_{i \sigma(i)}
	\]
\end{definition}
We can see if $n = 2$ and given
$\begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}$,
we ff

Given $A = (a_{ij})$, can define $B = (b_{ij})$ as
$b_{ij} = (-1)^{i+j}\det{A_{ji}}$ also makes sense,
so $AB = \det(A)I$ is true!

How can we prove this? Well, we saw $n = 2$, and could see an inductive proof.
But we will go about it in a different way using the properties of the determinant.
$\det(A)$ can be thought of as a function of the $n$-rows of
$A = \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix}$ where
$v_i$ are row vectors.
Check: swapping $2$ rows sends $\det(A) \to -\det(A)$;
adding a multiple of one to another doesn't change $\det(A)$;
multiplying a row by a constant scales $\det(A)$ by the same constant.
Now we can show there's a unique function (up to scalar) that satisfies this set of properties,
and our defined $\det$ satisfies these properties.
Finally, for real matrices, can use the transformations $1,2,3$ to put $A$ in reduced echelon from
to compute our typical formula for the $\det$ that way.

Note for those taking differential geometry,
this is an example of an exterior product.

Note we haven't done anything for the inverse, but we have just looked at the determinant.

This stuff is discussed somewhat in the book, 2.3.
But Nike will say more about this stuff next time.

\section{January 11}
\subsection{Determinants}
Three determinant properties ff (check overleaf and get notes from Sushrut)

Claim: these 3 properties determine $\det$ uniquely.
Observation: if $2$ rows are the same, then $\det = 0$.
Let $v_i = \begin{pmatrix} a_{i1} & a_{i2} & \cdots & a_{in} \end{pmatrix}$,
and $e_i$ be the $i$-th coordinate vector $\begin{pmatrix} a_{i1} & \cdots & 1 & \cdots 0 \end{pmatrix}$.
We note that $v_i = \sum{j=1}^n a_{ij}e_j$.
Then $\det(A) = \det(\sum a_{ij} e_j) = \det\begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix}$.
Using the linear property (?)
\begin{equation}\label{funky det}
	\det\begin{pmatrix} a_{11}e_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix}
	+ \det\begin{pmatrix} a_{12}e_2 \\ v_2 \\ \vdots \\ v_n \end{pmatrix}
	+ \cdots +
	\det\begin{pmatrix} a_{1n}e_n \\ v_2 \\ \vdots \\ v_n \end{pmatrix}
\end{equation}
And then repeat in the second row, and third row, etc.
The only terms that will survive have the formula
$\det\begin{pmatrix} e_{\sigma(1)} \\ e_{\sigma(2)} \\ \vdots \\ e_{\sigma(n)} \end{pmatrix}$
where $\sigma \in S_n$ and the coefficient (??) is the product of the a's.
We have
\begin{align*}
	\det\binom{ae_1 + be_2}{ce_1+de_2}
	&= \det\binom{ae_1}{ce_1 + de_2} + \det\binom{be_2}{ce_1+de_2}\\
	&= \det\binom{ae_1}{ce_1} + \det\binom{ae_1}{de_2}
	+ \det\binom{be_2}{ce_1} + \det\binom{be_2}{de_2}\\
	&= ad\det\binom{e_1}{e_2} - bc\det\binom{e_1}{e_2}\\
	&= ad-bc
\end{align*}

Expansion of $\det$ in rows and columns follows from (\ref{funky det}) (check).
Formula for inverse, adjugate etc. similar proof (check),
so $AA^* = \det(A)I$ is always true,
where $A^*$ (cofactor/adjugate) is made from minors.
If $\det{A}$ has an inverse in $R^*$,
then $A^{-1}$ exists in $M_n(R)$.
It is also true that $\det(AB) = \det(A)\det(B)$ in general,
and also follows from the three properties of the $\det$.

These basic facts are summarized on page 95 and 96 (but probably don't do this expansion).

\subsection{Ideals, quotients, and homomorphisms}
\begin{definition}[Ring homomorphism]
	If $R$ and $S$ are rings, a map $f \colon R \to S$ is
	called a proper \emph{homomorphism}
	if $f \colon (R,+) \to (S,+)$ is a homomorphism of gruops,
	$f(xy) = f(x)f(y)$,
	and $f(1_R) = 1_S$.
\end{definition}
Note the last condition: it is not free (monoid homomorphism).
Basically, as before, this lets us do algebra in $S$ the same as in $R$.
Not $f(ax + ay) = f(ax) = f(ay) = f(a)f(x) + f(a)f(y) = f(a)(f(x) + f(y))$.
\begin{definition}[Kernel of ring homomorphism]
	$\ker(f) := \{x \in R \mid f(x) = 0_S\}$.
\end{definition}
So $\ker(f)$ is an additive subgroup.
But multiplicatively, this is a little weird, not a monoid.
Let $I = \ker(f)$.
Note if $y\in R$, $x \in I$, then $yx, xy \in I$
since $f(yx) = f(y)f(x) = f(y)\cdot 0 = 0$.
So $I$ is closed under multiplication by $R$.
Note, if $1_R \in I$, then $y \cdot 1_R \in I$ and so $y \in I \forall y$,
which means $f(y) = 0 \forall y \implies f(1_R) = 0$ which
is not allowed for a proper homomorphism.
So $1_R \not\in I$ always.
Hence, $I$ is \emph{not} a subring of $R$: there is no multiplicative identity.

Note: we almost never consider the trivial ring in our statements.
We want $1 \neq 0$, so $1$ is invertible, and a lot of other nice things.
Without excluding, a lot of our statements about rings become trivially false.

\begin{definition}[Ideal]
	A (proper) \emph{ideal} in a ring is a subgroup $I \subsetneq (R,+)$
	such that $\forall y \in R, \forall x \in I$, $yx,xy \in I$.
\end{definition}
\begin{definition}[Quotient ring]
	Let $R$ be a ring and $I \subset R$ be a proper ideal.
	The \emph{quotient ring} is the set of coset $R/I$ (under $+$)
	where multiplication is $(x+I)(y+I) = xy + I$
	(identity is $1 + I$).
\end{definition}
Let us check that this is well-defined:
representatives for $x + I$ and $y + I$ are $x + i_1$,$y+i_2$ where $i_1,i_2\in I$.
Then $(x + i_1)\cdot(y + i_2) = xy + xi_2 + i_1y + i_1i_2 \in xy + I$.
So the multiplication is well-defined (?? check later... do we need
set inclusion the other direction? but definition?)

Example: Let $S$ be any ring, and $R = \Z$.
Define $f \colon \Z \to S$ by $f(1) = 1_S$, $f(n) = (1_S + \cdots + 1_S) = n1$,
and $f(-n) = -(1_S + \cdots + 1_S)$.
It is obvious this is a homomorphism (exercise).
This is called the \emph{canonical} homomorphism $f \colon \Z \to R$.
(Note this is the only way to map $\Z$ to $R$.)
There are 2 kinds of rings:
\begin{itemize}
	\item $f$ injective, then $f(\Z) \subseteq R$ and is isomorphic to $\Z$.
		We say that it has $\mathrm{char}(R) = 0$.
	\item $f$ is not injective, then $f$ contains a quotient of $\Z$
		so $R$ contains $\Z/n\Z$ for some $\min{n}>1$ characteristic in $\ker$.
\end{itemize}
So either $R \supseteq \Z$ (via $f$) or $R \supseteq \Z/n\Z$ for some $\min{n} > 1$ (via $f$).

In $\mathrm{char}(n)$,
$f(n) = (1_S + \cdots + 1_S) = 0$ be definition.
Then $nx = x + \cdots  x - x(1 + \cdots 1) = x\cdot0 = 0$.
So ``multiplication by $n$" means $0$ in rings of characteristic $n$.

Note that if we have $\mathrm{char}(2)$, then $1_S + 1_S = 0$,
so $x + x = 0 \forall x$, and so $x = -x$ (even when $x \neq 0$).
This is not nice, we don't like $1$ being its own inverse:
this is why a lot of things in number theory say ``consider all odd primes".

If $n = p =$ prime and $x,y$ commutative, then
\[
	(x+y)^p = \sum\binom{p}{i}x^iy^{p-i} = x^p + y^p
\]
since $\binom{p}{i}$ is divibisible by $p$ if $0 < i < p$.
Frobenius transform (??) $f \colon R \to R$, $x \mapsto x^p$ is a homomorphism
for commutative $R$ for $\mathrm{char}(p)$;
important in number theory.

\begin{definition}[Ideal generated by a set]
	Let $R$ be a ring and $\{x_j\}_{j \in J}$ be a collection of elements in $R$.
	The ideal generated by $J$ is the set of combinations of the form
	\[
		\sum Rx_jR
	\]
	which are combinations of $\alpha x_j \beta$, $\alpha,\beta\in R$
	(might be $R$ and not proper).
\end{definition}
Note proper ideals don't contain units (invertible elements).

If $I,J$ are ideals, then $I \cap J$ is an ideal,
$I + J = \{ i + j \mid i \in I, j \in J\}$ is an ideal (not neccesarily proper).
$I \cap J \supseteq IJ = \{ij \mid i \in I, j \in J\}$ is an ideal.

In general, if he gives us some random ring,
a hard problem to find the ideals in it.
Will usually study more simple properties in this class.
There is work in classyfing rings and their ideals.

All in section 2.5 and 2.6.
Will continue next time and briefly touch on homomorphism theorems,
same as before (read it).

\section{January 16}
Wrapping up stuff from last time.
\begin{enumerate}
	\item The $\det$ when the characteristic is $2$.
		We assume last time $1 \neq -1$.
		But we can actually just reword things.
		Recall $\det(A) = \sum_{\sigma \in S_n}\mathrm{sgn}(\sigma)\prod a_{i\sigma(i)}$.
		If row $i$, row $j$ are the same, then $\sigma$ term
		is the same as the term for $\sigma \tau$, $\tau = (ij)$.
		So each term appears twice, which is $0$ in $\mathrm{char} = 2$ as well.
	\item Define ideals as sets (additive subgroups) that are
		multiplicatively closed by elements of $R$ on both sides:
		$rI \subseteq I, Ir \subseteq I, r \in R$.
		Note, we also hvae left and right ideals, i.e. $rI \subseteq I$
		or $Ir \subseteq I$, however,
		these are not kernals of homomorphisms.
		But they will make an appearance studying noncommutative rings.
		If you want to make quotients, need two-sided ideals.
\end{enumerate}
Note, if $R$ is a commutative ring,
$R$ is a field $\iff$ there are no nontrivial ideals.
\begin{proof}
	Suppose $R$ is a field, $I \subset R$ is an ideal, and $I \neq \{0\}$.
	If $a \in I$, $a \neq 0$.
	$a^{-1} \in R$ (since it is a field), so $1 = a^{-1}\cdot a \in I$
	hence $I = R$.

	If $R$ has no zero ideals, then $R$ is a field.
	Pick $a \neq 0$, $aR = $ ideal,
	so $x(aR) = xaR = a(xR) \subseteq aR \implies aR = R \implies$
	there is a $b$ with $ab = 1$.
\end{proof}
\begin{corollary}
	If $R$ is a field, $f \colon R \to S$ a homomorphism,
	then $f$ is injective
\end{corollary}
\begin{proof}
	$\ker f = \{0\}$
\end{proof}

\subsection{Principal Ideals}
Let us assume that $R$ is commutative.
$\forall a \in R$, $aR$ is an ideal.
This is called a principal ideal generated by $a$.
$aR = R \iff a$ is a unit.
In general, $a$ not a unit $\implies$ $aR \subsetneq R$ (a proper ideal).
Example: $R = \Z$, $R/aR \cong \Z/a\Z$.

Let $R = \R[x] = $ polynomials with real coefficients.
Let $a = x^2 + 1$ and $I = aR = $ multiples of $x^2 + 1$.
Claim is that $R/I \cong \C$.
\begin{proof}
	Pick $p(x) \in \R[x] = R$.
	Using long division of polynomials
	$p(x) = \underbrace{q(x)(x^2+1)}_{\in I} + \alpha x + \beta$
	``Long division of polynomials is something everyone should be able to do.
	It's like long division of numbers, but worse."

	We want to show the cosets of $R/I$ are
	labelled by $\alpha I + \beta$, $\alpha,\beta \in \R$ (bijective correspondance).
	See $\alpha x + \beta + I = \alpha' x + \beta' + I
	\implies \alpha = \alpha', \beta = \beta'$
	since $\alpha x - \alpha'x + \beta - \beta' \in I
	\implies \alpha''x + \beta'' \in I = (x^2+1)R$
	and we have a linear equaling a quadratic, so $\alpha''=\beta''=0$.

	Multiplication: $(\alpha x + \beta)\cdot(\alpha'x + \beta')
	=$ coset of $gh$ (definition) $ = $ coset of
	$\alpha\alpha'x^2 + \alpha\beta'x + \beta\alpha'x + \beta\beta'
	\equiv -\alpha\alpha' + \alpha \beta'x + \beta\alpha'x + \beta\beta'$
	since $x^2 + 1 \in I \implies x^2 = -1 + I$.
	But this looks like multiplication in $\C$.
	In particular, $(x+I)(x+I) = -1$, so $(x+I) = i$ since $i^2 = -1$.
	And $\R[x]/I$ contains $\R$ via $0x+\beta$.
\end{proof}
$\C = \R + i\R$, $i^2 = -1$,
where $(\alpha i + \beta)(\alpha'i + \beta') = $ same formula as before.
So we have recovered $\C$ (``the correct definition of $\C$"):
\[
	\C = \frac{\R[x]}{I}, \quad I = (x^2+1)\R[x]
\]
Notation: $(a) = aR = $ the principal ideal generated by $a$.

Ex. $\Q[x]$ and $I = (x^3 - 2)$.
Cosets are repsented by polynomials of degree $\leq 2$, $ax^2 + bx + c$
(long divison).
We have $(ax^2+bx+c)(a'x^2+b'x+c) = aa'x^4+(\dots)x^3+ \cdots
= aa'x^3x + (\dots)x^3 + \cdots$,
have to do long division on this to get a quadratic representative.
We have $x^3 - 2 \in I \implies 2 + I = x^3 + I$.
In $R/I$ we have $\overline{2} = \overline{x}^3$.
So our polynomial becomes $aa'(2)x + (\dots)2 + \cdots$.
We get $\Q$ with a solution of $x^3 - 2 = 0$ i.e. $\sqrt[3]{2}$???
Note $\Q \hookrightarrow \Q[x] \to \Q[x]/I$
(the first map is to the constant polynomial).
Note that since the whole map is injective, we have that $\Q[x]/I$ contains $\Q$.
So we have enlarged $\Q$ by adding solutions to an equation which
did not have solutions in $\Q$.
We have an algebraic definition $\sqrt[3]{2}$, namely
$\overline{x}$ with $(\overline{x})^3 = 2$.
So technically, we think of all cube roots of $2$ as the same.
It's just some symbol.
It isn't until Galois theory that we might consider them different,
because then we're thinking of the symmetries of our roots,
and we'll see taking the complex conjugate fixes $1$ root and swaps $2$,
so there is a difference; but not right now.

Again, let $R = \Q[x]$.
Consider $f \colon R \to \C$ by $p(x) \mapsto p(\alpha)$, $\alpha \in \C$ is fixed.
Is this injective? What is $\ker(f)$?
Well, $\ker(f) = \{p(x) \mid p(\alpha) = 0\}$.
Theorem (deep): $\ker(f) = 0$ for almost all $\alpha$.
We need $\alpha$ to be a root of this polynomial.
There are only countably many $\alpha$ for which $\ker \neq 0$ nonzero
($\Q[x]$ is countable, but $\C$ is not).
These $\alpha$ are called \emph{algebraic}.
Generic $\alpha$ are called \emph{transcendental}:
not the root of any rational equation.
Despite the fact that almost all numbers are transcendental,
it is quite hard to prove that a specific transcendental.
$e, \pi, \dots$ are transcendental, but no straight forward proof,
and most numbers you could think of are algebraic.
(Note that zeros of $\Z[x]$ are the same: just scale by leading coefficient;
if monic polynomials, so leading coefficient is $1$, different and called algebraic integers).
\begin{remark}
	$\ker(f)$ does not determine $\alpha$.
	E.g. $\alpha_1 = \sqrt[3]{2} \in \R$,
	$\alpha_2 = \xi\sqrt[3]{2}$, $\alpha_3 = \xi^2\sqrt[3]{2}$
	where $\xi = e^{2\pi i/3}$.
	The kernel in all $3$ cases is $I = (x^3 - 2)$ (not obvious).
	We will get to this in more detail, something about polynomial irreducible.
\end{remark}
\begin{proof}
	Suppose $p(x)$ is such that $p(\alpha_i) = 0$.
	$p(x) = q(x)(x^3 - 2) + r(x)$ where $r(x) \in \Q[x]$ has degree $\leq 2$.
	$p(\alpha) = 0 \implies q(\alpha)\cdot 0 + r(\alpha) = 0
	\implies r(\alpha) = 0$.
	Have to check that none of these $\alpha$ satisfy rational
	polynomials of degree $\leq 2$
	(details are an exercise: can't be linear,
	and if quadratic, need them to be conjugate, but will see not in $\Q[x]$).
\end{proof}
Generic technique in number theory.
Start with a polynomial, and make a quotient ring, etc.

Note the fundamental theorem of algebra says roots of $\C[x]$
are in $\C$ (``algebraically closed").
But recall from our homomorphism, our roots of $\Q[x]$ are in $\C$.

Also note that behind all of this is the assumption
we are in a field of characteristic $0$.
If we don't have this, long division becomes a bit more complicated,
but we will talk about this later.

\subsection{Fundamental theorem of homomorphisms}
Same as for gruops (preimage, etc.).
Read it in the book (2.7).

\subsection{Fractions}
How do we get from $\Z$ to $\Q$.
What is the construction $\Z \rightsquigarrow \Q$?
Perhaps we define it by $\{\frac{a}{b} \mid a,b \in \Z, b \neq 0\}$
Problem is that $\frac{a}{b}$ is not a unique representative!
What if we try $\gcd(a,b) = 1$.
But this assumes existence of gcd $\iff $ unique factorization of $\Z$ (nontrivial).
Better: $\frac{a}{b} = \frac{c}{d} \iff ad = bc$.
\[
	\Q := \{(a,b) \mid a,b \in \Z, b \neq 0\}/\sim
\]
where $(a,b) \sim (c,d) \iff ad = bc$.
Check: rules of arithmetic apply (postpone for now).
``When you're doing fractions in grade 4, assuming unique factorization."
``One of the earliest times I realized I liked math
was when someone told me that we are using unique factorization
in our definition of fractions."

Goal: $R$ is commutative, integral domain (no zero divisors),
construct the ``smallest" field containing $R$ ($\Z$ gives $\Q$).
Let $\Q := \{(a,b) \mid a,b \in \Z, b \neq 0\}/\sim$
where $(a,b) \sim (c,d) \iff ad = bc$,
which contain rules of arithmetic.
\begin{theorem}
	$S$ is a field, $\exists! \iota \colon R \hookrightarrow S$.
	If $T$ is any other field with $j \colon \R \hookrightarrow T$,
	then $\exists! f$ which makes
	\[
		ff commutative diagram
	\]
	(basically $f \colon \iota = j$ and $f \colon S \to T$;
	$f$ is injective because any map between two fields has $\ker(f) = 0$).
\end{theorem}
This is the universal property that defines the ring of fractions.

Thursday quiz will be up until ideals.
Review linear algebra: matrices will be on the quiz.
Quiz will probably be the second half of the class.

\section{January 23}
Because of the snow and bus strikes, quiz is just gonna be the same
day as quiz 2, unfortunately after add/drop deadline;
it will be the full class.

\subsection{Fractions over a commutative domain}
Recall we were look at fractions last time.
Let $R$ be a commutative, integral domain.
Want to embed $R$ into the smallest possible field $S$.
Start with $R \times R$, consider pairs $(a,b)$, $b\neq 0$
modulo the relation $(a,b) \sim (c,d) \iff ad = bc$
(motivation: $\frac{a}{b} = \frac{c}{d}$).
It is easy to check this is an equivalence relation.
Then $S = R' \times R' /\sim$ (second coord nonzero subset),
which are the pairs $(a,b)$, $(0,0)$ is the class of $(0,b)$??? I thought we excluded.
Case work is exactly like fractions:
$(a,b) + (c,d) = (ad+bc,bd)$ etc.
$\frac{a}{b}\cdot\frac{c}{d} = \frac{ac}{bd}$, $1 = (1,1)$, $0 = (0,0)$.

Ex. $(a,b) \sim (p,q) \iff aq = bp$ and $(c,d) \sim (r,s) \iff cs = dr$.
$(a,b) + (c,d) = \frac{ad+bc}{bd} = (ad+bc,bd)$
and $(p,q) + (r,s) = \frac{ps+rq}{qs} = (ps+rq,qs)$... are these equal?
$ff$ computing.

Note each nonzero elemenet is invertible, namely $(a,b)^{-1} = (b,a)$.
So $R \hookrightarrow S$ via $(a,1) \leftrightarrow a \in R$
thus we have found a field $S$ plus an injective map $R \hookrightarrow S$.
Claim: if $\phi \colon R \hookrightarrow T$ is an injective map,
$T$ a field, then there exists a unique $\psi \colon S \to T$ such that the diagram
ff ($R$ to $S$ via $\iota$, $S$ to $T$ via $\psi$, $R$ to $T$ via $\phi$)
commutes.
\begin{proof}
	It is clear that if $\psi$ exists then $\psi(a,1) = \phi(a)$.
	What about $\psi(a,b)$?
	Note $(a,b) = (a,1)\cdot(1,g) \implies \psi(a,b)$ has to satisfy
	$\psi(a,b) = \psi(a,1)\cdot\psi(1,b) = \phi(a)\phi(b)^{-1}$
	since this is an element that satisfies the multiplicative relation
	$(b,1)(1,b) = (b,b) = (1,1)$, the multiplicative identity.
	So $\psi(1,1) = 1 \iff \psi(b,1)\psi(1,b) 1$,
	hence $psi$ is defined uniquely by $\psi(a,1) = \phi(a)$,
	$\psi(1,b) = \phi(b)^{-1}$ (which exists since $T$ is a field).
	We have to check this is a well-defined homomorphism, but it is an easy calculation.
\end{proof}
Thus, $S$ is the smallest field containing $R$.

If $R = \Z$, then $S \cong \Q$.
If $R = \Z + \Z[i]$, $S \cong \Q[i]$,
where $(a+bi)^{-1} = \frac{a-bi}{a^2+b^2} \in \Q[i]$.
\begin{remark}
	$S$ is an abstract field, not necessarily subfield of $\C$
	just because we think of $R$ as a subfield of $\C$.
	But since there is some $\phi$ from $R \hookrightarrow \C$
	(so $R$ lives inside $\C$),
	then the commutative diagram says that $S$ lives inside $\C$.
\end{remark}
$\psi$ need not be injective (?),
but $\phi$ is minimal because it comes from $\psi \circ \iota$...
the existence and uniqueness requires $\psi$ or $\phi$??? to be injective...
worth pondering.
But anyway, we get for free that these are injective because mapping into fields,
and any homomorphism into fields is injective.

\subsection{Polynomial rings}
What is a polynomial, e.g. with real coefficients?
In calculus, we think of it as a function $f \colon \R \to \R$ where
\[
	f(X) = \sum_{n=1}^m a_nX^n = a_mX^n + \cdots + a_0, \, a_i \in \R
\]
We pick some $X \in \R$, we get value $f(X) \in \R$ and can get a graph.

This is \emph{not} what a polynomial is in algebra!
Polynomials are not functions.
Let $R = \Z/3\Z$.
Think of the function $X^2 \colon R \to R$.
Then $0 \mapsto 0$, $1 \mapsto 1$, $2 \mapsto 1$.
What about $X^3$?
then $0 \mapsto 0$, $1 \mapsto 1$, $2 \mapsto 2$.
So as a function, $X^3$ and $X$ are the same!
This does not look like a good thing:
we want the notion of the degree of a polynomial to be somewhat sensible,
but if we just think of this as a function, we don't really get this.
So we \emph{cannot} think of polynomials as functions.

$X$ is not a variable, with some domain.
For us $X$ is just a placeholder.
So if $R$ is commutative, we define
\begin{definition}[Polynomial with coefficients in $R$]
	A polynomial with coefficients in $R$ (commutative) of degree $m$
	is a sequence $(a_0,a_1,\dots,a_m) \in R \times R \times \cdots \times R$
	($m+1$ times) with $a_m \neq 0$.
	Morally: $a_mX^m + \cdots + a_0 = f(X)$
	(we use the $X$ notation, with the provision that $X$ is a symbol).
\end{definition}
The space of polynomials with coefficients in $R$ is the set
of bounded sequences $(a_0,a_1,\dots)$ meaning $a_r = 0$
for all $r$ sufficiently large (only finitely many nonzero terms).

We have to make this into a ring.
We do addition via coordinates: $\sum a_iX^i = \sum b_iX^I = \sum(a_i+b_i)X^i$.
Our multiplication is done how we would expect:
$(a_0,\dots)(b_0,\dots) = (c_0,\dots)$ or
$(\sum a_iX^i)(\sum b_i X^i) = \sum c_iX^i$
which is obtained by multiplying LHS and collecting powers of $X$,
$c_n = \sum_{p+q=n} a_p b_q$.
This defines the multiplicative identity $1 = (1,0,\dots) = 1X^0$.
We can check distributive, but it is routine.
This comes with an injective map $\iota \colon R \to $ Polynomials
$ = R[X]$ by $r \mapsto (r,0,\dots)$.
So $R =$ constant polynomials ($R$ is a ring of constants).

The fundamental property of $R[X]$:
suppose we are given a homomorphism $\phi \colon R \to S$ for
some ring $S$ ($S$ is unrestricted).
We can think of $\phi(r) \in S$; this means constants are moved inside $S$
($\phi$ may not be injective).
Now, if $s \in S$ is any element, $f(X) \in R[x]$ is a polynomial,
we can define the value of $f$ at $X = s$ via
$a_0 + a_1X + \cdots + a_mX^m \mapsto f(s) = \sum\phi(a_i)s^i \in S$.
We get a homomorphism (!) $R[X] \to S$, $X \mapsto s \in S$
(! is both surprising and unique).

We are allowing the domain of our function to really be anything that
can be mapped to from $R$ via a homomorphism (otherwise we don't end with a homomorphism);
unlike calculus, which normally restricts by what the coefficients were.
Now domain is very dependant on $\phi \colon R \to S$.
By not specifying what $X$ is, it represents any ring.

The fundamental example is $R = \Z$, $f(X) \in \Z[X]$.
Let $S$ be any commutative ring that comes with a homomorphism $\Z \to S$,
thus for any $s \in S$, we have an evaluation map $\Z[X] \to S$
by $X \mapsto s \in S$.
In particular, this applies when $S = \Q, \Z/p\Z$ for $p$ prime.
So a polynomial with integer coefficients can be evaluated at rational numbers,
we well as any $x \in \Z/p\Z$ for any $p$.
This is weird: we don't normally think of polynomials like this,
normally evalue at $\R$.
The study of these polynomials is algebraic geometry.
Big themes in modern number theory is if we think of a polynomial with these extended values,
we can patch together the values on $\Z/p\Z$, we can say something about the values in $\Q$.
This is a subject called arithmetic geometry.

Last time, we saw examples of this when $f(X) \in \Q[X]$,
and we can evaluate this at $\alpha \in \C$,
and we get a homomorphism $\Q[X] \to \C$ where $X \mapsto \alpha$.
If it was injective, then $\alpha$ is transcendental,
if it had a nonzero kernel, then it was algebraic,
i.e. there exists some $f(X)$ in $\Q[x]$ with $f(\alpha) = 0$ ($f \in \ker$)
and $\alpha$ satisfies some algebraic equation $f(X) = 0$.

Ex. $R = \Q[X]/(f(X))$ where $I = f(X) =$ multiples of $f(X)$.
If $R$ a field, then $\exists \phi \colon R \to T \iff
\exists t \in T$ such that $f(t) = 0$.
Reason: $f(X) = \sum a_nX^n \in I$.
In $R/I$ have $\sum a_n \overline{X}^n = 0$
($\overline{X}$ is the image of $X$ in $R$).
So if there's a homomorphism $\psi \colon R \to T$
then $t = \psi(\overline{X})$ has to satisfy $\sum \psi(a_n)t^n = 0$.
Think about this a little, says something about roots of polynomials.
Near the guts of Galois theory.
Read up in the textbook, and we will talk about it next time.

\section{January 25}
\subsection{More polynomials}
In all of this, $R$ is a commutative ring, and $R[x]$ are polynomials in $X$.
Polynomials will be identified as a sequence of coefficients.
This $X$ is not anything other than a symbol,
just keeps track of multiplication and addition.

The fundamental property we discussed last time:
given some $\phi \colon R \to S$ which is a homomorphism,
and some $s \in S$, then $\exists!$ extension of $\phi$ to $R[x] \to S$
such that $X \mapsto s$.
This is the evaluation homomorphism at $s \in S$.
This is given by $\sum_i a_i X^i \mapsto \sum \phi(a_i)s^i$.

This is not as weird as it looks.
For example, $\phi \colon \Q \hookrightarrow \R$,
think of rational coefficients as complex numbers.
Can also take $\phi \colon \Z \to \Z/p\Z := \mathbb{F}_p$:
in this case, if $p = 5$ and $f(X) = X^2$ and $s = [2]$,
then $1 + X \mapsto 1 + [2] = [3]$ (wait, why did he say $f(X)$?).
So we evaluate $f(X)$, then reduce mod $p$.
Another example: $\phi \colon \C \to \C$ where $z \mapsto \bar{z}$.
So $\sum a_i X^i \mapsto \sum \bar{a}_iX^2$.
so $\phi$-evaluation at $s$ gives $\sum \bar{a}_i s^i$.

You have to be careful you know what $\phi$ is in the background.
Consider $R = \Q[X]/I$ where $I = (X^2+1)$.
By long division, our ring is just $\Q + \Q x$ where $x = X + I$.
Multiplication comes $0 = x^2 + 1 = (X+I)^2 + 1 = X^2 + 1 + I = I$
(what is this showing??)
To get $\phi$ from $R \to \C$ we have to send $x$ to some $s \in S$ such that $s^2 + 1 = 0$.
We have $x^2 + 1 = 0 \in R$ so $\phi(x)^2 + \phi(1) = 0$ in $\C$,
so we need $s \pm i$ (both are valid)...
(something something about the ideal, I'm confused what he is showing again).
We can also send $\Q[X] \hookrightarrow \C$ with $X \mapsto \pi$ or $X \mapsto e$;
since these are transcendetal, not the root of any polynomial,
so not in any ideal.
So $\ker = 0$ in both cases.

\subsubsection{More variables}

We can extend this to more than $1$ variable.
Start with $R$, form $R[X] = S$.
From $S[Y] ``=" R[X,Y]$,
because $S[Y] = \sum a_i Y^i$ where $a_i \in R[X]$.
Expand out to get $\sum a_{ij} X^iY^j, a_{ij} \in R$.
We an repeat to get $R[X_1,\dots,X_n]$.
Fact: if $\sigma \in S_n$ is any permutation,
then $\exists!$ $\phi \colon R[X_1,\dots,X_n] \to R[X_1,\dots,X_n]$
where $X_i \mapsto X_{\sigma(i)}$, which is the identity on $R$ (constant terms don't change...
or coefficients don't change???).
Proof: in the text (symbole pushing + uniqueness of $1$ variable polynomial maps).

Same universal property as before:
given $\phi \colon R \to S$, $s_1,s_2,\dots,s_n \in S$, $\exists!$ extension
$R[X_1,\dots,X_n] \to S$ which sends $X_i \mapsto s_i$.
Same as before, just evaluating an $n$-tuple.

Note the variables $X_i,X_j$ commute, in the definition (is it??).

Multivariable polynomials look a little odd.
The monomoial of (multi) degree $\vec{a}$ is
$\sum c_{\vec{a}}\prod X_1^{a_1}X_2^{a_2}\cdots X_n^{a_n}$
where $\vec{a} = (a_1,\dots,a_n)$ is an $n$-tuple of non-negative integers,
and $c_{\vec{a}} \in R$.
The (total) degree of $\vec{a}$ is $\sum a_i$.

It is quite helpful to look at the proofs in the book to understand
how proofs work with these polynomials.
For example, $X^2 + 2XY + Y^2 = X^1(2Y) = X^0Y + X^21 \in R[Y][X]$.

\subsubsection{Division of one variable polynomials}
Let's stick to the case of one variable for now.
\[
	f(X) = \sum_{i=0}^n a_i X^i, \quad a_n \neq 0
\]
$a_n$ is called the leading coefficient.
$f(X)$ is called \emph{monic} if $a_n = 1$.
$n$ is called the degree of $f$,
$\mathrm{deg} \colon R[X] \to $ non-negative integers.

Fact: if $R$ is a domain, $\deg(f) + \deg(g) = \deg(fg)$,
since $(a_nX_n + \cdots)(b_mX^m + \cdots) = a_nb_mX^{n+m} + \cdots$
lower coefficient, and $a_nb_m \neq 0$ since it is a domain.
\begin{corollary}
	If $D$ is a domain, so is $D[X]$ and $D[X_1,\dots,X_n]$.
\end{corollary}

Now we come to long division in a ring.
Assume that $R$ is a domain and commutative as always.
Given $f(X), g(X)$ in $R[X]$, want to divide $f(x)$ by $g(x)$
and get a remainder of degree less than $g$.
We want  $f(X) = q(X)g(X) + r(X), \deg(r) < \deg(g)$.
This is \emph{not} always possible.
E.g. $R = \Z, f(X) = X^3 + X +1, g(X) = 2x + 1$.
Then $q(X)g(X) = (2X+1)(a_mX^m+\cdots) = 2a_mX^{m+1} + \cdots$.
$\deg(r) = 0$ is $r(X)$ is a constant.
So $q(x)g(x) + r(x)$ has a leading cofficient greater than $1$,
so not equal (Napkin does it again).
It will be possible in a field.

What is true in general is the following:
\begin{theorem}[Long Division]
	Let $R$ be a commutative ring, and $f,g \in R[x]$.
	Let $g(X) = b_mX^{m} + \cdots + b_0, b_m \neq 0$.
	Then $b^k_mf(X) = q(X)g(X) = r(X)$ where $\deg(r) < \deg(g)$
	(not necessarily unique), for some $k$.
\end{theorem}
Don't need to be in a domain, but probably need it for uniqueness.

Ex. $X^3 + X + 1 = f(X)$ and $g(X) = 2x + 1$.
$2f(X) = 2X^3 + 2X + @$,
find $q(X)$ that makes $q(X)g(X)$.
We will start with the $2X^3$ term, so $q_1(X) = X^2$ works.
Then $2X^3 + 2X + 2 = X^2(2X+1) + r_1(X)
= 2X^3 + X^2 + (-X^2 + 2X + 2) = q_1(X)g(X) + (-X^2 + 2X + 2)$.
Repeat with $-X^2+2X+2$.
Divide by $g$ again, $2(-X^2 + 2X + 2) = q_2(X)(2X + 2) + r_2$
works out what $r_2$ has to be
(we had the multiply by $2$ again).

That is the division algorithm, and it's kinda a pain,
but we are going to need it.

This is easier in the world of fields, we just divide out by the
leading coefficient to make $g(X)$ monic.
So let $R$ be a field.
In this case, get $f(X) = q(X)g(X) + r(X)$ where $\deg(r)<\deg(g)$,
and $q,r$ are unique.
We can show uniqueness easily:
$q(X)g(X) + r(X) = q_1(X)g(X) + r_1(X)
\implies g(X)(q(X)-q_1(X)) = r_1(X) - r(X)$.
The LHS has $\deg \geq \deg g$ and the RHS has $\deg < \deg g$,
so both $r-r_1$ and $g-g_1$ are zero.
Can see how this breaks: $\deg$ doesn't play nice.
Now, there exists a division algorithm in $k[X]$, $k$ a field,
reduce the degree by division.
There are interesting work in doing multivarible division.
The algorithms are very sensitive to what you consider the degree as.

Consequence of long division:
\begin{theorem}[Factor theorem]
	Suppose $k$ is a field, $f(X) \in k[X]$.
	Then if $\alpha \in k$ is a solution to $f(X) = 0$,
	then $(X-\alpha)$ divides $f(X)$ (remainder $r = 0$),
	and conversely, if $(X-\alpha)$ divides $f(X)$ then $\alpha$ solves $f(X) = 0$.
\end{theorem}
\begin{proof}
	Backwards is obvious: $f(X) = (X-\alpha)g(X) \implies f(\alpha) = (\alpha-\alpha)g(\alpha) = 0$.
	Conversely, suppose $f(\alpha) = 0$.
	(Remark: $f(\alpha)$ by definition is the image of $X$ under $k[X] \to k$, $X \mapsto \alpha$,
	what we began the class with.)
	$f(X) = q(X)(X-\alpha) + \alpha_0$ and $\alpha_0$ is a constant in $k$.
	Set $X = \alpha \implies 0 = 0 + \alpha_0 \implies \alpha_0 = 0$.
\end{proof}

For general $f(X), \alpha \in k$,
have $f(X) = q(X)(X-\alpha) + \alpha_0$, $\alpha_0 \in k$.
Pur $X = \alpha \implies f(\alpha) = 0 + \alpha_0$ then $\alpha_0 = f(\alpha)$
(remainder of division of $f(X)$ by $(X-\alpha)$ is $f(\alpha)$).

\begin{theorem}
	$k$ a field, $R = k[X]$, $I \subseteq R$ any ideal.
	Then $\exists$ a unique monic polynommial $f(X)$
	such that $I = (f(X)) = $ principal ideal generated by $f$.
\end{theorem}
\begin{remark}
	False for multivariable rings
\end{remark}
\begin{remark}
	$k[X]$ is similar to $\Z$ in this sense.
	Principle ideal domains: integral domain in which every ideal is principle.
\end{remark}
What is special about $k[X]$ that you can't do in every PID is that
you can do long division, so we call it a Euclidean ring.

Cutoff for quiz 2 is all of today.
\end{document}
