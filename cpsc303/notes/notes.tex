\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{geometry, hyperref}
\geometry{letterpaper, margin=2.0cm, includefoot, footskip=30pt}

\usepackage{fancyhdr}
\pagestyle{fancy}

\lhead{CPSC 303}
\chead{Notes}
\rhead{Nicholas Rees}
\cfoot{Page \thepage}

\newtheorem*{problem}{Problem}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\theoremstyle{remark}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\ep}{{\varepsilon}}
\newcommand{\SR}{{\mathcal R}}

\renewcommand{\theenumi}{(\alph{enumi})}

\begin{document}
\section{January 8}
\subsection{Logistics}
Within the three body problem is the entirety of this course.

``In mathematics you don't understand things. You just get used to them." - John von Neumann.
(Bro von Neumann is so washed for this.)
For context, von Neumann was challenged that he didn't actually understand
the method of characteristics.
He was talking about this with his Dad, who just retired as a math prof from Ohio State,
and he didn't like it.
Joel's alternate form: ``In mathematics it takes time for ideas and examples to sink in."

\href{https://www.cs.ubc.ca/~jf/courses/303.S2024/index.html}{Click here} for course website link.

Content:
Parts of Chapters 10-12, 14-16 of textbook by Axcher and Greif (online, free).
Topics: Interpolation, approximation (Ch 10 - 12);
Differentiation, Integration, ODE's [PDE's] (Ch 14-16).
Back in the day, 303 was meant as a followup to 302, but not anymore,
so may be some repeated material
(norms and condition numbers, but not thee point of this course anyway).

Discussion: please post to piazza page.
If this fails, please email to jf@cs.ubc.ca with subject CPSC 303.

Grading: $(10\%)\mathrm{max}(h,m,f) + (35\%)\mathrm{max}(m,f) + (55\%)f$
where $h$ is homework, $m$ is midterm, $f$ is final.
So technically, can ace final and ace course.
Because back in the day, Joel knew someone who couldn't attend any classes,
but got 100\% in the final only to get C+ in the course.
There is a phenomena where if someone gets 100 on the midterm,
stops doing homework.
But really, these assessments are good preperation and indicators of where you stand in the course.

Please sign up for piaaza and gradescope through canvas.ubc.ca (especially gradescope).

Homework: Set Thursday 11:59pm and due Thursday 11:59pm.
There is both individual and group homework.
Group homework: at most 4 people, and will cover most material; only submit one.
Individual homework: These are the types of things he wants to make sure everyone can do,
and will be like the things on exams;
you must write up your own solution even if you work with others.

\subsection{Intro to ODE's}
1.2 and 4.2 (norms), 14.2 (differentiation), 16.1 and 16.2 (ODE's).

He typically begins courses with the most difficult stuff the course will get.
This is tough, but not the worst.
Now, this course only requires two terms of calculus,
but with how pertinent ML is now, most people are taking multivariable calc anyway.
We will get some idea of what to expect and some intuition, but will revisit later in the course.
The reason the emphasis is on ODEs and not PDEs is because
the general theory for ODEs really applies to all of them,
even if you have to solve differently.
Families of PDEs have their own properties that have to be studied separately.

We are heading towards Ordinary Differential Equations (Ch. 16).

\subsubsection{Absolute vs. Relative Error}
If $v \in \R$ is an approximation to $u \in \R$,
then absolute error (in $v$) (as an approximation to $u$) is $\lvert u - v \rvert$,
and the relative error is $\frac{\lvert u - v\rvert}{\lvert u \rvert}$.
The same works in $\R^n$ (or any normed vector space):
\[
	\lVert \vec{u} \rVert_2 = \lVert (u_1, \dots, u_n) \rVert_2
	= \sqrt{u_1^2 + u_2^2 + \cdots u_n^2}
\]
We also use $\lVert \vec{u} \rVert_1 = |u_1|+ \cdots |u_n|$
and $\lVert \vec{u}\rVert_{\mathrm{max}} = \lVert \vec{u} \rVert_\infty
= \mathrm{max}_{1 \leq i \leq n} |u_i|$.
The absolute error in $\vec{v}$ as an approximation to
$\vec{u}$ is $\lVert \vec{u} - \vec{v}\rVert_p$
and the relative error is $\frac{\lVert \vec{u} - \vec{v} \rVert}{\lVert \vec{u} \rVert_p}$
where $p = 1,2,\infty$.

\subsubsection{Taylor's Theorem (p. 5)}
\begin{theorem}
	For $f \colon (a,b) \to \R$ where $f$ is $k+1$ differentiable
	(so $f^{(k+1)}(x)$ exists in $(a,b)$),
	for some $x_0$, $x_0 + h$ that lie in $(a,b)$,
	\[
		f(x_0 + h) = f(x_0) + hf'(x_0) + \frac{h^2}{2}f''(x_0) + \cdots
		+ \frac{h^k}{k!}f^{(k)}(x_0) + \mathrm{error}
	\]
	where the error is $\frac{h^{k+1}}{(k+1)!}f^{(k+1)}(\xi)$
	where $\xi$ is between $x_0$ and $x_0 + h$.
\end{theorem}
\begin{remark}
	$h$ can be negative.
\end{remark}

We'll learn how to approximate derivatives, and then ODE solution.
\end{document}
