\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{geometry, hyperref}
\geometry{letterpaper, margin=2.0cm, includefoot, footskip=30pt}

\usepackage{fancyhdr}
\pagestyle{fancy}

\lhead{CPSC 303}
\chead{Notes}
\rhead{Nicholas Rees}
\cfoot{Page \thepage}

\newtheorem*{problem}{Problem}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\theoremstyle{remark}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\ep}{{\varepsilon}}
\newcommand{\SR}{{\mathcal R}}

\renewcommand{\theenumi}{(\alph{enumi})}

\begin{document}
\section{January 8}
\subsection{Logistics}
Within the three body problem is the entirety of this course.

``In mathematics you don't understand things. You just get used to them." - John von Neumann.
(Bro von Neumann is so washed for this.)
For context, von Neumann was challenged that he didn't actually understand
the method of characteristics.
He was talking about this with his Dad, who just retired as a math prof from Ohio State,
and he didn't like it.
Joel's alternate form: ``In mathematics it takes time for ideas and examples to sink in."

\href{https://www.cs.ubc.ca/~jf/courses/303.S2024/index.html}{Click here} for course website link.

Content:
Parts of Chapters 10-12, 14-16 of textbook by Ascher and Greif (online, free).
Topics: Interpolation, approximation (Ch 10 - 12);
Differentiation, Integration, ODE's [PDE's] (Ch 14-16).
Back in the day, 303 was meant as a followup to 302, but not anymore,
so may be some repeated material
(norms and condition numbers, but not thee point of this course anyway).

Discussion: please post to piazza page.
If this fails, please email to jf@cs.ubc.ca with subject CPSC 303.

Grading: $(10\%)\mathrm{max}(h,m,f) + (35\%)\mathrm{max}(m,f) + (55\%)f$
where $h$ is homework, $m$ is midterm, $f$ is final.
So technically, can ace final and ace course.
Because back in the day, Joel knew someone who couldn't attend any classes,
but got 100\% in the final only to get C+ in the course.
There is a phenomena where if someone gets 100 on the midterm,
stops doing homework.
But really, these assessments are good preperation and indicators of where you stand in the course.

Please sign up for piaaza and gradescope through canvas.ubc.ca (especially gradescope).

Homework: Set Thursday 11:59pm and due Thursday 11:59pm.
There is both individual and group homework.
Group homework: at most 4 people, and will cover most material; only submit one.
Individual homework: These are the types of things he wants to make sure everyone can do,
and will be like the things on exams;
you must write up your own solution even if you work with others.

\subsection{Intro to ODE's}
1.2 and 4.2 (norms), 14.2 (differentiation), 16.1 and 16.2 (ODE's).

He typically begins courses with the most difficult stuff the course will get.
This is tough, but not the worst.
Now, this course only requires two terms of calculus,
but with how pertinent ML is now, most people are taking multivariable calc anyway.
We will get some idea of what to expect and some intuition, but will revisit later in the course.
The reason the emphasis is on ODEs and not PDEs is because
the general theory for ODEs really applies to all of them,
even if you have to solve differently.
Families of PDEs have their own properties that have to be studied separately.

We are heading towards Ordinary Differential Equations (Ch. 16).

\subsubsection{Absolute vs. Relative Error}
If $v \in \R$ is an approximation to $u \in \R$,
then absolute error (in $v$) (as an approximation to $u$) is $\lvert u - v \rvert$,
and the relative error is $\frac{\lvert u - v\rvert}{\lvert u \rvert}$.
The same works in $\R^n$ (or any normed vector space):
\[
	\lVert \vec{u} \rVert_2 = \lVert (u_1, \dots, u_n) \rVert_2
	= \sqrt{u_1^2 + u_2^2 + \cdots u_n^2}
\]
We also use $\lVert \vec{u} \rVert_1 = |u_1|+ \cdots |u_n|$
and $\lVert \vec{u}\rVert_{\mathrm{max}} = \lVert \vec{u} \rVert_\infty
= \mathrm{max}_{1 \leq i \leq n} |u_i|$.
The absolute error in $\vec{v}$ as an approximation to
$\vec{u}$ is $\lVert \vec{u} - \vec{v}\rVert_p$
and the relative error is $\frac{\lVert \vec{u} - \vec{v} \rVert}{\lVert \vec{u} \rVert_p}$
where $p = 1,2,\infty$.

\subsubsection{Taylor's Theorem (p. 5)}
\begin{theorem}
	For $f \colon (a,b) \to \R$ where $f$ is $k+1$ differentiable
	(so $f^{(k+1)}(x)$ exists in $(a,b)$),
	for some $x_0$, $x_0 + h$ that lie in $(a,b)$,
	\[
		f(x_0 + h) = f(x_0) + hf'(x_0) + \frac{h^2}{2}f''(x_0) + \cdots
		+ \frac{h^k}{k!}f^{(k)}(x_0) + \mathrm{error}
	\]
	where the error is $\frac{h^{k+1}}{(k+1)!}f^{(k+1)}(\xi)$
	where $\xi$ is between $x_0$ and $x_0 + h$.
\end{theorem}
\begin{remark}
	$h$ can be negative.
\end{remark}

We'll learn how to approximate derivatives, and then ODE solution.

\section{January 10}
Housekeeping:
\begin{itemize}
	\item HW1 will be assigned on Han 11, due on gradescope on Jan 18
	\item Access gradscope via Canvas
	\item Today: Separable ODE's (see, e.g. CLP 2, Appendix D of Ascher and Grief)
\end{itemize}
Last time:
Ch 1: ``Reviewing" termonology.
Ch 4: $\lVert \vec{u} \rVert_2 = \sqrt{u_1^2 + \cdots + u_n^2}$ for $\vec{u} \in \R^n$.
We saw classes of functions, Taylor Series, and ODE's.
An ODE is where the derivatives are a function of the same variable??
So $y' = f(t,y)$.
PDEs on the other hand have partial derivatives $h = h(t,x_1,\dots,x_n)$,
and maybe something like the heat equation
$\frac{\partial h}{\partial t} = - \Delta_{x_1,\dots,x_n}h$.
There might be a course on the foundations of elliptic PDEs, parabolic PDEs, etc.,
but won't be the focus here.

And then he talks about a sketch of proof of Taylor's,
except I literally did it this morning.
Essentially, we use MVT to get better approximations (not rigourously).
We have
\[
	f(x_0 + h) = f(x_0) + hf'(x_0) + \frac{h^2}{2}f''(x_0) + \frac{h^3}{6}f'''(\xi_1)
\]
\[
	hf'(x_0) = f(x_0 + h) - f(x_0) - \frac{h^2}{2}f''(\xi_2)
\]
\[
	f'(x_0) = \frac{f(x_0 + h) - f(x_0)}{h} - \frac{h}{2}f''(\xi_2)
	= \frac{f(x_0 + h) - f(x_0)}{h} + \mathrm{Order}(h)
\]
where $O(h)$ is some function (depends on $f,x_0$).
We have $\lvert \mathrm{Order}(h) \rVert \leq \frac{h}{2} M_2$
where $M_2$ is bound on $f''$ in the interval $[x_0,x_0+h]$.
By definition, $f'(x_0) = \lim_{h\to0} \frac{f(x_0+h)-f(x_0)}{h}$.
All this is really taken from [A\&G], Ch. 14, Sections 1,2.

We will continue our discussion of derivatives, but first, some useful notation.
If $a < b \in \R$,
$C[a,b] := \{ f\colon [a,b] \to \R \text{ such that }f \text{ is continuous}\}$.
When $k \in \N$,
\[
	C^k(a,b) = \{ f\colon (a,b) \to \R
		\text{ such that }f \text{ has } k \text{ continuous derivatives }
	\forall x \in (a,b)\}
\]
,
and similarly for $C^k[a,b]$.
$C^\infty(a,b)$ is the set of $f$ that has derivatieves of all order.
\begin{definition}[Real Analytic Functions]
\[
	C^\omega(a,b) :=
	\{f \colon (a,b) \to \R \text{ such that for all }
	x_0 \in (a,b), \; f(x_0+h) = f(x_0) + hf'(x_0) + \frac{h^2}{2!}f''(x_0) + \cdots\}
\]
A function $f \colon (a,b) \to \R$ is real analytic when $f \in C^\omega(a,b)$.
\end{definition}
Real analytic is a stronger condition than $C^\infty$,
so $C^\omega \subset C^\infty \subset \cdots \subset C^2 \subset C^1 \subset C^0$.
[Hmm... I would like a better definition of convergence here ff]

For example, $e^x$ ff he scrolled.

\subsection{Start ODE}
Simple ODE [A\&G]:
\[
	y' = f(t,y)
\]
where we use the notation $y' = \frac{dy}{dt} = \dot{y}$.
(Caution: math textbooks typically use $y' = \frac{dy}{dx} = f(x,y)$.)

To solve for $y = y(t)$, we are given an ``intial condition",
that is we have $y_o,t_o \in \R$ and impose $y(t_0) = y_0$.

We expect a unique solution. Say we are given $A \in \R$, and find a $y$ that satisfies
\[
	y'(t) = Ay(t)
\]
We claim that $y(t) = e^{At}C$ is a solution.
We can verify: $y'(t) = (e^{At}C)' = (Ae^{At})C = Ay(t)$.
So we have solved it by shamelessly guessing.

We can plug in our $t_0$ and $y_0$ which fixes $C = y_0e^{-At_0}$.
Then
\[
	y(t) = y_0 e^{A(t-t_0)}
\]
So when $A$ is big enough (greater than $0$),
we have exponential growth.

Is this solution unique? Can we simply guess and it provides the only solution? More on Friday.

\section{January 12}
Today:
\begin{itemize}
	\item ODE: $y' = y$ ``isoclines"
	\item ODE's of the form $y' = f(y)$
	\item Later $\vec{y}' = \vec{f}(t,\vec{y})$, system of $m$ ODE's where
	$\vec{y} = \vec{y}(t) \colon \R \to \R^m$
	and $\vec{f}(t,y) \colon \R \times \R^m \to \R^m$.
	\item Examples; $y' = y^2$, $y' = |y|^{1/2}$.
\end{itemize}
Question, what could possibly go wrong?

Solve $y' = f(y)$, $y(t_0) = y_0$. Then
\begin{enumerate}
	\item If $f$ continuous near $y = y_0$ then a solution exists locally
	\item Moreover, if $f$ is Lipschitz (or differentiable) near $y = y_0$,
		the local solution is unique
	\item If $f$ is analytic near $y = y_0$, the unique local solution is analytic
	\item If $|f(y)| \leq Ky$ for some $K$ constant, then there is a global solution
\end{enumerate}

We will start looking at Matlab next week: we will specifically be looking for how it breaks.
He doesn't cover as much content as other people do in 303, but more in depth.
``I just became a grandfather last week, so I get to say back in my day
we always had to make our own software."
So we always knew how it broke.
Now we have to figure out why other people's code breaks.

Recall last time, we were looking at $y' = Ay$, $y(t_0) = y_0$
where $y \colon \R \to \R$.
Isocline picture: slopes at unit points in the $y-t$ plane.
And then we guessed an answer last time: $y(t) = e^{A(t-t_0)}y_0$.
Now, if we had a theorem that said this was unique, we'd be done.

We can solve this with integration:
$y' = y \implies \frac{dy}{y} = dt \implies \int\frac{1}{y}dy = \int dt$,
and so $\ln(y) + c_1 = t + c_2$ hence $y = e^{t+c}$.
Now with $t_0,y_0$, we can determine our constant to get $y(t) = e^{t-t_0}y$.
However, this method is not foolproof.
Our solution could blow up to infinity.
Consider $y' = y^2$ and $y(1) = 1$.
And then $\frac{1}{y^2}dy = dt \implies \frac{-1}{y} = t+c$.
Then, $y= \frac{-1}{t+c}$, and plugging in the initial conditions, we get
$c = -2$.
So $y = \frac{1}{2-t}$.
Singularities can happen, as $y(t) \to \infty$ as $t \to 2$.

What about $y' = |y|^{1/2}$? What could possible go wrong?
When $y > 0$ we have $y' = y^{1/2}$.
Solving in the way we did before, we can get
$y^{1/2} = \frac12(t+C)$
so $y(t) = \frac{1}{4}(t+C)^2$.
Note $y(-C) = 0$... but our slope should always be increasing,
yet is $0$ at a point? ff idk
Now when $y < 0$, we can find (with the method as before)
that $y= -\frac14(t+C)^2$.
So we have piecewise function of $y$ depending on if $y > 0$ or $y < 0$.

Is this a unique solution?
Actually, let $a < b$. Then
\[
	y(t) = \begin{cases}
		\frac14(t-b)^2 & b \leq t\\
		0 & a \leq t \leq b\\
	-\frac14(t-a)^2 & t \leq a \end{cases}
\]
is also a solution.
The bad situation occurs when $y = 0$.
We could stay ``arbitrarily" long at $y = 0$,
even if to the right and left are parabola!
We will continue looking at $y' = |y|^{1/2}$ next time.

\section{January 17}
Today's outline:
\begin{itemize}
	\item Euler's method
	\item MATLAB and Euler's method
	\item Plugging in $y' = Ay, y(t_0) = y_0$
\end{itemize}

\subsection{Euler's Method}
Say we are given $y' = f(t,y)$
(or $y' = f(y)$, or $\vec{y}' = \vec{f}(t,\vec{y})$, etc.)
where $f$ is a function,
and we have) initial value $y(t_0) = y_0$, $y_0,t_0 \in \R$.
We have the approximation
\[
	y'(t) \approx \frac{y(t+h) - y(t)}{h}
\]
for small $h$, since $y'(t) = \lim_{h\to0}\frac{y(t+h) - y(t)}{h}$.
But
\[
	y'(t) \approx \frac{y(t+h) - y(t-h)}{2h}
\]
is a much better approximation (usually).
Rearranging gives $y(t+h) \approx y(t) + hy'(t)$;
in fact, by Taylor's theorem, $y(t+h) = y(t) + hy'(t) + \frac{h^2}{2}y''(\xi)$
for some $\xi$ between $t$ and $t+h$.
Substituting $f$, we have
\[
	y(t+h) = y(t) + hf(t,y) + \frac{h^2}{2}y''(\xi)
\]
So for small $h$,
\[
	y(t+h) \approx y(t) + hf(t,y)
\]
(we will ignore the final term for now, but will be useful later for calculating error).
This last approximation is the essence of Euler's method.

So given $t_0 = $ initial time and $y_0 = $ initial value,
\begin{enumerate}
	\item[Step 1.] Start with $y(t_0) = y_0$.
		[Pick a value of $h$, smaller the better (usually)]
	\item[Step 2.] $y(t_0 + h) = y_0 + hf(t_0,y(t_0)) := y_1$
	\item[Step 3.] $y(t_0+2h) = y_1 + hf(t_1,y_1) := y_2$
		where $t_i = t_{i-1} + h = ih+t_0$.
	\item[Step $n$.] Repeat
\end{enumerate}

Actual ODE solvers (like in MATLAB) change $h$ based off of $f$
(especially makes $h$ small when $f$ is very large or $f$ is changing quickly).
When we saw the three body problem, we had error when two bodies got close
because the gravitational force got so big it couldn't make $h$ small enough
to figure out what was going on.

If $y' = 2y$ and given $y_0,t_0$,
recall that the exact solution was $y(t) = e^{2(t-t_0)}$.
With the numerical approximation, we get
$y_1 = y_0 + h(2y_0) = y_0(1+2h)$
and $y_2 = y-1 + h(2y_1) = (1+2h)y_1$.
So $y(t_i) = y(t_0 + ih) = (1+2h)^iy_0$.
Now say we fix a $t_{\text{end}}$, perhaps $N$ steps
and so $t_{\text{end}} = t_0 + Nh$.
Then $h = \frac{t_{\text{end}} - t_0}{N}$.
So
\[
	y(t_{\text{end}}) = (1+2h)^Ny_0
	= \left(1+\frac{2(t_{\text{end}} - t_0)}{N}\right)^Ny_0
	= \left(1+\frac{\text{something}}{N}\right)^Ny_0
\]
Hmm... this looks a lot like a defition of $e$.
As $N \to \infty$, we have $e^{\text{something}}y_0 = e^{2(t_{\text{end}}-t_0}y_0$.
We will see this in MATLAB.
On the homework, we will see MATLAB not working too well for $y' = \lvert y \rvert^{1/2}$.

And then he shows us his MATLAB for Euler's method.
MATLAB is quirky.
The first function you define in a file will always be run
when you call the file,
and it will assume the other functions in the file are not meant to be run globally.

\section{January 24}
Topics to finish:
\begin{itemize}
	\item Look at MATLAB for a bit (Solutions to HW2 and HW1-ish)
	\item Vandermonde matrices and linear algebra without linear algebra
	\item Exponentiation and eigenpairs and norms
	\item Higher ordeer versions of Euler's method
	\item More celestial mechanics
\end{itemize}

Some quirky MATLAB:
Something good to know is putting a \verb|;| at the end of a line
to supress the output (so we don't see giant vector).
\verb|1e-200| is defined as its own thing, but \verb|1e-400| is considered \verb|0|.
\verb|1/0| gives \verb|Inf| and \verb|-1/0| gives \verb|-Inf|.
But \verb|-Inf + Inf| gives \verb|NaN| (not a number).
But all this follows the IEEE standard.
MATLAB also calls the first function in a file
as the name of a file when called elsewhere.
We will look more at MATLAB later when we try to break it.

\subsection{Vandermonde matrices}
Given
\[
	A = \begin{bmatrix} 1 & x_0 & x_0^2 & \cdots & x_0^n\\
		1 & x_1 & x_1^2 & \cdots & x_1^n \\
		\vdots & \vdots & \ddots & \vdots\\
	1 & x_n & x_n^2 & \cdots & x_n^n\end{bmatrix}
\]
Does this have a unique solution?

The following are equivalent:
\begin{enumerate}
	\item[1.] $A\begin{bmatrix} c_0 \\ \vdots \\ c_n \end{bmatrix}
		= \begin{bmatrix} y_0 \\ \vdots \\ y_n \end{bmatrix}$
		has a unique solution for all $y_0,\dots,y_n$.
	\item[2.] $A$ is invertible
	\item[3.] $\det(A) \neq 0$
	\item[4.] $A$ is of rank $n+1$
\end{enumerate}

We have actually seen Vandermonde matrices in HW1, just disguised.
We were looking at $3$ point schemes.
We took a bunch of these formulas:
\[
	\begin{cases}
		f(x_0 + L) = \cdots\\
		f(x_0) = \cdots\\
		f(x_0 - h) = \cdots\\
		f(x_0 + 2h) = \cdots\\
		f(x_0 + \sqrt{2}h) = \cdots\\
		f(x_0 + rh) = \cdots
	\end{cases}
\]
(where $r$ is a real number, often an integer, often $0,1,2,-1,-2,\dots$),
and used the formula
\begin{align*}
	c_0\cdot f(x_0 + r_0h) &= c_0\left(f(x_0) + r_0jf'(x_0) + r_0^2\frac{h^2}{2}f''(x_0)
	+ r_0^3\frac{h^3}{3!}f'''(x_0) + O(h^4)\right)\\
		c_1 \cdot f(x_0 + r_1h) &= c_1(\dots)\\
		c_2 \cdot f(x_0 + r_1h) &= c_2(\dots)\\
		c_3 \cdot f(x_0 + r_1h) &= c_3(\dots)
\end{align*}
and we try to rearrange and change $c_i$ and derive a scheme to get
$0f(x_0) + 1hf'(x_0) + 0\frac{h^2}{2}f''(x_0) + 0\frac{h^3}{3!}f'''(x_0)$.
I.e., we are looking for $c_0,c_1,c_2,c_3$ such that
\begin{align*}
	0 &= c_0 + c_1 + c_2 + c_3\\
	1 &= r_0c_0 + r_1c_1 + r_2c_2 + r_2c_3\\
	0 &= r_0^2c_0 + r_1^2c_1 + r_2^2c_2 + r_2^2c_3\\
	0 &= r_0^3c_0 + r_1^3c_1 + r_2^3c_2 + r_2^3c_3
\end{align*}
In homework 1, we saw this was solving a vandermonde matrix
(anything that is of the form of the matrix below):
\[
	\begin{bmatrix}
		1 & 1 & 1 & 1\\
		r_0 & r_1 & r_2 & r_3\\
		r_0^2 & r_1^2 & r_2^2 & r_3^2\\
		r_0^3 & r_1^3 & r_2^3 & r_3^3
	\end{bmatrix}
	\begin{bmatrix} c_0 \\ c_1 \\ c_2 \\ c_3 \end{bmatrix}
	= \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix}
\]
(Observation: this looks like interpolation.)

\begin{theorem}
	The above system has a unique solution.
\end{theorem}
\begin{lemma}
	Say that $p \colon \R \to \R$, $p = p(x)$ is a polynomial and $p$
	has $n+1$ distincr roots, $x_0,\dots,x_n$.
	Then
	\begin{enumerate}
		\item[1.] $p(x) = (x - x_0)(x-x_1)\dots(x-x_n)q(x)$ where $q(x)$ is a polynomial.
			(Bruh people in this class are actually so stupid,
			none of them recognize factoring).
		\item[2.] $p'(x)$ has $n$ distinct roots between $x_0$ and $x_n$
			(assuming $x_0 < x_1 < \cdots < x_n$).
		\item $p''(x)$ has $n-1$ distinct roots between $x_0$ and $x_n$
		\item $p'''(x)$ has $n-2$ distinct roots between $x_0$ and $x_n$
	\end{enumerate}
\end{lemma}

To prove all of these, just done by Rolle's theorem
\begin{theorem}[Rolle's Theorem]
	If $f \in C^{0}[a,b]$ (continuous on $[a,b]$), $f(a) = f(b) = 0$, and
	$f$ is differentiable on $(a,b)$, then $f'(\xi) = 0$ for some $a < \xi < b$.
\end{theorem}
(Bro why is no one putting up their hand to haveing SEEN Rolle's theorem before,
I swear there are too many bums in this course.)
``How many people have seen a proof of this? Yeah, you would do this in Math 320."
(Loewen could never)
But this is quite intuitive anyway.

We do get a useful fact from the lemma:
\begin{corollary}
	If $p = p(x)$ is a polynomial of degree $\leq n$,
	$p(x) = c_0 + c_1x + \cdots + c_nx^n$ and $p$ has $n+1$ distinct roots,
	then $p = 0$
\end{corollary}

\begin{remark}
	$p(x) = x^2 + 1$, then $p$ has no real roots.
	But $p'(x) = 2x$ has a real root, so the lemma doesn't go the other way
	(i.e. $q(x) = 0$ from bullet point 1.).
\end{remark}

Claim: the system
\[
	\begin{bmatrix} 1 & x_0 & x_0^2 & \cdots & x_0^n\\
		1 & x_1 & x_1^2 & \cdots & x_1^n \\
		\vdots & \vdots & \ddots & \vdots\\
	1 & x_n & x_n^2 & \cdots & x_n^n\end{bmatrix}
	\begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{bmatrix} = 0
\]
has $c_1 = c_2 = \cdots = c_n = 0$ as its unique solution.
Why? This just says $p(x) = c_0 + c_1x + c_2x^2 + \cdots + c_nx^n$
satisfies $p(x_0) = 0, p(x_1) = 0, \dots, p(x_n) = 0$,
so $p$ has $n+1$ distinct roots,
hence by our corollary, $p = 0$.
(Important in the definition of a Vandermonde matrix is that
$x_0,x_1,\dots,x_n$ are distinct, otherwise not invertible).

Homework: $e^{\begin{bmatrix}0 & -1 \\ 1 & 0 \end{bmatrix}t}
= \begin{bmatrix} \cos{t} & -\sin{t} \\ \sin{t} & \cos{t}\end{bmatrix}$
and $e^{\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix}t}
= \begin{bmatrix} \cosh{t} & \sinh{t} \\ \sinh{t} & \cosh{t}\end{bmatrix}$
(meant to get to this today).
\end{document}
