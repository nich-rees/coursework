\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{geometry}
\geometry{letterpaper, margin=2.0cm, includefoot, footskip=30pt}

\usepackage{fancyhdr}
\pagestyle{fancy}

\lhead{Math 321}
\chead{Notes}
\rhead{Nicholas Rees}
\cfoot{Page \thepage}

\newtheorem*{problem}{Problem}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\theoremstyle{remark}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\ep}{{\varepsilon}}
\newcommand{\SR}{{\mathcal R}}

%\renewcommand{\theenumi}{(\alph{enumi})}

\begin{document}
\section{January 8}
``Sometimes MVT stands for `most-valuable theorem'."
\subsection{Logistics}
\begin{itemize}
	\item Homework: Due on Fridays at the start of class (Canvas), posted on Thursday or Friday.
		List name of people you worked with at the top
		\begin{itemize}
			\item Homework 0 is due this Friday, not due marks,
				but he is using to test automated test system to hand back
			\item Some help: watch Monty Python Holy Grail
		\end{itemize}
	\item Office hours: Zahl Wed. 1-2; TA TBD; TA2 TBD
	\item Weighting:
		\begin{itemize}
			\item HW 30\%
			\item MT 30\% Feb 14
			\item Final 40\%
		\end{itemize}
\end{itemize}

\subsection*{320 Addendum}
Typically, 321 picks up at integration after finishing with differentiation in 320.
But we will pick up some missed material at the end of 320.

Recall
\begin{definition}
	$f \colon [a,b] \to \R$, $c \in [a,b]$, we say that $f$
	is \emph{differentiable at $c$} if
	$\lim_{x \to c} \frac{f(x) - f(c)}{x-c}$ exists (as a real number).
	We denote this by $f'(c)$.
\end{definition}
This is nice, but could even do in high school.
But we can go up many levels of abstraction with our limit.
\begin{itemize}
	\item $c$ is a limit point in $[a,b]$
		(for every $\ep$, a point not $c$ exists inside the open ball)
	\item $g(x) = \frac{f(x) - f(c)}{x-c}$ is a function with
		domain $[a,b] \setminus \{c\}$ (thankfully, $c$ is still a limit point of this).
	\item If $c \in (a,b)$, the high school definition of the limit works.
		If $c = a,b$, then one-sided limit.
\end{itemize}
\begin{definition}
	If $f \colon [a,b] \to \R$ is differentiable at every point $c \in [a,b]$,
	then we say that $f$ is \emph{differentiable on $[a,b]$}
	and this gives us a new function $f' \colon [a,b] \to \R$.
\end{definition}
\begin{definition}
	If $f'$ is differentiable at $c \in [a,b]$, write $f''(c) = (f')'(c)$.
	Alternate notations:
	\[
		\begin{matrix}
			f(c), & f'(c), & f''(c), & f'''(c)\\
			f^{(0)}(c), & f^{(1)}(c), & f^{(2)}(c), & f^{(3)}(c), &
			\dots f^{(k)}(c) = \frac{d^k}{dx^k} f(x) |_{x=c}
		\end{matrix}
	\]
\end{definition}

Some questions to consider:
\begin{itemize}
	\item Why have codomain $\R$? Why not $\C$? Field $F$? General set / metric space?
	\item Why make domain a closed interval?? More general subset of $\R$? $\C$? Set / metric space?
\end{itemize}
The derivative is one of the most important concepts,
and so makes sense people have thought about making it more general.
Sometimes it works, sometimes it doesn't.

We've used the field structure of $\R$ in an important way
(not necessarily the order structure)...
more than just a metric space.
Seems ambitiuous to have a topological definition of a derivative,
because a derivative is a quantitative rate of change,
and we don't get that in a topology.
You can probably find people who have constructed a topological derivative,
but will have needed to give up desired properties.

\subsection{Taylor's Theorem}
Recall the special case to MVT that is used to prove MVT:
\begin{theorem}[Roll'es Theorem]
	Let $f \colon [a,b] \to \R$ be differentiable, with $f(a) = f(b)$.
	Then $\exists c \in (a,b)$ such that $f'(c) = 0$.
\end{theorem}
If you don't remember how to prove this, good exercise to go through
that uses a lot of material from Math 320.

We will use this to prove Taylor's theorem,
which seems really strong and handles most of our everyday functions,
but easy to step on landmines
(slightly rewording a true statement gives a really wrong one).
We are going to give sufficient hypotheses, could technically weaken, but not as clear.
\begin{theorem}[Taylor's Theorem (5.15 in Rudin)]
	Let $f \colon [a,b] \to \R$, let $n \geq 0$ be an integer.
	Suppose that $f$ is $(n+1)$ times differentiable on $[a,b]$.
	Let $x_0$ and $x$ be points in $[a,b]$ with $x_0 \neq x$.
	Then there exists a point $c$ strictly between $x_0$ and $x$ such that
	\begin{equation}\label{Taylor}
		f(x) = \sum_{k = 0}^n \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k
		+ \frac{f^{(n+1)}(c)}{(n+1)!}(x-x_0)^{n+1}
	\end{equation}
\end{theorem}
We hope the error term is small so we can control it.
We won't prove this today because of the time.
But we will say $P_n(x) = \sum_{k = 0}^n \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k$
is the ``degree n Taylor expansion of $f$ around $x_0$".
When you are choosing notation, there are competing goals:
don't want it to be flowery so that it becomes more complicated $P_n^{f,x_0}(x)$,
but will have to remember what we are hiding.
And also because what is of interest might be when we fix $f,x_0$,
and sending $n$ to infinity.

Is this equation helpful? Will depend on how small the ``error term" gets (could dominate!).
But polynomial if $x$ is close to $x_0$ is going to zero geometrically,
and factorial is going to zero faster than geometric,
so actually for most functions, we get to say something nice.

Question: let $f \colon \R \to \R$, infinitely differentiable.
Suppose $f^{(k)}(0) = 0$ for all $k$.
Is it true that $f$ must be the zero function?
Because then the function is just the error term, and so Taylor's theorem fails in the worst way.

\section{January 10}
\subsection{Proof of Taylor's Theorem}
How do we go about proving statement's like this?
There are many different proofs of this result.
One thing to do when confronted by these statements is to consider special cases.

Warm-up ($n = 0$): (\ref{Taylor}) becomes $f(x) = f(x_0) + f'(c)(x-x_0)$.
This is just MVT, which we have already proved.
So in some sense, more general than MVT.
We can write this in a clever way to make the proof use Rolle's instead.

\begin{proof}
	Define $A \in \R$ by
	\[
		f(x) - P_n(x) = \frac{A}{(n+1)!}(x-x_0)^{n+1}
	\]
	This $A$ exists, since we can just bring the other factors
	on the right to the left and get $A$ explictly.
	Our goal is to show there exists $c$ between $x_0$ and $x$ such that $f^{(n+1)}(c) = A$.

	Define $g(t) = f(t) - P_n(t) - \frac{A}{(n+1)!}(t-x_0)^{n+1}$.
	Note to use Rolle's theorem,
	we have the freedom to shrink our endpoints so that they are equal to each other
	(you might have seen this trick to prove MVT from Rolle's).
	Common to construct a new function that meets the hypotheses of a theorem we want,
	and use that to inform us about our original function.
	We claim that we can shrink it to $x$ and $x_0$.
	Observe $g(x_0) = f(x_0) - P_n(x_0) - 0 = f(x_0) - f(x_0) = 0$ and
	$g(x) = 0$ by the definition of $A$.

	For $j = 0, \dots, n$, then
	$g^{(j)}(x_0) = f^{(j)}(x_0) - P_n^{(j)}(x_0) -
	\underbrace{\frac{d^j}{dt^j}\frac{A}{(n+1)!}(t-x_0)^{n+1}\big\vert_{t=x_0}}_0$.
	But also $P_n^{(j)}(x_0) = f^{(j)}(x_0)$,
	since if $k < j$, then the derivatives kill the term,
	and if $j > k$, then we have a $(x_0 - x_0) = 0$ as a factor.
	Hence, $g^{(j)}(x_0) = f^{(j)}(x_0) - f^{(j)}(x_0) = 0$.
	We also have $g^{(n+1)}(t) = f^{(n+1)}(t) - 0 - A$.
	So we could reword our goal to be to find $c$ such that $g^{(n+1)}(c) = 0$.

	Now it is time to start using Rolle's.
	We have $g(x_0) = 0$ and $g(x) = 0$.
	Then by Rolle's, there exists $c_1$ between $x$ and $x_0$ such that
	$g'(c_1) = 0$.
	But we can apply Rolle's theorem again,
	since $g'(x_0) = 0$ and $g'(c_1) = 0$.
	So there exists $c_2$ between $c_1$ and $x_0$ such that $g''(c_2) = 0$.
	We can repeat this $n$ times to get $g^{(n)}(x_0) = g^({n)}(c_n) = 0$.
	Hence, there exists $c_{n+1}$ between $x_0$ and $c_{n+1}$ such that $g^{(n+1)}(c_{n+1}) = 0$.
	Let $c = c_{n+1}$, and so we have achieved our goal.
\end{proof}
Now this seems a bit magic.
You had to cook up a magic function $g$ and it solves it for you.
But Zahl would do this by looking at $n=1$, maybe $n=2$ and $n=3$,
find something that makes the derivative vanish.
How do we remember how to prove this?
You really want to chunk things into a small number of ideas:
you want to use Rolle's theorem (many times),
want to cook up an axuilliary function $g$ that satisfies Rolle's theorem hypothesis each time,
and given 20 minutes, you could do it.

Given random people and chess grandmasters and random chessboards,
did equally bad in recreating the board by memory.
But given a chessboard that was halfway into a game,
chess grandmasters did significantly better.
The takeaway: the grandmasters weren't remembering the exact location of every piece,
they were remembering by chunking and creating a narrative.

\noindent Examples ($x_0 = 0$):
\begin{enumerate}
	\item $f$ a polynomial of degree $D$.
		$P_n(t)$ is the first terms of $f$, up to degree $n$.
		So the Taylor expansion eventually becomes the polynomial.
	\item $f(t) = e^t$.
		$P_n(t) = \frac{1}{0!} + \frac{t}{1!} + \frac{t^2}{2!} + \cdots + \frac{t^n}{n!}$.
	\item $f(t) = \sin{t}$.
		$P_n(t) = 0 + t + 0 - \frac{t^3}{3!} - 0 + \frac{t^5}{5!}$.
\end{enumerate}

We have talked about convergence in metric spaces.
$P_n$ is a sequence, so how do we talk about $P_n$ converging to $f$?
When does it? What metric space are we in?
We could let our metric space be infinitely differentiable real functions,
with the metric $d(f,g) = \sup_{x\in D}|f(x) - g(x)|$.
The first example converges, since it is eventually $0$.
What about $e^t$?
Well, if we take $n$ before $x$, not true,
exponential vs polynomial.
But if fix a closed interval, then we do get an $n$.
This is the difference between pointwise and uniform convergence.
Our metric was normally used for bounded functions, so this is necessary.
We will pick up this discussion next time.

\section{January 12}
We ended last class with some discussion of convergence of functions.
This is something he wants to defer until later in the term,
but we get some of this on the homework.

Recall he asked two lectures ago if $f \colon \R \to \R$ (or $[-1,1] \to \R$),
and $f(0) = 0, f'(0) = 0, \dots, f^{(k)}(0) = 0$ for all $k$,
must it be true that $f(t) = 0$ for all $t$.
Taylor's theorem makes us think this is true:
otherwise, $f(x)$ is wholly its error term always.
But consider
\[
	f(x) = \begin{cases} 0, & \text{if }x \leq 0\\ e^{-1/x} & \text{if } x > 0 \end{cases}
\]
We can see that this is infinitely differentiable on all of $\R$
(consider the domains seperately).
For $x \leq 0$, $f^{(k)}(x) = 0$ and for $x > 0$, $f^{(k)}(x) = Q(x)e^{-1/x}$
where $Q(x)$ is a rational function.
And exponential will shrink always faster than $Q(x)$,
so goes to $0$ at origin as well.
Proving this is something he's given in homework, but it eats a whole week,
and so probably not this year.

It is valuable in analysis (or all math) to have a large bank of
interesting examples at your finger tips
that exemplify the weird behaviour of how functions behave (or things behave).
Some theorems say this is what happens with these four examples,
and then everything else can be derived from these interesting examples.

\subsection{The Riemann and Riemann-Stieltjes Integral}
\subsubsection{The Riemann Integral}
\begin{definition}[Rudin 6.1]
	A \emph{partition} of $[a,b]$ is a finite set
	\[
		P = \{x_0, x_1, x_2, \dots, x_n\}
	\]
	with $a = x_0 < x_1 < x_2 < \cdots < x_n = b$.
\end{definition}
For $i = 1, \dots, n$, let $\Delta x_i = x_i - x_{i-1}$.
For $f \colon [a,b] \to \R$ bounded, define
\begin{align*}
	M_i &= \sup\{f(x) \colon x \in [x_{i-1}, x_i]\}\\
	m_i &= \inf\{f(x) \colon x \in [x_{i-1}, x_i]\}
\end{align*}
And these will always exist in $\R$, since the interval is nonempty
and $f(x)$ is bounded.
Define the upper and lower Reimann sums of the partition $P$ and function $f(x)$:
\begin{align*}
	U(P,f) &= \sum_{i=1}^n M_i \Delta x_i\\
	L(P,f) &= \sum_{i=1}^n m_i \Delta x_i\\
\end{align*}
Intuition: drawing boxes for each $\Delta x_i$ whose height is
$M_i$ or $m_i$.
As we take smaller widths of the boxes, we would like to see these two values converge.

We define the upper Riemann integral
\[
	\overline{\int_a^b}fdx = \inf_P U(P,f)
\]
and the lower Riemann integral
\[
	\underline{\int_a^b}fdx = \sup_P L(P,f)
\]
where the supremum and infinum is taken voer all partitions of $[a,b]$. 

\begin{definition}
	We say $f \colon [a,b] \to \R$ is \emph{Riemann integrable}	if
	\[
		\overline{\int_a^b}fdx = \underline{\int_a^b}fdx
	\]
	(both should always exist),
	in which case we denote this number by
	\[
		\int_a^b fdx
	\]
	and we say $f \in \mathcal{R}[a,b]$
	(the set of Riemann integrable functions on $[a,b]$).
\end{definition}
Example: $[a,b] = [0,1]$, $f(x) = x$.
If $P = \{x_0,\dots,x_n\}$ is a partition,
$M_i = x_i$ and $m_i = x_{i-1}$.
For the sake of concreteness, consider the evenly-spaced partition
$\{0, \frac1n, \frac2n, \dots, \frac{n}{n}\}$.
Then
\[
	U(P,f) = \sum_{i=1}^n \frac{i}{n}\frac{1}{n}
	= \frac{1}{n^2}\sum_{i=1}^n i = \frac{1}{n^2}\frac12 n(n+1)
	= \frac12 + \frac{1}{2n}
\]
In particular, $\overline{\int_a^b}fdx
\leq \inf\{\frac12+\frac{1}{2n} \colon n \in \N\} = \frac12$.
Also
\[
	L(P,f) = \sum_{i=1}^n \frac{i-1}{n}\frac{1}{n}
	= \frac{1}{n^2}\frac{1}{2}n(n-1)
	= \frac12 - \frac{1}{2n}
\]
In particular, $\underline{\int_a^b}fdx
\geq \sup\{\frac12-\frac{1}{2n} \colon n \in \N\} = \frac12$.
Can we now conclude that $f \in \mathcal{R}[0,1]$
and $\int_a^b fdx = \frac12$?
Well, we need that $\underline{\int_a^b}fdx \leq \overline{\int_a^b}fdx$
always, which we don't have yet.
[Is this not a result of the $\liminf \leq \limsup$ inequality?]
We will prove this actually for a more general class of itnegrals.

\subsubsection{The Riemann-Stieltjes Integral}
\begin{definition}[Rudin 6.2]
	Let $\alpha \colon [a,b] \to \R$ be (weakly) monotone increasing,
	let $P = \{x_0, \dots, x_n\}$ be a partition of $[a,b]$.
\end{definition}
For $i = 1,\dots, n$, let $\Delta \alpha_i = \alpha(x_i) - \alpha(x_{i-1})$
(if $\alpha(x) = x$, then $\Delta \alpha_i = \Delta x_i$).

For $f \colon [a,b] \to \R$ bounded define
\begin{align*}
	U(P,f,\alpha) &= \sum_{i=1}^n M_i \Delta \alpha_i\\
	L(P,f,\alpha) &= \sum_{i=1}^n m_i \Delta \alpha_i\\
\end{align*}
(where $M_i,m_i$ are the same as before).
As before, we define
\begin{align*}
	\overline{\int_a^b}fd\alpha &= \inf_P U(P,f,\alpha)\\
	\underline{\int_a^b}fd\alpha &= \sup_P L(P,f,\alpha)\\
\end{align*}
If
\[
	\overline{\int_a^b}fd\alpha = \underline{\int_a^b}fd\alpha
\]
then we denote the common value by $\int_a^b fd\alpha$
and we say $f \in \mathcal{R}_\alpha[a,b]$.
When $\alpha(x) = x$, we get the Riemann integral from before.
But consider $\alpha(x) =\begin{cases} 0, & x \leq 0\\ 1, & x > 0\end{cases}$.
What does $\int_0^1 fd\alpha$ look like ($f$ continuous).
This actually evalues to $f(0)$ (Dirac delta from physics).

\section{January 15}
\begin{definition}[Rudin 6.3]
	Let $P$ and $P^*$ be partitions of $[a,b]$.
	We say $P^*$ is a \emph{refinement} of $P$ if $P \subset P^*$.
	If $P_1$ and $P_2$ are partitions of $[a,b]$, the \emph{common refinement}
	is the partition $P_1 \cup P_2$.
\end{definition}
\begin{theorem}[Rudin 6.4]
	Let $P^*$ be a refinement of $P$.
	Then $L(P,f\alpha) \leq L(P^*,f\alpha) \leq U(P^*,f,\alpha) \leq U(P,f,\alpha)$.
\end{theorem}
\begin{proof}
	The middle inequality, we have already seen.

	Since partitions are finnite, it sufficies to prove the inequality
	when $P^*$ has one additional point
	(i.e. $x_i < x^* < x_{i+1}$ and $P^* = P \cup \{x^*\}$).
	
	Lets compare $L(P,f,\alpha)$ vs $L(P^*,f,\alpha$.
	Recall $m_j = \in\{f(x) \colon x \in[x_{j-1},x_j]\}$. See
	\[
		L(P,f,\alpha) = \sum_{j=1}^n m_j \Delta \alpha
	\]
	\begin{align*}
		L(P^*,f,\alpha) = \sum_{j=1}^i m_j\Delta \alpha_j
		&+ (\inf\{f(x) \colon x \in [x_i,x^*]\})(\alpha(x^*) - \alpha(x_i))\\
		&+ (\inf\{f(x) \colon x \in [x^*,x_{i+1}]\})(\alpha(x_{i+1}) - \alpha(x^*))\\
		&+ \sum_{j=i+2}^n m_j \Delta \alpha_j
	\end{align*}
	Then
	\begin{align*}
		L(P^*,f,\alpha) - L(P,f,\alpha)
		&= \left(\inf_{x\in[x_i,x^*]} f(x) \right)(\alpha(x^*)-\alpha(x_i))
		+ \left(\inf_{x\in[x^*,x_{i+1}]} f(x) \right)(\alpha(x_{i+1})-\alpha(x^*))
		- m_{i+1}\Delta \alpha_{i+1}\\
		&\geq \left(\inf_{x\in[x_i,x_{i+1}]} f(x) \right)(\alpha(x^*)-\alpha(x_i))
		+ \left(\inf_{x\in[x_i,x_{i+1}]} f(x) \right)(\alpha(x_{i+1})-\alpha(x^*))
		- m_{i+1}\Delta \alpha_{i+1}\\
		&= m_{i+1}(\alpha(x^*) - \alpha(x_i) + \alpha(x_{i+1} - \alpha(x^*))
		- m_{i+1} \Delta \alpha_{i+1}\\
		&= m_{i+1}\Delta \alpha_{i+1} - m_{i+1}\Delta \alpha_{i+1} = 0
	\end{align*}
	where we are using the fact that adding points to a set
	can only decrease its infinum.
	Lastly, the final inequality is proven similarly.
\end{proof}

Recall we wanted to prove something before we moved to the Riemann-Stieltjes integral.
\begin{theorem}[Rudin 6.5]
	Let $f \colon [a,b] \to \R$ bounded,
	$\alpha \colon [a,b] \to \R$ monotone increasing. Then
	\[
		\underline{\int_a^b} fd\alpha \leq \overline{\int_a^b}fd\alpha
	\]
\end{theorem}
\begin{proof}
	Let $P_1$ and $P_2$ be partitions of $[a,b]$, leet $P^* = P_1 \cup P_2$.
	By Theorem 6.4, $L(P_1,f,\alpha) \leq U(P_2,f,\alpha)$, jhence
	\[
		\underline{\int_a^b}fd\alpha = \sup_{P_1} L(P_1,f,\alpha)
		\leq U(P_2,f,\alpha)
	\]
	See that this is true for every $P_2$.
	\[
		\underline{\int_a^b}fd\alpha \leq \inf_{P_2} U(P_2,f,\alpha)
		= \overline{\int_a^b}fd\alpha
	\]
\end{proof}
This was the missing piece to show $\int xdx = \frac12$.

Now we are going to look at some facts about the Riemann-Stieltjes integral,
and they will slowly get less intuitive.
\begin{theorem}[Rudin 6.6]
	Let $f \colon [a,b] \to \R$ bounded and
	$\alpha \colon [a,b] \to \R$ monotonically increasing. Then
	\[
		f \in \mathcal{R}_\alpha[a,b] \iff
		\forall \ep, \exists P \text{ such that }U(P,f,\alpha) - L(P,f\alpha) < \ep
	\]
\end{theorem}
\begin{proof}
	Forward direction: by hypothesis
	\[
		\sup_P L(P,f,\alpha) = \int_a^b fd\alpha = \inf_P U(P,f,\alpha)
	\]
	Let $\ep > 0$. Then $\exists$ a partition $P_1$ such that
	\[
		L(P_1,f,\alpha) > \int_a^b fd\alpha - \ep/2
	\]
	$\exists P_2$ such that $U(P_2,f,\alpha < \int_a^b fd\alpha + \ep/2$.
	Let $P = P_1 \cup P_2$.
	By theorem 6.4
	\[
		L(P_1,f,\alpha) \leq L(P,f,\alpha)
		\leq U(P,f,\alpha) \leq U(P_2,f,\alpha)
	\]
	Hence $U(P,f,\alpha) - L(P,f,\alpha) < \ep$.

	Backwards direction follows from definition.
\end{proof}

\section{January 17}
We are going to continue understanding the Riemann-Stieltjes integral.
We showed when it existed (upper and lower Riemann-Stieltjes integral agrees),
but we don't know a lot about functions that are Riemann-Stieltjes integrable.
We show this now for a large class of functions.
\begin{theorem}[Rudin 6.8]
	Let $\alpha \colon [a,b] \to \R$ be monotonically increasing.
	Let $f \colon [a,b] \to \R$ be continuous.
	Then $f \in \mathcal{R}_\alpha[a,b]$,
	i.e. $C([a,b]) \subset \mathcal{R}_\alpha[a,b]$.
\end{theorem}
\begin{proof}
	Let $f$ be continuous, and $[a,b]$ compact.
	Hence, $f$ is uniformly continuous.
	Hence, for all $\ep_1 > 0$, $\exists \delta > 0$ such that
	$|f(x) - f(y)| < \ep_1$ for all $x,y$ with $|x-y|<\delta$.
	Thus, if $P$ is a partition with $\Delta x_i < \delta$ for all $i$,
	then $M_i - m_i < \ep_1$ for all $i$
	(hmm might be important here for the strict inequality that $f$
	attain max/min because compact set... closed subset of compact).
	Hence $U(P,f,\alpha) - L(P,f,\alpha) =
	\sum_i M_i\Delta\alpha_i - \sum_i m_i \Delta\alpha_i
	\leq \sum_{i=1}^n \ep_1\Delta \alpha_i = \ep_1(\alpha(b) - \alpha(a))$.
	Given $\ep > 0$, select $\ep_1$ sufficiently small so that
	$\ep_1(\alpha(b) - \alpha(a)) < \ep$.
	Choose $P$ as above for the corresponding $\ep_1$.
	We have shown:
	for $\ep > 0$, $\exists$ a partition $P$ such that
	$U(P,f,\alpha) - L(P,f,\alpha) < \ep$.
	So by Theorem 6.6, $f \in \mathcal{R}_\alpha[a,b]$.
\end{proof}

Question: Can we describe/characterze $\mathcal{R}_\alpha[a,b]$ or $\mathcal{R}[a,b]$?
We will pick this up later.

For now, we will expand the functions that are Riemann integrable.
\begin{theorem}[Rudin 6.9]
	Let $f \colon [a,b] \to \R$ be monotone (increasing or decreasing)
	and $\alpha \colon [a,b] \to \R$ monotone (weakly) increasing and continuous.
	Then $f \in \mathcal{R}_\alpha[a,b]$.
\end{theorem}
\begin{remark}
	This theorem neither implies or is implied by the previous theorem.
\end{remark}
\begin{proof}
	Let $n \in \N$. By the intermediate value theorem,
	$\exists$ a partition $P$ such that
	$\Delta \alpha_i = \frac{\alpha(b) - \alpha(a)}{n}$ for all $i=1,\dots,n$.
	\begin{align*}
		U(P,f,\alpha) - L(P,f,\alpha)
		&= \sum_{i=1}^n (M_i - m_i)\Delta \alpha_i\\
		&= \frac{\alpha(b) - \alpha(a)}{n}\sum_{i=1}^n (M_i - m_i)
	\end{align*}
	Suppose, WLOG, that $f$ is monotone increasing,
	then $M_i = f(x_i), m_i = f(x_{i-1})$.
	So our value from before becomes
	\begin{align*}
		U(P,f,\alpha) - L(P,f,\alpha)
		&= \frac{\alpha(b) - \alpha(a)}{n} \sum_{i=1}^n\left(f(x_i) - f(x_{i-1})\right)\\
		&= \frac{\alpha(b) - \alpha(a)}{n}
		\left(f(x_1) - f(x_0) +f(x_2) - f(x_1) +- \cdots +f(x_n) - f(x_{n-1})\right)\\
		&= \frac{\alpha(b) - \alpha(a)}{n}\left(f(x_n) - f(x_0)\right)\\
		&= \frac{\alpha(b) - \alpha(a)}{n}\left(f(b) - f(a)\right)\\
		&= \frac{1}{n}\underbrace{\left(\alpha(b) - \alpha(a)\right)
	\left(f(b) - f(a)\right)}_{\in\R}
	\end{align*}
	Given $\ep > 0$, select $n \in \N$ such that
	\[
		\left\lvert \left(\alpha(b) - \alpha(a)\right)\left(f(b) - f(a)\right)\right\rvert < \ep
	\]
	For such a partition $P$, $U(P,f,\alpha) - L(P,f,\alpha) < \ep$.
	By Theorem 6.6, $f \in \mathcal{R}_\alpha[a,b]$.
	If $f$ is monotonically decreasing, an identical proof works.
\end{proof}

\begin{theorem}[Rudin 6.10]
	Let $f \colon [a,b] \to \R$ be bounded and continuous at all
	but finitely many points.
	Let $\alpha \colon [a,b] \to \R$ be monotone increasing
	and must be continuous at every point where $f$ is not continuous.
	Then $f \in \mathcal{R}_\alpha[a,b]$.
\end{theorem}
Probably won't prove today, but some examples.
ff

And then tangent into monotonically increasing functions with countably many discontinuitys.
Can show that the Devil's staircase $\alpha \colon [0,1] \to [0,1]$
is monotonically increasing and discontinuous.
Remember he mentioned that it is useful to have examples in your head.
For Riemann-Stieltjes integral, $\alpha(x) = x$ is a good one (Riemann integral),
but also this Cantor-Lebesgue function.

\section*{January 22}
\subsection{Properties of the Riemann-Stieltjes Integral}
Assume that $\alpha \colon [a,b] \to \R$ is monotonically increasing,
and $f,f_1,f_2 \colon [a,b] \to \R$ are $f,f_1,f_2 \in \mathcal{R}_\alpha[a,b]$.
Then we get some more functions that are integrable.
\begin{enumerate}
	\item[(a).] Linearity: $f_1 + f_2 \in \mathcal{R}_\alpha[a,b]$
		and $\int_a^b(f_1+f_2)d\alpha = \int_a^b f_1d\alpha + \int_a^bf_2d\alpha$;
		$c \in \R$ then $cf \in cf\in\mathcal{R}_\alpha[a,b]$
		and $\int_a^b cfd\alpha = c\int_a^bfd\alpha$.
	\item[(b).] Non-negativity: If $f(x) \geq 0$ for all $x \in [a,b]$
		then $\int_a^b fd\alpha \geq 0$.
		This then implies if $f_1(x) \leq f_2(x)$ for all $x \in [a,b]$
		then $\int f_1d\alpha \leq \int f_2d\alpha$
	\item[(c).] For $c \in (a,b)$, then $f \in \mathcal{R}_\alpha[a,c]$
		and $f \in \mathcal{R}_\alpha[c,b]$
		and $\int_a^c fd\alpha + \int_c^b fd\alpha = \int_a^bfd\alpha$
	\item[(d).] Boundedness: If $|f| \leq M$ then
		$\left\lvert \int_a^b fd\alpha\right\rvert \leq M(\alpha(b) - \alpha(a))$.
	\item[(e).] Let $\alpha_1,\alpha_2 \colon [a,b] \to \R$ and they're monotone increasing,
		and $f \colon [a,b] \to \R$ where $f \in \mathcal{R}_{\alpha_1}[a,b]$
		and $f \in \mathcal{R}_{\alpha_2}[a,b]$.
		Then $f \in \mathcal{R}_{\alpha_1+\alpha_2}[a,b]$
		and $\int_a^b fd(\alpha_1 + \alpha_2) = \int_a^b fd\alpha_1 +
		\int_a^b fd\alpha_2$.
		If $c \in \R$, then $f \in \mathcal{R}_{c\alpha_1}[a,b]$
		and $\int_a^b fd(c\alpha_1) = c\int_a^bfd\alpha_1$.
\end{enumerate}
Proof: See Rudin; this is theorem 6.12.

Now we will have an informal discussion about what this all tells us.
Why bother with a more complicated version of an integral,
and why this specific setup?
Recall from 320 $C([a,b])$, the space of continuous functions $f \colon [a,b] \to \R$.
We define $\lVert f \rVert_{C([a,b])} = \sup_{x\in[a,b]}\lvert f(x)\rvert$
and so $d(f,g) = \lVert f - g \rVert_{C([a,b])}$
(also called the $C_0$ norm).
This space is a vector space, infinite dimensional
(field is reals for now, but can extend to complex).
Thus, $(C([a,b]), \lVert \cdot \rVert_{C([a,b])})$ is a normed vector space.
In linear algebra, we are often interested in linear functions from vector spaces,
or here, linear transforms between normed vector spaces.
Property (a) from theorem 6.12 says:
If $\alpha \colon [a,b] \to \R$ is monotone increasing,
then the function $T(f) = \int_a^b fd\alpha$ is a linear function
from the vector space $C([a,b])$ to $\R$ (for now, we will restrict our domain),
i.e. $T(f+g) = T(f) + T(g), T(cf) = cT(f)$.
Property (d) says that $T$ is bounded,
i.e. $|Tf| \leq A\lVert f \rVert_{C([a,b])}$,
or more specifically, $|Tf| \leq (\alpha(b) - \alpha(a)\lVert f \rVert_{C([a,b])}$
(since $|f| \leq M$ when $f \in C([a,b])$ means $\lVert f \rVert_{C([a,b])} \leq M$).
Notation: people sometimes write $Tf$ instead of $T(f)$
(in linear algebra, we write $Mv$).
Property (b) says that $T$ is non-negative, i.e.
if $f \in C([a,b])$ with $f(x) \geq 0$ for all $x \in [a,b]$, then $Tf \geq 0$.

Why have we changed our wording from theorem 6.12 to this?
In functional analysis (Math 421) and more generally in physics,
we want to study linear functions whose domain is $C([a,b])$ (or more general)
and whose codomain is often $\R$ or $\C$.
Functions of this type are called ``(linear) operators" or ``(linear) functionals".
Can't say too much right now about why this is an interesting area of study,
but a starting point in modern quantum mechanics is,
instead of thinking of particles as points, we think of it as a function.
In classic mechanics, a particle might be described with $6$ numbers
($x,y,z,v_x,v_y,v_z$),
and if you were to ask what the position of this particle, we just take the first three
(and likewise with the velocity/momentum).
Instead, we now consider transformations on the function.

\begin{theorem}[Riesz Representation Theorem V1]
	Let $T \colon C([a,b]) \to \R$ be linear, bounded, and non-negative.
	Then, there exists $\alpha \colon [a,b] \to \R$ monotonically increasing
	such that $Tf = \int_a^b fd\alpha$.
\end{theorem}
The whole point of this theorem is that you can go in the other direction.
$f$ and $\alpha$ uniquely define a linear bounded non-negative operator $T \colon [a,b] \to \R$,
and $T$ uniquely determines $f,\alpha$.

This non-negative condition seems a little weird...
what happens when our codomain is $\C$?
\begin{theorem}[Riesz Representation Theorem V2]
	Let $T \colon C([a,b]) \to \R$ be linear and bounded.
	``Let $T$ be a real-valued linear functional on $C([a,b])$."
	Then there exists $\alpha,\beta \colon [a,b] \to \R$ monotone increasing such that
	\[
		T(f) = \int_a^b fd\alpha - \int_a^b fd\beta = ``\int_a^bfd(\alpha-\beta)"
	\]
	(but careful, because $\alpha - \beta$ is not necessarily monotone increasing,
	but can just define difference of monotone increasing functions;
	call these ``functions of bounded variation").
\end{theorem}
How would we go about proving this?
Comes from Hanh-Banach Theorem from functional analysis.
Banach spaces are complete normed vector spaces.
If you have a linear function from a dense subspace to the reals,
can extend this to the entire space to the reals.
Won't prove this... requires axiom of choice
(even though Riesz Representation doesn't require it).
And then we're in good shape...
get $T$ is the integral with various test functions on dense subspace.

\section{January 24}

\begin{theorem}[Rudin 6.13]
	Let $f,g \in \mathcal{R}_\alpha[a,b]$.
	\begin{enumerate}
		\item[(a).] Then $fg \in \mathcal{R}_\alpha[a,b]$.
		\item[(b).] Then $\lvert f \rvert \in \mathcal{R}_\alpha[a,b]$
			and $\lvert \int_a^bfd\alpha \rvert \leq \int_a^b \lvert f \rvert \alpha$
	\end{enumerate}
\end{theorem}
\begin{proof}
	First proof of (a).
	By theorem 6.11, $\alpha(x)=x^2$, $(f+g)^2,(f-g)^2 \in \mathcal{R}_\alpha[a,b]$.
	By theorem 6.12a, $(f+g)^2-(f-g)^2 = 4fg \in \mathcal{R}_\alpha[a,b]$.
	By theorem 6.12a, ($c = \frac14$) $fg \in \mathcal{R}_\alpha[a,b]$.

	Now proof of (b).
	By theorem 6.11 $\alpha(x) = \lvert x \rvert$, $\lvert f \rvert \in \mathcal{R}_\alpha[a,b]$.
	Let $c = \mathrm{sgn}\int_a^b fd\alpha$ so
	$\lvert \int_a^b fd\alpha = c\int_a^bfd\alpha = \int_a^bcfd\alpha
	\leq \int_a^b \lvert f \rvert d\alpha$ (by theorem 6.12a).
\end{proof}

\begin{theorem}[Rudin 6.15]
	Let $f \colon [a,b] \to \R$ be bounded.
	Let $s \in (a,b)$ and suppose $f$ is continuous at $s$.
	Let $\alpha(x) = \begin{cases} 0 & x \leq s \\ 1 & x > s\end{cases}$.
	Then $f \in \mathcal{R}_\alpha[a,b]$ and $\int_a^b fd\alpha = f(s)$.
\end{theorem}
\begin{proof}
	Let $P = \{x_0,x_1,x_2,x_3\}$ where $a = x_0, s = x_1, b = x_3$.
	Then $U(P,f,\alpha) = \sum_{i=1}^3 M_i \Delta \alpha_i = M_2 = \sup_{x\in[x_1,x_2]}f(x)$
	and $L(P,f,\alpha) = \sum_{i=1}^3 m_i \Delta \alpha_i = m_2 = \inf_{x\in[x_1,x_2]}f(x)$.
	Since $f$ is continuous at $s$, $\forall \ep > 0$, $\exists \delta$
	such that if $|x-y| < \delta$, then $|f(x) - f(y)| < \frac{\ep}{2}$,
	so let $x_2 \in (x_1,x_1+\delta)$.
	Then,
	\[
		\sup_{x\in[x_1,x_2]}f(x) \leq f(x_1) + \ep/2 \implies M_2 \leq f(x_1) + \ep/2
	\]
	\[
		\inf_{x\in[x_1,x_2]}f(x) \geq f(x_1) - \ep/2 \implies m_2 \geq f(x_1 - \ep/2
	\]
	Hence, $M_2 - m_2 \leq \ep$.
\end{proof}
How would we change proof if $f(x) = 1$ for $x \geq s$?
We would get the same result, but would need to change which switch $x_1,x_2$ roles.
If defined at neither, then probably $s \in [x_1,x_2]$.

This step function is actually quite important in electrical engineering.
Even has a special name:
\begin{definition}[Heavyside step function]
	\[
		I(x) = \begin{cases} 0, & x \leq 0\\ 1, & x > 0\end{cases}
	\]
\end{definition}

\begin{theorem}[Rudin 6.16]
	Let $\{c_n\}_{n=1}^\infty$ be positive real numbers, with $\sum_{n=1}^\infty c_n < \infty$.
	Let $[a,b]$ be an interval, and let $\{s_n\}_{n=1}^\infty \subset (a,b)$
	be distinct points.
	Let $\alpha(x) = \sum_{n=1}^\infty c_nI(x-s_n)$
	(a bunch of steps at $s_n$ by $c_n$).
	Let $f \colon [a,b] \to \R$ be continuous,
	so $f \in \mathcal{R}_\alpha[a,b]$
	(always true, $f$ is continuous and $\alpha$ monotone increasing).
	Then
	\[
		\int_a^b fd\alpha = \sum_{n=1}^\infty cf(s_n)
	\]
\end{theorem}
(We know the integral exists,
and the sum exists because $\sum c_n < \infty$ and $f$ is bounded.)
``I love nitpicking, because math is meant to be precise."
\begin{proof}
	Let $R_N = \int_a^bfd\alpha - \sum_{n=1}^N c_nf(s_n)$.
	Goal: $\forall \ep > 0$, $\exists N_0$ such that
	$\forall N \geq N_0$m $\lvert R_N \rvert < \ep$.
	So fix $N$, let $\alpha_1 = \sum_{n=1}^N c_nI(x-s_n)$,
	$\alpha_2 = \sum_{n=N+1}^\infty c_n I(x-s_n)$.
	By Theorem 6.12e, $\int_a^b fd\alpha = \int_a^b fd\alpha_1 + \int_a^bfd\alpha_2$
	(we meet the hypothesis that $f \in \mathcal{R}_{\alpha_1},\mathcal{R}_{\alpha_2}$,
	since $f$ is continuous).
	\[
		\int_a^b fd\alpha_1 = \sum_{n=1}^N \int_a^b f(x) d[c_nI(x-s_n)]
		= \sum_{n=1}^N c_n f(s_n)
	\]
	(by Theorem 6.15).
	So $R_N = \int_a^b fd\alpha_2$.
	Let $K = \sup_{x \in [a,b]} \lvert f \rvert$.
	By 6.12b, $\int_a^b fd\alpha_2 \leq K \int_a^B 1 d\alpha_2
	= K(\alpha_2(b) - \alpha_2(a)) = K\sum_{n=N+1}^\infty c_n \to 0$ as $N \to \infty$.
	Overview: we wrote our difference in terms of a main term and tail.
	And we showed the tail goes to zero
	(we can make our proof more formal with $\ep$, etc., but out of time).
\end{proof}

\section{January 26}
\begin{theorem}[Rudin 6.17]
	Let $f \colon [a,b] \to \R$ bounded, $\alpha \colon [a,b] \to \R$
	differentiable and monotone increasing.
	Suppose $\alpha' \in \mathcal{R}[a,b]$.
	Then $f \in \mathcal{R}_\alpha[a,b] \iff f\alpha' \in \mathcal{R}[a,b]$
	and if so
	\[
		\int_a^b fd\alpha = \int_a^b f\alpha' dx
	\]
\end{theorem}
Step 1: it sufficies to show that
\[
	\overline{\int_a^b}fd\alpha = \overline{\int_a^b}f\alpha' xdx
	\qquad \text{ and } \qquad
	\underline{\int_a^b}fd\alpha = \underline{\int_a^b}f\alpha'xdx
\]
We will prove the first equality, second one is an exercise.

Step 2: since $\alpha' \in \mathcal{R}[a,b]$
for all $\ep > 0$, $P$ (of $[a,b]$) such that
\[
	U(P,\alpha') - L(P,\alpha') < \ep
\]
This inequality continues to hold for every refinement $P'$ of $P$.

We have $U(P,\alpha') - L(P,\alpha') = \sum_{i=1}^n(A_i - a_i)\Delta x_i$,
where $A_i = \sup\{\alpha'(x) \colon x \in [x_{i-1},x_i]\}$
and likewise for $a_i$.
By the mean value theorem for each $i = 1,\dots, n$
$\exists t_i \in [x_{i-1},x_i]$ such that
$\Delta \alpha_i = \alpha'(t_i)\Delta x_i$.
Now this suggests $\overline{\int}fd\alpha = \overline{\int}f\alpha'dx$,
but need to be careful.

For every $s_i \in [x_{i-1},x_i]$, we have
$|\alpha'(s_i) - \alpha'(t_i)| \leq A_i - a_i$
(literally did this so many times on the homework lol)
so $\sum_{i=1}^n |\alpha'(s_i) - \alpha'(t_i)|\Delta x_i
\leq \sum_{i=1}^n (A_i - a_i)\Delta x_i$
for \emph{every} choice of $s_i \in [x_{i-1},x_i]$, $i = 1,\dots,n$.
Let $K = \sup_{x \in [a,b]}|f|$, then
\begin{equation}\label{6.17 ineq}
	\sum_{i=1}^n |f(s_i)\alpha'(s_i)\Delta x_i - f(s_i)\alpha'(t_i)\Delta x_i|
	\leq K\ep
\end{equation}
Hence
\[
	\sum_{i=1}^n f(s_i)\Delta \alpha_i \leq \sum_{i=1}^n f(s_i)\alpha'(s_i)\Delta x_i + Ke
	\leq U(P,f\alpha') + K\ep
\]
Recall triangle inequality:
if $|a+b| \leq c$, then $a \leq b + c$ and $b \leq a + c$.

When we have an inequality, might wonder where the inequality is
``sharp" or ``tight":
i.e. equality or the closest to equality as possible.
Taking the supremum of $s_i \in [x_{i-1},x_i]$, $i = 1, \dots, n$,
we conclude (if $M_i = \sup\{f(x) \colon x \in [x_{i-1},x_i]\}$)
\[
	U(P,f,\alpha) = \sum_{i=1}^n M_i \Delta \alpha_i \leq U(P,f\alpha') + K\ep
\]
Hence
\[
	\overline{\int_a^b}fd\alpha \leq U(P,f,\alpha) \leq U(P,f\alpha') + K\ep
\]
So for every $\ep$, we found some partition $P$ that makes the inequality hold.
Recall that it also holds for every refinement $P'$ of $P$.

How can we make this inequality be as close to equality as possible
(how much strength can squeeze out of inequality).
Taking the infinum over all refinements $P'$ of $P$, we have
\[
	\overline{\int_a^b}fd\alpha \leq \int_{P'} U(P',f\alpha) + K\ep
\]
We are taking a more restricted set of partitions,
will this give us the infinum we want?
Yes: for any non-refinement partition,
just union it with $P$ to get a refinement,
i.e. $\forall \ep > 0$, $\overline{\int_a^b}fd\alpha \leq \overline{\int_a^b}f\alpha'dx + KE
\implies \overline{\int_{a}^b} fd\alpha \leq \int_a^b \alpha'dx$.

Once you've seen this, can do the other three inequalities.
Will just say a word or two about the reverse inequality, i.e.
\[
	\overline{\int_a^b}fd\alpha \geq \overline{\int_a^b}f\alpha'dx
\]
Taking (\ref{6.17 ineq}), get
\[
	\sum_{i=1}^n f(s_i) \alpha'(s_i)\Delta x_i \leq
	\sum_{i=1}^n f(s_i)\Delta\alpha_i + K\ep
\]
Etc.

\section{January 29}
For the next theorem, note that a continuous bijective map
must be strictly monotone (good exercise).
\begin{theorem}[Change of Variables (Rudin 6.19)]
	Let $\phi \colon [A,B] \to [a,b]$, strictly increasing,
	onto (i.e. $1-1$), continuous.
	Let $\alpha \colon [a,b] \to \R$ monotone increasing.
	Let $f \in \mathcal{R}_\alpha[a,b]$.
	Define $g = f \circ \phi \colon [A,B] \to \R$,
	$\beta \colon \alpha \circ \phi \colon [A,B] \to \R$.
	Then $g \in \mathcal{R}_\beta[A,B]$ and
	\[
		\int_A^B gd\beta = \int_a^b fd\alpha
	\]
\end{theorem}

\begin{remark}
	Let $\alpha(x) = x$, and $\phi$ is differentiable.
	Then $d\beta = \phi'(x)dx$ (why???), i.e.
	\[
		\int_a^b fd\alpha = \int_{\phi^{-1}(a)}^{\phi^{-1}(b)}f(x)\phi'(x)dx
	\]
	And this lines up with our normal change of variables formula.
\end{remark}
\begin{proof}
	Partitions $P = \{a=x_0, x_1,\dots,x_n = b\}$ of $[a,b]$
	and partitions $Q$ of $[A,B]$
	are in $1-1$ correspondace via $x_i = \phi(y_i)$.
	We have $\alpha(x_i) = \alpha \circ \phi(y_i) = \beta(y_i)$
	and $\{f(x) \colon x \in [x_{i-1},x_i]\} = \{g(y) \colon y \in [y_{i-1},y_i]\}$.
	Then $U(P,f,\alpha) = U(Q,g,\beta)$ and $L(P,f,\alpha) = L(Q,g,\beta)$.

	Let $\ep > 0$. Since $f \in \mathcal{R}_\alpha[a,b]$,
	there exists a partition $P$ of $[a,b]$ such that
	$U(P,f,\alpha) - L(P,f,\alpha) < \ep$,
	hence $U(Q,g,\beta) - L(Q,g,\beta) < \ep$.
	Then $g \in \mathcal{R}_\beta[A,B]$.
	So first part of theorem done \checkmark

	Finally,
	\[
		\int_A^B g d\beta = \inf_Q U(Q,g,\beta)
		= \inf_P U(P,f,\alpha) = \int_a^b f d\alpha
	\]
\end{proof}

Recall the hypotheses of this theorem:
$\phi$ is continuous, $1-1$ and onto.
Where did we use them?
Where we got a $1-1$ correspondance between $P$ and $Q$,
and makes the rest of the proof fall.

Now we start inching towards the fundamental theorem of calculus.
\begin{theorem}[Rudin 6.20]
	Let $f \in \mathcal{R}[a,b]$.
	For $x \in [a,b]$, define $F(x) = \int_a^x f(t)dt$, $F(a) = 0$.
	Then: $F$ is continuous on $[a,b]$.
	And if $c \in [a,b]$ and $f$ is continuous at $c$,
	then $F$ is differentiable at $c$,
	and $F'(c) = f(c)$.
\end{theorem}
\begin{proof}
	Proving continuity: Let $K = \sup_{t\in[a,b]}\lvert f(t) \rvert$.
	By theorem 6.12c (linear in the domain), for $a \leq x \leq y \leq b$
	\[
		F(y) - F(x) = \int_a^y f(t)dt - \int_a^y f(t)dt
		= \int_x^y f(t)dt
	\]
	Essentially, the biggest this can be is $K$,
	and so the largest the integral can be is $\in_a^b K dt$,
	but have to do this carefully.
	Thus
	\[
		\lvert F(y) - F(x) \rvert = \left\lvert \int_x^y f(t)dt \right\rvert
		\leq \int_x^y \lvert f(t)\rvert dt \leq \int_x^y Kdt = K(y-x)
	\]
	(where we used theorem 6.13 for the first inequality, and 6.12b for the second).
	Hence, for every $\ep > 0$, select $\delta = \ep/K$ (ro $\delta = \ep$ if $K=0$).
	Then if $\lvert x - y \rvert < \delta$,
	$\lvert F(x) - F(y) \rvert < \ep$.

	Proving differentiability at $c$:
	suppose $c \neq b$, i.e. $c \in [a,b)$.
	Let us compute $\lim_{h \searrow 0} \frac{F(c+h) - F(c)}{h}$.
	For $h > 0$, we have
	\[
		\left\lvert\frac{1}{h}[F(c+h) - F(c)] - f(c)\right\rvert
		= \left\lvert\int_c^{c+h}f(t)dt - f(c) \right\rvert
	\]
	In analysis, we often want to exploit some cancellation between
	two quantities we think of as close together.
	We are sampling values of $f$ quite close to $c$,
	and comparing them to $f(c)$.
	How do we exploit cancellation?
	One common technique when a quantity is not changing much over
	a region is take its average and bring it inside an integral to cancel.
	So, since $f(c) = \frac{1}{h}\int_c^{c+h} f(c)dt$, we get
	\[
		= \left\lvert \frac{1}{h} \int_c^{c+h} (f(t) - f(c))dt\right\rvert
		\leq \frac{1}{h} \int_c^{c+h} \lvert f(t) - f(c) \rvert dt
	\]
	(triangle inequality for integrals?)
	Since $f$ is continuous at $c$, $\forall \ep > 0$,
	$\exists \delta > 0$ such that for all $y \in [a,b]$ with $|y-c| < \delta$,
	we have $\lvert f(c) - f(y)\rvert < \ep$.
	So for $h < \delta$, we have
	\[
		\frac{1}{h}\int_c^{c+h} \lvert f(t) - f(c) \rvert dt
		< \frac{1}{h} \int_c^{c+h} \ep dt = \ep
	\]
	i.e. $\forall \ep > 0$, $\exists \delta > 0$ such that $\forall 0 < h < \delta$,
	\[
		\left\lvert \frac{1}{h} \left( F(c+h) - F(c)\right) - f(c) \right\rvert < \ep
	\]
	i.e. $\lim_{h \searrow 0} \frac{F(c+h) - F(c)}{h} = f(c)$.

	If $c \neq a$, i.e. if $c \in (a,b]$,
	an identical argument shows
	$\lim_{h \nearrow 0}\frac{F(c+h)-F(c)}{h} = f(c)$.
\end{proof}

Question: If $f$ is not continuous at $c$,
is $F$ never differentiable at $c$, maybe differentiable at $c$,
or is always differentiable at $c$ (our trichotomy).
\begin{remark}
	If $f(x) = g(x)$ except at one point,
	then $\int_a^b f(x) dx = \int_a^b g(x) dx$.
	I.e. the Riemann integral does not see the value of the function
	at one point (not true generally for Riemann-Stieltjes,
	we saw $\alpha$ is Heaviside step function only cared about
	the function at a single point).
\end{remark}
When we have a jump discontinuity, $F$ will never be continuous at a point.

\section{January 31!}
ff
\end{document}
