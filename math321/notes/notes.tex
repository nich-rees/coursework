\documentclass{article}
\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage{geometry}
\geometry{letterpaper, margin=2.0cm, includefoot, footskip=30pt}

\usepackage{fancyhdr}
\pagestyle{fancy}

\lhead{Math 321}
\chead{Notes}
\rhead{Nicholas Rees}
\cfoot{Page \thepage}

\newtheorem*{problem}{Problem}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\theoremstyle{remark}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newcommand{\N}{{\mathbb N}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\ep}{{\varepsilon}}
\newcommand{\SR}{{\mathcal R}}

%\renewcommand{\theenumi}{(\alph{enumi})}

\begin{document}
\section{January 8}
``Sometimes MVT stands for `most-valuable theorem'."
\subsection{Logistics}
\begin{itemize}
	\item Homework: Due on Fridays at the start of class (Canvas), posted on Thursday or Friday.
		List name of people you worked with at the top
		\begin{itemize}
			\item Homework 0 is due this Friday, not due marks,
				but he is using to test automated test system to hand back
			\item Some help: watch Monty Python Holy Grail
		\end{itemize}
	\item Office hours: Zahl Wed. 1-2; TA TBD; TA2 TBD
	\item Weighting:
		\begin{itemize}
			\item HW 30\%
			\item MT 30\% Feb 14
			\item Final 40\%
		\end{itemize}
\end{itemize}

\subsection*{320 Addendum}
Typically, 321 picks up at integration after finishing with differentiation in 320.
But we will pick up some missed material at the end of 320.

Recall
\begin{definition}
	$f \colon [a,b] \to \R$, $c \in [a,b]$, we say that $f$
	is \emph{differentiable at $c$} if
	$\lim_{x \to c} \frac{f(x) - f(c)}{x-c}$ exists (as a real number).
	We denote this by $f'(c)$.
\end{definition}
This is nice, but could even do in high school.
But we can go up many levels of abstraction with our limit.
\begin{itemize}
	\item $c$ is a limit point in $[a,b]$
		(for every $\ep$, a point not $c$ exists inside the open ball)
	\item $g(x) = \frac{f(x) - f(c)}{x-c}$ is a function with
		domain $[a,b] \setminus \{c\}$ (thankfully, $c$ is still a limit point of this).
	\item If $c \in (a,b)$, the high school definition of the limit works.
		If $c = a,b$, then one-sided limit.
\end{itemize}
\begin{definition}
	If $f \colon [a,b] \to \R$ is differentiable at every point $c \in [a,b]$,
	then we say that $f$ is \emph{differentiable on $[a,b]$}
	and this gives us a new function $f' \colon [a,b] \to \R$.
\end{definition}
\begin{definition}
	If $f'$ is differentiable at $c \in [a,b]$, write $f''(c) = (f')'(c)$.
	Alternate notations:
	\[
		\begin{matrix}
			f(c), & f'(c), & f''(c), & f'''(c)\\
			f^{(0)}(c), & f^{(1)}(c), & f^{(2)}(c), & f^{(3)}(c), &
			\dots f^{(k)}(c) = \frac{d^k}{dx^k} f(x) |_{x=c}
		\end{matrix}
	\]
\end{definition}

Some questions to consider:
\begin{itemize}
	\item Why have codomain $\R$? Why not $\C$? Field $F$? General set / metric space?
	\item Why make domain a closed interval?? More general subset of $\R$? $\C$? Set / metric space?
\end{itemize}
The derivative is one of the most important concepts,
and so makes sense people have thought about making it more general.
Sometimes it works, sometimes it doesn't.

We've used the field structure of $\R$ in an important way
(not necessarily the order structure)...
more than just a metric space.
Seems ambitiuous to have a topological definition of a derivative,
because a derivative is a quantitative rate of change,
and we don't get that in a topology.
You can probably find people who have constructed a topological derivative,
but will have needed to give up desired properties.

\subsection{Taylor's Theorem}
Recall the special case to MVT that is used to prove MVT:
\begin{theorem}[Roll'es Theorem]
	Let $f \colon [a,b] \to \R$ be differentiable, with $f(a) = f(b)$.
	Then $\exists c \in (a,b)$ such that $f'(c) = 0$.
\end{theorem}
If you don't remember how to prove this, good exercise to go through
that uses a lot of material from Math 320.

We will use this to prove Taylor's theorem,
which seems really strong and handles most of our everyday functions,
but easy to step on landmines
(slightly rewording a true statement gives a really wrong one).
We are going to give sufficient hypotheses, could technically weaken, but not as clear.
\begin{theorem}[Taylor's Theorem (5.15 in Rudin)]
	Let $f \colon [a,b] \to \R$, let $n \geq 0$ be an integer.
	Suppose that $f$ is $(n+1)$ times differentiable on $[a,b]$.
	Let $x_0$ and $x$ be points in $[a,b]$ with $x_0 \neq x$.
	Then there exists a point $c$ strictly between $x_0$ and $x$ such that
	\begin{equation}\label{Taylor}
		f(x) = \sum_{k = 0}^n \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k
		+ \frac{f^{(n+1)}(c)}{(n+1)!}(x-x_0)^{n+1}
	\end{equation}
\end{theorem}
We hope the error term is small so we can control it.
We won't prove this today because of the time.
But we will say $P_n(x) = \sum_{k = 0}^n \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k$
is the ``degree n Taylor expansion of $f$ around $x_0$".
When you are choosing notation, there are competing goals:
don't want it to be flowery so that it becomes more complicated $P_n^{f,x_0}(x)$,
but will have to remember what we are hiding.
And also because what is of interest might be when we fix $f,x_0$,
and sending $n$ to infinity.

Is this equation helpful? Will depend on how small the ``error term" gets (could dominate!).
But polynomial if $x$ is close to $x_0$ is going to zero geometrically,
and factorial is going to zero faster than geometric,
so actually for most functions, we get to say something nice.

Question: let $f \colon \R \to \R$, infinitely differentiable.
Suppose $f^{(k)}(0) = 0$ for all $k$.
Is it true that $f$ must be the zero function?
Because then the function is just the error term, and so Taylor's theorem fails in the worst way.

\section{January 10}
\subsection{Proof of Taylor's Theorem}
How do we go about proving statement's like this?
There are many different proofs of this result.
One thing to do when confronted by these statements is to consider special cases.

Warm-up ($n = 0$): (\ref{Taylor}) becomes $f(x) = f(x_0) + f'(c)(x-x_0)$.
This is just MVT, which we have already proved.
So in some sense, more general than MVT.
We can write this in a clever way to make the proof use Rolle's instead.

\begin{proof}
	Define $A \in \R$ by
	\[
		f(x) - P_n(x) = \frac{A}{(n+1)!}(x-x_0)^{n+1}
	\]
	This $A$ exists, since we can just bring the other factors
	on the right to the left and get $A$ explictly.
	Our goal is to show there exists $c$ between $x_0$ and $x$ such that $f^{(n+1)}(c) = A$.

	Define $g(t) = f(t) - P_n(t) - \frac{A}{(n+1)!}(t-x_0)^{n+1}$.
	Note to use Rolle's theorem,
	we have the freedom to shrink our endpoints so that they are equal to each other
	(you might have seen this trick to prove MVT from Rolle's).
	Common to construct a new function that meets the hypotheses of a theorem we want,
	and use that to inform us about our original function.
	We claim that we can shrink it to $x$ and $x_0$.
	Observe $g(x_0) = f(x_0) - P_n(x_0) - 0 = f(x_0) - f(x_0) = 0$ and
	$g(x) = 0$ by the definition of $A$.

	For $j = 0, \dots, n$, then
	$g^{(j)}(x_0) = f^{(j)}(x_0) - P_n^{(j)}(x_0) -
	\underbrace{\frac{d^j}{dt^j}\frac{A}{(n+1)!}(t-x_0)^{n+1}\big\vert_{t=x_0}}_0$.
	But also $P_n^{(j)}(x_0) = f^{(j)}(x_0)$,
	since if $k < j$, then the derivatives kill the term,
	and if $j > k$, then we have a $(x_0 - x_0) = 0$ as a factor.
	Hence, $g^{(j)}(x_0) = f^{(j)}(x_0) - f^{(j)}(x_0) = 0$.
	We also have $g^{(n+1)}(t) = f^{(n+1)}(t) - 0 - A$.
	So we could reword our goal to be to find $c$ such that $g^{(n+1)}(c) = 0$.

	Now it is time to start using Rolle's.
	We have $g(x_0) = 0$ and $g(x) = 0$.
	Then by Rolle's, there exists $c_1$ between $x$ and $x_0$ such that
	$g'(c_1) = 0$.
	But we can apply Rolle's theorem again,
	since $g'(x_0) = 0$ and $g'(c_1) = 0$.
	So there exists $c_2$ between $c_1$ and $x_0$ such that $g''(c_2) = 0$.
	We can repeat this $n$ times to get $g^{(n)}(x_0) = g^({n)}(c_n) = 0$.
	Hence, there exists $c_{n+1}$ between $x_0$ and $c_{n+1}$ such that $g^{(n+1)}(c_{n+1}) = 0$.
	Let $c = c_{n+1}$, and so we have achieved our goal.
\end{proof}
Now this seems a bit magic.
You had to cook up a magic function $g$ and it solves it for you.
But Zahl would do this by looking at $n=1$, maybe $n=2$ and $n=3$,
find something that makes the derivative vanish.
How do we remember how to prove this?
You really want to chunk things into a small number of ideas:
you want to use Rolle's theorem (many times),
want to cook up an axuilliary function $g$ that satisfies Rolle's theorem hypothesis each time,
and given 20 minutes, you could do it.

Given random people and chess grandmasters and random chessboards,
did equally bad in recreating the board by memory.
But given a chessboard that was halfway into a game,
chess grandmasters did significantly better.
The takeaway: the grandmasters weren't remembering the exact location of every piece,
they were remembering by chunking and creating a narrative.

\noindent Examples ($x_0 = 0$):
\begin{enumerate}
	\item $f$ a polynomial of degree $D$.
		$P_n(t)$ is the first terms of $f$, up to degree $n$.
		So the Taylor expansion eventually becomes the polynomial.
	\item $f(t) = e^t$.
		$P_n(t) = \frac{1}{0!} + \frac{t}{1!} + \frac{t^2}{2!} + \cdots + \frac{t^n}{n!}$.
	\item $f(t) = \sin{t}$.
		$P_n(t) = 0 + t + 0 - \frac{t^3}{3!} - 0 + \frac{t^5}{5!}$.
\end{enumerate}

We have talked about convergence in metric spaces.
$P_n$ is a sequence, so how do we talk about $P_n$ converging to $f$?
When does it? What metric space are we in?
We could let our metric space be infinitely differentiable real functions,
with the metric $d(f,g) = \sup_{x\in D}|f(x) - g(x)|$.
The first example converges, since it is eventually $0$.
What about $e^t$?
Well, if we take $n$ before $x$, not true,
exponential vs polynomial.
But if fix a closed interval, then we do get an $n$.
This is the difference between pointwise and uniform convergence.
Our metric was normally used for bounded functions, so this is necessary.
We will pick up this discussion next time.

\section{January 12}
We ended last class with some discussion of convergence of functions.
This is something he wants to defer until later in the term,
but we get some of this on the homework.

Recall he asked two lectures ago if $f \colon \R \to \R$ (or $[-1,1] \to \R$),
and $f(0) = 0, f'(0) = 0, \dots, f^{(k)}(0) = 0$ for all $k$,
must it be true that $f(t) = 0$ for all $t$.
Taylor's theorem makes us think this is true:
otherwise, $f(x)$ is wholly its error term always.
But consider
\[
	f(x) = \begin{cases} 0, & \text{if }x \leq 0\\ e^{-1/x} & \text{if } x > 0 \end{cases}
\]
We can see that this is infinitely differentiable on all of $\R$
(consider the domains seperately).
For $x \leq 0$, $f^{(k)}(x) = 0$ and for $x > 0$, $f^{(k)}(x) = Q(x)e^{-1/x}$
where $Q(x)$ is a rational function.
And exponential will shrink always faster than $Q(x)$,
so goes to $0$ at origin as well.
Proving this is something he's given in homework, but it eats a whole week,
and so probably not this year.

It is valuable in analysis (or all math) to have a large bank of
interesting examples at your finger tips
that exemplify the weird behaviour of how functions behave (or things behave).
Some theorems say this is what happens with these four examples,
and then everything else can be derived from these interesting examples.

\subsection{The Riemann and Riemann-Stieltjes Integral}
\subsubsection{The Riemann Integral}
\begin{definition}[Rudin 6.1]
	A \emph{partition} of $[a,b]$ is a finite set
	\[
		P = \{x_0, x_1, x_2, \dots, x_n\}
	\]
	with $a = x_0 < x_1 < x_2 < \cdots < x_n = b$.
\end{definition}
For $i = 1, \dots, n$, let $\Delta x_i = x_i - x_{i-1}$.
For $f \colon [a,b] \to \R$ bounded, define
\begin{align*}
	M_i &= \sup\{f(x) \colon x \in [x_{i-1}, x_i]\}\\
	m_i &= \inf\{f(x) \colon x \in [x_{i-1}, x_i]\}
\end{align*}
And these will always exist in $\R$, since the interval is nonempty
and $f(x)$ is bounded.
Define the upper and lower Reimann sums of the partition $P$ and function $f(x)$:
\begin{align*}
	U(P,f) &= \sum_{i=1}^n M_i \Delta x_i\\
	L(P,f) &= \sum_{i=1}^n m_i \Delta x_i\\
\end{align*}
Intuition: drawing boxes for each $\Delta x_i$ whose height is
$M_i$ or $m_i$.
As we take smaller widths of the boxes, we would like to see these two values converge.

We define the upper Riemann integral
\[
	\overline{\int_a^b}fdx = \inf_P U(P,f)
\]
and the lower Riemann integral
\[
	\underline{\int_a^b}fdx = \sup_P L(P,f)
\]
where the supremum and infinum is taken voer all partitions of $[a,b]$. 

\begin{definition}
	We say $f \colon [a,b] \to \R$ is \emph{Riemann integrable}	if
	\[
		\overline{\int_a^b}fdx = \underline{\int_a^b}fdx
	\]
	(both should always exist),
	in which case we denote this number by
	\[
		\int_a^b fdx
	\]
	and we say $f \in \mathcal{R}[a,b]$
	(the set of Riemann integrable functions on $[a,b]$).
\end{definition}
Example: $[a,b] = [0,1]$, $f(x) = x$.
If $P = \{x_0,\dots,x_n\}$ is a partition,
$M_i = x_i$ and $m_i = x_{i-1}$.
For the sake of concreteness, consider the evenly-spaced partition
$\{0, \frac1n, \frac2n, \dots, \frac{n}{n}\}$.
Then
\[
	U(P,f) = \sum_{i=1}^n \frac{i}{n}\frac{1}{n}
	= \frac{1}{n^2}\sum_{i=1}^n i = \frac{1}{n^2}\frac12 n(n+1)
	= \frac12 + \frac{1}{2n}
\]
In particular, $\overline{\int_a^b}fdx
\leq \inf\{\frac12+\frac{1}{2n} \colon n \in \N\} = \frac12$.
Also
\[
	L(P,f) = \sum_{i=1}^n \frac{i-1}{n}\frac{1}{n}
	= \frac{1}{n^2}\frac{1}{2}n(n-1)
	= \frac12 - \frac{1}{2n}
\]
In particular, $\underline{\int_a^b}fdx
\geq \sup\{\frac12-\frac{1}{2n} \colon n \in \N\} = \frac12$.
Can we now conclude that $f \in \mathcal{R}[0,1]$
and $\int_a^b fdx = \frac12$?
Well, we need that $\underline{\int_a^b}fdx \leq \overline{\int_a^b}fdx$
always, which we don't have yet.
[Is this not a result of the $\liminf \leq \limsup$ inequality?]
We will prove this actually for a more general class of itnegrals.

\subsubsection{The Riemann-Stieltjes Integral}
\begin{definition}[Rudin 6.2]
	Let $\alpha \colon [a,b] \to \R$ be (weakly) monotone increasing,
	let $P = \{x_0, \dots, x_n\}$ be a partition of $[a,b]$.
\end{definition}
For $i = 1,\dots, n$, let $\Delta \alpha_i = \alpha(x_i) - \alpha(x_{i-1})$
(if $\alpha(x) = x$, then $\Delta \alpha_i = \Delta x_i$).

For $f \colon [a,b] \to \R$ bounded define
\begin{align*}
	U(P,f,\alpha) &= \sum_{i=1}^n M_i \Delta \alpha_i\\
	L(P,f,\alpha) &= \sum_{i=1}^n m_i \Delta \alpha_i\\
\end{align*}
(where $M_i,m_i$ are the same as before).
As before, we define
\begin{align*}
	\overline{\int_a^b}fd\alpha &= \inf_P U(P,f,\alpha)\\
	\underline{\int_a^b}fd\alpha &= \sup_P L(P,f,\alpha)\\
\end{align*}
If
\[
	\overline{\int_a^b}fd\alpha = \underline{\int_a^b}fd\alpha
\]
then we denote the common value by $\int_a^b fd\alpha$
and we say $f \in \mathcal{R}_\alpha[a,b]$.
When $\alpha(x) = x$, we get the Riemann integral from before.
But consider $\alpha(x) =\begin{cases} 0, & x \leq 0\\ 1, & x > 0\end{cases}$.
What does $\int_0^1 fd\alpha$ look like ($f$ continuous).
This actually evalues to $f(0)$ (Dirac delta from physics).

\section{January 15}
\begin{definition}[Rudin 6.3]
	Let $P$ and $P^*$ be partitions of $[a,b]$.
	We say $P^*$ is a \emph{refinement} of $P$ if $P \subset P^*$.
	If $P_1$ and $P_2$ are partitions of $[a,b]$, the \emph{common refinement}
	is the partition $P_1 \cup P_2$.
\end{definition}
\begin{theorem}[Rudin 6.4]
	Let $P^*$ be a refinement of $P$.
	Then $L(P,f\alpha) \leq L(P^*,f\alpha) \leq U(P^*,f,\alpha) \leq U(P,f,\alpha)$.
\end{theorem}
\begin{proof}
	The middle inequality, we have already seen.

	Since partitions are finnite, it sufficies to prove the inequality
	when $P^*$ has one additional point
	(i.e. $x_i < x^* < x_{i+1}$ and $P^* = P \cup \{x^*\}$).
	
	Lets compare $L(P,f,\alpha)$ vs $L(P^*,f,\alpha$.
	Recall $m_j = \in\{f(x) \colon x \in[x_{j-1},x_j]\}$. See
	\[
		L(P,f,\alpha) = \sum_{j=1}^n m_j \Delta \alpha
	\]
	\begin{align*}
		L(P^*,f,\alpha) = \sum_{j=1}^i m_j\Delta \alpha_j
		&+ (\inf\{f(x) \colon x \in [x_i,x^*]\})(\alpha(x^*) - \alpha(x_i))\\
		&+ (\inf\{f(x) \colon x \in [x^*,x_{i+1}]\})(\alpha(x_{i+1}) - \alpha(x^*))\\
		&+ \sum_{j=i+2}^n m_j \Delta \alpha_j
	\end{align*}
	Then
	\begin{align*}
		L(P^*,f,\alpha) - L(P,f,\alpha)
		&= \left(\inf_{x\in[x_i,x^*]} f(x) \right)(\alpha(x^*)-\alpha(x_i))
		+ \left(\inf_{x\in[x^*,x_{i+1}]} f(x) \right)(\alpha(x_{i+1})-\alpha(x^*))
		- m_{i+1}\Delta \alpha_{i+1}\\
		&\geq \left(\inf_{x\in[x_i,x_{i+1}]} f(x) \right)(\alpha(x^*)-\alpha(x_i))
		+ \left(\inf_{x\in[x_i,x_{i+1}]} f(x) \right)(\alpha(x_{i+1})-\alpha(x^*))
		- m_{i+1}\Delta \alpha_{i+1}\\
		&= m_{i+1}(\alpha(x^*) - \alpha(x_i) + \alpha(x_{i+1} - \alpha(x^*))
		- m_{i+1} \Delta \alpha_{i+1}\\
		&= m_{i+1}\Delta \alpha_{i+1} - m_{i+1}\Delta \alpha_{i+1} = 0
	\end{align*}
	where we are using the fact that adding points to a set
	can only decrease its infinum.
	Lastly, the final inequality is proven similarly.
\end{proof}

Recall we wanted to prove something before we moved to the Riemann-Stieltjes integral.
\begin{theorem}[Rudin 6.5]
	Let $f \colon [a,b] \to \R$ bounded,
	$\alpha \colon [a,b] \to \R$ monotone increasing. Then
	\[
		\underline{\int_a^b} fd\alpha \leq \overline{\int_a^b}fd\alpha
	\]
\end{theorem}
\begin{proof}
	Let $P_1$ and $P_2$ be partitions of $[a,b]$, leet $P^* = P_1 \cup P_2$.
	By Theorem 6.4, $L(P_1,f,\alpha) \leq U(P_2,f,\alpha)$, jhence
	\[
		\underline{\int_a^b}fd\alpha = \sup_{P_1} L(P_1,f,\alpha)
		\leq U(P_2,f,\alpha)
	\]
	See that this is true for every $P_2$.
	\[
		\underline{\int_a^b}fd\alpha \leq \inf_{P_2} U(P_2,f,\alpha)
		= \overline{\int_a^b}fd\alpha
	\]
\end{proof}
This was the missing piece to show $\int xdx = \frac12$.

Now we are going to look at some facts about the Riemann-Stieltjes integral,
and they will slowly get less intuitive.
\begin{theorem}[Rudin 6.6]
	Let $f \colon [a,b] \to \R$ bounded and
	$\alpha \colon [a,b] \to \R$ monotonically increasing. Then
	\[
		f \in \mathcal{R}_\alpha[a,b] \iff
		\forall \ep, \exists P \text{ such that }U(P,f,\alpha) - L(P,f\alpha) < \ep
	\]
\end{theorem}
\begin{proof}
	Forward direction: by hypothesis
	\[
		\sup_P L(P,f,\alpha) = \int_a^b fd\alpha = \inf_P U(P,f,\alpha)
	\]
	Let $\ep > 0$. Then $\exists$ a partition $P_1$ such that
	\[
		L(P_1,f,\alpha) > \int_a^b fd\alpha - \ep/2
	\]
	$\exists P_2$ such that $U(P_2,f,\alpha < \int_a^b fd\alpha + \ep/2$.
	Let $P = P_1 \cup P_2$.
	By theorem 6.4
	\[
		L(P_1,f,\alpha) \leq L(P,f,\alpha)
		\leq U(P,f,\alpha) \leq U(P_2,f,\alpha)
	\]
	Hence $U(P,f,\alpha) - L(P,f,\alpha) < \ep$.

	Backwards direction follows from definition.
\end{proof}

\section{January 17}
We are going to continue understanding the Riemann-Stieltjes integral.
We showed when it existed (upper and lower Riemann-Stieltjes integral agrees),
but we don't know a lot about functions that are Riemann-Stieltjes integrable.
We show this now for a large class of functions.
\begin{theorem}[Rudin 6.8]
	Let $\alpha \colon [a,b] \to \R$ be monotonically increasing.
	Let $f \colon [a,b] \to \R$ be continuous.
	Then $f \in \mathcal{R}_\alpha[a,b]$,
	i.e. $C([a,b]) \subset \mathcal{R}_\alpha[a,b]$.
\end{theorem}
\begin{proof}
	Let $f$ be continuous, and $[a,b]$ compact.
	Hence, $f$ is uniformly continuous.
	Hence, for all $\ep_1 > 0$, $\exists \delta > 0$ such that
	$|f(x) - f(y)| < \ep_1$ for all $x,y$ with $|x-y|<\delta$.
	Thus, if $P$ is a partition with $\Delta x_i < \delta$ for all $i$,
	then $M_i - m_i < \ep_1$ for all $i$
	(hmm might be important here for the strict inequality that $f$
	attain max/min because compact set... closed subset of compact).
	Hence $U(P,f,\alpha) - L(P,f,\alpha) =
	\sum_i M_i\Delta\alpha_i - \sum_i m_i \Delta\alpha_i
	\leq \sum_{i=1}^n \ep_1\Delta \alpha_i = \ep_1(\alpha(b) - \alpha(a))$.
	Given $\ep > 0$, select $\ep_1$ sufficiently small so that
	$\ep_1(\alpha(b) - \alpha(a)) < \ep$.
	Choose $P$ as above for the corresponding $\ep_1$.
	We have shown:
	for $\ep > 0$, $\exists$ a partition $P$ such that
	$U(P,f,\alpha) - L(P,f,\alpha) < \ep$.
	So by Theorem 6.6, $f \in \mathcal{R}_\alpha[a,b]$.
\end{proof}

Question: Can we describe/characterze $\mathcal{R}_\alpha[a,b]$ or $\mathcal{R}[a,b]$?
We will pick this up later.

For now, we will expand the functions that are Riemann integrable.
\begin{theorem}[Rudin 6.9]
	Let $f \colon [a,b] \to \R$ be monotone (increasing or decreasing)
	and $\alpha \colon [a,b] \to \R$ monotone (weakly) increasing and continuous.
	Then $f \in \mathcal{R}_\alpha[a,b]$.
\end{theorem}
\begin{remark}
	This theorem neither implies or is implied by the previous theorem.
\end{remark}
\begin{proof}
	Let $n \in \N$. By the intermediate value theorem,
	$\exists$ a partition $P$ such that
	$\Delta \alpha_i = \frac{\alpha(b) - \alpha(a)}{n}$ for all $i=1,\dots,n$.
	\begin{align*}
		U(P,f,\alpha) - L(P,f,\alpha)
		&= \sum_{i=1}^n (M_i - m_i)\Delta \alpha_i\\
		&= \frac{\alpha(b) - \alpha(a)}{n}\sum_{i=1}^n (M_i - m_i)
	\end{align*}
	Suppose, WLOG, that $f$ is monotone increasing,
	then $M_i = f(x_i), m_i = f(x_{i-1})$.
	So our value from before becomes
	\begin{align*}
		U(P,f,\alpha) - L(P,f,\alpha)
		&= \frac{\alpha(b) - \alpha(a)}{n} \sum_{i=1}^n\left(f(x_i) - f(x_{i-1})\right)\\
		&= \frac{\alpha(b) - \alpha(a)}{n}
		\left(f(x_1) - f(x_0) +f(x_2) - f(x_1) +- \cdots +f(x_n) - f(x_{n-1})\right)\\
		&= \frac{\alpha(b) - \alpha(a)}{n}\left(f(x_n) - f(x_0)\right)\\
		&= \frac{\alpha(b) - \alpha(a)}{n}\left(f(b) - f(a)\right)\\
		&= \frac{1}{n}\underbrace{\left(\alpha(b) - \alpha(a)\right)
	\left(f(b) - f(a)\right)}_{\in\R}
	\end{align*}
	Given $\ep > 0$, select $n \in \N$ such that
	\[
		\left\lvert \left(\alpha(b) - \alpha(a)\right)\left(f(b) - f(a)\right)\right\rvert < \ep
	\]
	For such a partition $P$, $U(P,f,\alpha) - L(P,f,\alpha) < \ep$.
	By Theorem 6.6, $f \in \mathcal{R}_\alpha[a,b]$.
	If $f$ is monotonically decreasing, an identical proof works.
\end{proof}

\begin{theorem}[Rudin 6.10]
	Let $f \colon [a,b] \to \R$ be bounded and continuous at all
	but finitely many points.
	Let $\alpha \colon [a,b] \to \R$ be monotone increasing
	and must be continuous at every point where $f$ is not continuous.
	Then $f \in \mathcal{R}_\alpha[a,b]$.
\end{theorem}
Probably won't prove today, but some examples.
ff

And then tangent into monotonically increasing functions with countably many discontinuitys.
Can show that the Devil's staircase $\alpha \colon [0,1] \to [0,1]$
is monotonically increasing and discontinuous.
Remember he mentioned that it is useful to have examples in your head.
For Riemann-Stieltjes integral, $\alpha(x) = x$ is a good one (Riemann integral),
but also this Cantor-Lebesgue function.

\section*{January 22}
\subsection{Properties of the Riemann-Stieltjes Integral}
Assume that $\alpha \colon [a,b] \to \R$ is monotonically increasing,
and $f,f_1,f_2 \colon [a,b] \to \R$ are $f,f_1,f_2 \in \mathcal{R}_\alpha[a,b]$.
Then we get some more functions that are integrable.
\begin{enumerate}
	\item[(a).] Linearity: $f_1 + f_2 \in \mathcal{R}_\alpha[a,b]$
		and $\int_a^b(f_1+f_2)d\alpha = \int_a^b f_1d\alpha + \int_a^bf_2d\alpha$;
		$c \in \R$ then $cf \in cf\in\mathcal{R}_\alpha[a,b]$
		and $\int_a^b cfd\alpha = c\int_a^bfd\alpha$.
	\item[(b).] Non-negativity: If $f(x) \geq 0$ for all $x \in [a,b]$
		then $\int_a^b fd\alpha \geq 0$.
		This then implies if $f_1(x) \leq f_2(x)$ for all $x \in [a,b]$
		then $\int f_1d\alpha \leq \int f_2d\alpha$
	\item[(c).] For $c \in (a,b)$, then $f \in \mathcal{R}_\alpha[a,c]$
		and $f \in \mathcal{R}_\alpha[c,b]$
		and $\int_a^c fd\alpha + \int_c^b fd\alpha = \int_a^bfd\alpha$
	\item[(d).] Boundedness: If $|f| \leq M$ then
		$\left\lvert \int_a^b fd\alpha\right\rvert \leq M(\alpha(b) - \alpha(a))$.
	\item[(e).] Let $\alpha_1,\alpha_2 \colon [a,b] \to \R$ and they're monotone increasing,
		and $f \colon [a,b] \to \R$ where $f \in \mathcal{R}_{\alpha_1}[a,b]$
		and $f \in \mathcal{R}_{\alpha_2}[a,b]$.
		Then $f \in \mathcal{R}_{\alpha_1+\alpha_2}[a,b]$
		and $\int_a^b fd(\alpha_1 + \alpha_2) = \int_a^b fd\alpha_1 +
		\int_a^b fd\alpha_2$.
		If $c \in \R$, then $f \in \mathcal{R}_{c\alpha_1}[a,b]$
		and $\int_a^b fd(c\alpha_1) = c\int_a^bfd\alpha_1$.
\end{enumerate}
Proof: See Rudin; this is theorem 6.12.

Now we will have an informal discussion about what this all tells us.
Why bother with a more complicated version of an integral,
and why this specific setup?
Recall from 320 $C([a,b])$, the space of continuous functions $f \colon [a,b] \to \R$.
We define $\lVert f \rVert_{C([a,b])} = \sup_{x\in[a,b]}\lvert f(x)\rvert$
and so $d(f,g) = \lVert f - g \rVert_{C([a,b])}$
(also called the $C_0$ norm).
This space is a vector space, infinite dimensional
(field is reals for now, but can extend to complex).
Thus, $(C([a,b]), \lVert \cdot \rVert_{C([a,b])})$ is a normed vector space.
In linear algebra, we are often interested in linear functions from vector spaces,
or here, linear transforms between normed vector spaces.
Property (a) from theorem 6.12 says:
If $\alpha \colon [a,b] \to \R$ is monotone increasing,
then the function $T(f) = \int_a^b fd\alpha$ is a linear function
from the vector space $C([a,b])$ to $\R$ (for now, we will restrict our domain),
i.e. $T(f+g) = T(f) + T(g), T(cf) = cT(f)$.
Property (d) says that $T$ is bounded,
i.e. $|Tf| \leq A\lVert f \rVert_{C([a,b])}$,
or more specifically, $|Tf| \leq (\alpha(b) - \alpha(a)\lVert f \rVert_{C([a,b])}$
(since $|f| \leq M$ when $f \in C([a,b])$ means $\lVert f \rVert_{C([a,b])} \leq M$).
Notation: people sometimes write $Tf$ instead of $T(f)$
(in linear algebra, we write $Mv$).
Property (b) says that $T$ is non-negative, i.e.
if $f \in C([a,b])$ with $f(x) \geq 0$ for all $x \in [a,b]$, then $Tf \geq 0$.

Why have we changed our wording from theorem 6.12 to this?
In functional analysis (Math 421) and more generally in physics,
we want to study linear functions whose domain is $C([a,b])$ (or more general)
and whose codomain is often $\R$ or $\C$.
Functions of this type are called ``(linear) operators" or ``(linear) functionals".
Can't say too much right now about why this is an interesting area of study,
but a starting point in modern quantum mechanics is,
instead of thinking of particles as points, we think of it as a function.
In classic mechanics, a particle might be described with $6$ numbers
($x,y,z,v_x,v_y,v_z$),
and if you were to ask what the position of this particle, we just take the first three
(and likewise with the velocity/momentum).
Instead, we now consider transformations on the function.

\begin{theorem}[Riesz Representation Theorem V1]
	Let $T \colon C([a,b]) \to \R$ be linear, bounded, and non-negative.
	Then, there exists $\alpha \colon [a,b] \to \R$ monotonically increasing
	such that $Tf = \int_a^b fd\alpha$.
\end{theorem}
The whole point of this theorem is that you can go in the other direction.
$f$ and $\alpha$ uniquely define a linear bounded non-negative operator $T \colon [a,b] \to \R$,
and $T$ uniquely determines $f,\alpha$.

This non-negative condition seems a little weird...
what happens when our codomain is $\C$?
\begin{theorem}[Riesz Representation Theorem V2]
	Let $T \colon C([a,b]) \to \R$ be linear and bounded.
	``Let $T$ be a real-valued linear functional on $C([a,b])$."
	Then there exists $\alpha,\beta \colon [a,b] \to \R$ monotone increasing such that
	\[
		T(f) = \int_a^b fd\alpha - \int_a^b fd\beta = ``\int_a^bfd(\alpha-\beta)"
	\]
	(but careful, because $\alpha - \beta$ is not necessarily monotone increasing,
	but can just define difference of monotone increasing functions;
	call these ``functions of bounded variation").
\end{theorem}
How would we go about proving this?
Comes from Hanh-Banach Theorem from functional analysis.
Banach spaces are complete normed vector spaces.
If you have a linear function from a dense subspace to the reals,
can extend this to the entire space to the reals.
Won't prove this... requires axiom of choice
(even though Riesz Representation doesn't require it).
And then we're in good shape...
get $T$ is the integral with various test functions on dense subspace.

\section{January 24}

\begin{theorem}[Rudin 6.13]
	Let $f,g \in \mathcal{R}_\alpha[a,b]$.
	\begin{enumerate}
		\item[(a).] Then $fg \in \mathcal{R}_\alpha[a,b]$.
		\item[(b).] Then $\lvert f \rvert \in \mathcal{R}_\alpha[a,b]$
			and $\lvert \int_a^bfd\alpha \rvert \leq \int_a^b \lvert f \rvert \alpha$
	\end{enumerate}
\end{theorem}
\begin{proof}
	First proof of (a).
	By theorem 6.11, $\alpha(x)=x^2$, $(f+g)^2,(f-g)^2 \in \mathcal{R}_\alpha[a,b]$.
	By theorem 6.12a, $(f+g)^2-(f-g)^2 = 4fg \in \mathcal{R}_\alpha[a,b]$.
	By theorem 6.12a, ($c = \frac14$) $fg \in \mathcal{R}_\alpha[a,b]$.

	Now proof of (b).
	By theorem 6.11 $\alpha(x) = \lvert x \rvert$, $\lvert f \rvert \in \mathcal{R}_\alpha[a,b]$.
	Let $c = \mathrm{sgn}\int_a^b fd\alpha$ so
	$\lvert \int_a^b fd\alpha = c\int_a^bfd\alpha = \int_a^bcfd\alpha
	\leq \int_a^b \lvert f \rvert d\alpha$ (by theorem 6.12a).
\end{proof}

\begin{theorem}[Rudin 6.15]
	Let $f \colon [a,b] \to \R$ be bounded.
	Let $s \in (a,b)$ and suppose $f$ is continuous at $s$.
	Let $\alpha(x) = \begin{cases} 0 & x \leq s \\ 1 & x > s\end{cases}$.
	Then $f \in \mathcal{R}_\alpha[a,b]$ and $\int_a^b fd\alpha = f(s)$.
\end{theorem}
\begin{proof}
	Let $P = \{x_0,x_1,x_2,x_3\}$ where $a = x_0, s = x_1, b = x_3$.
	Then $U(P,f,\alpha) = \sum_{i=1}^3 M_i \Delta \alpha_i = M_2 = \sup_{x\in[x_1,x_2]}f(x)$
	and $L(P,f,\alpha) = \sum_{i=1}^3 m_i \Delta \alpha_i = m_2 = \inf_{x\in[x_1,x_2]}f(x)$.
	Since $f$ is continuous at $s$, $\forall \ep > 0$, $\exists \delta$
	such that if $|x-y| < \delta$, then $|f(x) - f(y)| < \frac{\ep}{2}$,
	so let $x_2 \in (x_1,x_1+\delta)$.
	Then,
	\[
		\sup_{x\in[x_1,x_2]}f(x) \leq f(x_1) + \ep/2 \implies M_2 \leq f(x_1) + \ep/2
	\]
	\[
		\inf_{x\in[x_1,x_2]}f(x) \geq f(x_1) - \ep/2 \implies m_2 \geq f(x_1 - \ep/2
	\]
	Hence, $M_2 - m_2 \leq \ep$.
\end{proof}
How would we change proof if $f(x) = 1$ for $x \geq s$?
We would get the same result, but would need to change which switch $x_1,x_2$ roles.
If defined at neither, then probably $s \in [x_1,x_2]$.

This step function is actually quite important in electrical engineering.
Even has a special name:
\begin{definition}[Heavyside step function]
	\[
		I(x) = \begin{cases} 0, & x \leq 0\\ 1, & x > 0\end{cases}
	\]
\end{definition}

\begin{theorem}[Rudin 6.16]
	Let $\{c_n\}_{n=1}^\infty$ be positive real numbers, with $\sum_{n=1}^\infty c_n < \infty$.
	Let $[a,b]$ be an interval, and let $\{s_n\}_{n=1}^\infty \subset (a,b)$
	be distinct points.
	Let $\alpha(x) = \sum_{n=1}^\infty c_nI(x-s_n)$
	(a bunch of steps at $s_n$ by $c_n$).
	Let $f \colon [a,b] \to \R$ be continuous,
	so $f \in \mathcal{R}_\alpha[a,b]$
	(always true, $f$ is continuous and $\alpha$ monotone increasing).
	Then
	\[
		\int_a^b fd\alpha = \sum_{n=1}^\infty cf(s_n)
	\]
\end{theorem}
(We know the integral exists,
and the sum exists because $\sum c_n < \infty$ and $f$ is bounded.)
``I love nitpicking, because math is meant to be precise."
\begin{proof}
	Let $R_N = \int_a^bfd\alpha - \sum_{n=1}^N c_nf(s_n)$.
	Goal: $\forall \ep > 0$, $\exists N_0$ such that
	$\forall N \geq N_0$m $\lvert R_N \rvert < \ep$.
	So fix $N$, let $\alpha_1 = \sum_{n=1}^N c_nI(x-s_n)$,
	$\alpha_2 = \sum_{n=N+1}^\infty c_n I(x-s_n)$.
	By Theorem 6.12e, $\int_a^b fd\alpha = \int_a^b fd\alpha_1 + \int_a^bfd\alpha_2$
	(we meet the hypothesis that $f \in \mathcal{R}_{\alpha_1},\mathcal{R}_{\alpha_2}$,
	since $f$ is continuous).
	\[
		\int_a^b fd\alpha_1 = \sum_{n=1}^N \int_a^b f(x) d[c_nI(x-s_n)]
		= \sum_{n=1}^N c_n f(s_n)
	\]
	(by Theorem 6.15).
	So $R_N = \int_a^b fd\alpha_2$.
	Let $K = \sup_{x \in [a,b]} \lvert f \rvert$.
	By 6.12b, $\int_a^b fd\alpha_2 \leq K \int_a^B 1 d\alpha_2
	= K(\alpha_2(b) - \alpha_2(a)) = K\sum_{n=N+1}^\infty c_n \to 0$ as $N \to \infty$.
	Overview: we wrote our difference in terms of a main term and tail.
	And we showed the tail goes to zero
	(we can make our proof more formal with $\ep$, etc., but out of time).
\end{proof}

\section{January 26}
\begin{theorem}[Rudin 6.17]
	Let $f \colon [a,b] \to \R$ bounded, $\alpha \colon [a,b] \to \R$
	differentiable and monotone increasing.
	Suppose $\alpha' \in \mathcal{R}[a,b]$.
	Then $f \in \mathcal{R}_\alpha[a,b] \iff f\alpha' \in \mathcal{R}[a,b]$
	and if so
	\[
		\int_a^b fd\alpha = \int_a^b f\alpha' dx
	\]
\end{theorem}
Step 1: it sufficies to show that
\[
	\overline{\int_a^b}fd\alpha = \overline{\int_a^b}f\alpha' xdx
	\qquad \text{ and } \qquad
	\underline{\int_a^b}fd\alpha = \underline{\int_a^b}f\alpha'xdx
\]
We will prove the first equality, second one is an exercise.

Step 2: since $\alpha' \in \mathcal{R}[a,b]$
for all $\ep > 0$, $P$ (of $[a,b]$) such that
\[
	U(P,\alpha') - L(P,\alpha') < \ep
\]
This inequality continues to hold for every refinement $P'$ of $P$.

We have $U(P,\alpha') - L(P,\alpha') = \sum_{i=1}^n(A_i - a_i)\Delta x_i$,
where $A_i = \sup\{\alpha'(x) \colon x \in [x_{i-1},x_i]\}$
and likewise for $a_i$.
By the mean value theorem for each $i = 1,\dots, n$
$\exists t_i \in [x_{i-1},x_i]$ such that
$\Delta \alpha_i = \alpha'(t_i)\Delta x_i$.
Now this suggests $\overline{\int}fd\alpha = \overline{\int}f\alpha'dx$,
but need to be careful.

For every $s_i \in [x_{i-1},x_i]$, we have
$|\alpha'(s_i) - \alpha'(t_i)| \leq A_i - a_i$
(literally did this so many times on the homework lol)
so $\sum_{i=1}^n |\alpha'(s_i) - \alpha'(t_i)|\Delta x_i
\leq \sum_{i=1}^n (A_i - a_i)\Delta x_i$
for \emph{every} choice of $s_i \in [x_{i-1},x_i]$, $i = 1,\dots,n$.
Let $K = \sup_{x \in [a,b]}|f|$, then
\begin{equation}\label{6.17 ineq}
	\sum_{i=1}^n |f(s_i)\alpha'(s_i)\Delta x_i - f(s_i)\alpha'(t_i)\Delta x_i|
	\leq K\ep
\end{equation}
Hence
\[
	\sum_{i=1}^n f(s_i)\Delta \alpha_i \leq \sum_{i=1}^n f(s_i)\alpha'(s_i)\Delta x_i + Ke
	\leq U(P,f\alpha') + K\ep
\]
Recall triangle inequality:
if $|a+b| \leq c$, then $a \leq b + c$ and $b \leq a + c$.

When we have an inequality, might wonder where the inequality is
``sharp" or ``tight":
i.e. equality or the closest to equality as possible.
Taking the supremum of $s_i \in [x_{i-1},x_i]$, $i = 1, \dots, n$,
we conclude (if $M_i = \sup\{f(x) \colon x \in [x_{i-1},x_i]\}$)
\[
	U(P,f,\alpha) = \sum_{i=1}^n M_i \Delta \alpha_i \leq U(P,f\alpha') + K\ep
\]
Hence
\[
	\overline{\int_a^b}fd\alpha \leq U(P,f,\alpha) \leq U(P,f\alpha') + K\ep
\]
So for every $\ep$, we found some partition $P$ that makes the inequality hold.
Recall that it also holds for every refinement $P'$ of $P$.

How can we make this inequality be as close to equality as possible
(how much strength can squeeze out of inequality).
Taking the infinum over all refinements $P'$ of $P$, we have
\[
	\overline{\int_a^b}fd\alpha \leq \int_{P'} U(P',f\alpha) + K\ep
\]
We are taking a more restricted set of partitions,
will this give us the infinum we want?
Yes: for any non-refinement partition,
just union it with $P$ to get a refinement,
i.e. $\forall \ep > 0$, $\overline{\int_a^b}fd\alpha \leq \overline{\int_a^b}f\alpha'dx + KE
\implies \overline{\int_{a}^b} fd\alpha \leq \int_a^b \alpha'dx$.

Once you've seen this, can do the other three inequalities.
Will just say a word or two about the reverse inequality, i.e.
\[
	\overline{\int_a^b}fd\alpha \geq \overline{\int_a^b}f\alpha'dx
\]
Taking (\ref{6.17 ineq}), get
\[
	\sum_{i=1}^n f(s_i) \alpha'(s_i)\Delta x_i \leq
	\sum_{i=1}^n f(s_i)\Delta\alpha_i + K\ep
\]
Etc.

\section{January 29}
For the next theorem, note that a continuous bijective map
must be strictly monotone (good exercise).
\begin{theorem}[Change of Variables (Rudin 6.19)]
	Let $\phi \colon [A,B] \to [a,b]$, strictly increasing,
	onto (i.e. $1-1$), continuous.
	Let $\alpha \colon [a,b] \to \R$ monotone increasing.
	Let $f \in \mathcal{R}_\alpha[a,b]$.
	Define $g = f \circ \phi \colon [A,B] \to \R$,
	$\beta \colon \alpha \circ \phi \colon [A,B] \to \R$.
	Then $g \in \mathcal{R}_\beta[A,B]$ and
	\[
		\int_A^B gd\beta = \int_a^b fd\alpha
	\]
\end{theorem}

\begin{remark}
	Let $\alpha(x) = x$, and $\phi$ is differentiable.
	Then $d\beta = \phi'(x)dx$ (why???), i.e.
	\[
		\int_a^b fd\alpha = \int_{\phi^{-1}(a)}^{\phi^{-1}(b)}f(x)\phi'(x)dx
	\]
	And this lines up with our normal change of variables formula.
\end{remark}
\begin{proof}
	Partitions $P = \{a=x_0, x_1,\dots,x_n = b\}$ of $[a,b]$
	and partitions $Q$ of $[A,B]$
	are in $1-1$ correspondace via $x_i = \phi(y_i)$.
	We have $\alpha(x_i) = \alpha \circ \phi(y_i) = \beta(y_i)$
	and $\{f(x) \colon x \in [x_{i-1},x_i]\} = \{g(y) \colon y \in [y_{i-1},y_i]\}$.
	Then $U(P,f,\alpha) = U(Q,g,\beta)$ and $L(P,f,\alpha) = L(Q,g,\beta)$.

	Let $\ep > 0$. Since $f \in \mathcal{R}_\alpha[a,b]$,
	there exists a partition $P$ of $[a,b]$ such that
	$U(P,f,\alpha) - L(P,f,\alpha) < \ep$,
	hence $U(Q,g,\beta) - L(Q,g,\beta) < \ep$.
	Then $g \in \mathcal{R}_\beta[A,B]$.
	So first part of theorem done \checkmark

	Finally,
	\[
		\int_A^B g d\beta = \inf_Q U(Q,g,\beta)
		= \inf_P U(P,f,\alpha) = \int_a^b f d\alpha
	\]
\end{proof}

Recall the hypotheses of this theorem:
$\phi$ is continuous, $1-1$ and onto.
Where did we use them?
Where we got a $1-1$ correspondance between $P$ and $Q$,
and makes the rest of the proof fall.

Now we start inching towards the fundamental theorem of calculus.
\begin{theorem}[Rudin 6.20]
	Let $f \in \mathcal{R}[a,b]$.
	For $x \in [a,b]$, define $F(x) = \int_a^x f(t)dt$, $F(a) = 0$.
	Then: $F$ is continuous on $[a,b]$.
	And if $c \in [a,b]$ and $f$ is continuous at $c$,
	then $F$ is differentiable at $c$,
	and $F'(c) = f(c)$.
\end{theorem}
\begin{proof}
	Proving continuity: Let $K = \sup_{t\in[a,b]}\lvert f(t) \rvert$.
	By theorem 6.12c (linear in the domain), for $a \leq x \leq y \leq b$
	\[
		F(y) - F(x) = \int_a^y f(t)dt - \int_a^y f(t)dt
		= \int_x^y f(t)dt
	\]
	Essentially, the biggest this can be is $K$,
	and so the largest the integral can be is $\in_a^b K dt$,
	but have to do this carefully.
	Thus
	\[
		\lvert F(y) - F(x) \rvert = \left\lvert \int_x^y f(t)dt \right\rvert
		\leq \int_x^y \lvert f(t)\rvert dt \leq \int_x^y Kdt = K(y-x)
	\]
	(where we used theorem 6.13 for the first inequality, and 6.12b for the second).
	Hence, for every $\ep > 0$, select $\delta = \ep/K$ (ro $\delta = \ep$ if $K=0$).
	Then if $\lvert x - y \rvert < \delta$,
	$\lvert F(x) - F(y) \rvert < \ep$.

	Proving differentiability at $c$:
	suppose $c \neq b$, i.e. $c \in [a,b)$.
	Let us compute $\lim_{h \searrow 0} \frac{F(c+h) - F(c)}{h}$.
	For $h > 0$, we have
	\[
		\left\lvert\frac{1}{h}[F(c+h) - F(c)] - f(c)\right\rvert
		= \left\lvert\int_c^{c+h}f(t)dt - f(c) \right\rvert
	\]
	In analysis, we often want to exploit some cancellation between
	two quantities we think of as close together.
	We are sampling values of $f$ quite close to $c$,
	and comparing them to $f(c)$.
	How do we exploit cancellation?
	One common technique when a quantity is not changing much over
	a region is take its average and bring it inside an integral to cancel.
	So, since $f(c) = \frac{1}{h}\int_c^{c+h} f(c)dt$, we get
	\[
		= \left\lvert \frac{1}{h} \int_c^{c+h} (f(t) - f(c))dt\right\rvert
		\leq \frac{1}{h} \int_c^{c+h} \lvert f(t) - f(c) \rvert dt
	\]
	(triangle inequality for integrals?)
	Since $f$ is continuous at $c$, $\forall \ep > 0$,
	$\exists \delta > 0$ such that for all $y \in [a,b]$ with $|y-c| < \delta$,
	we have $\lvert f(c) - f(y)\rvert < \ep$.
	So for $h < \delta$, we have
	\[
		\frac{1}{h}\int_c^{c+h} \lvert f(t) - f(c) \rvert dt
		< \frac{1}{h} \int_c^{c+h} \ep dt = \ep
	\]
	i.e. $\forall \ep > 0$, $\exists \delta > 0$ such that $\forall 0 < h < \delta$,
	\[
		\left\lvert \frac{1}{h} \left( F(c+h) - F(c)\right) - f(c) \right\rvert < \ep
	\]
	i.e. $\lim_{h \searrow 0} \frac{F(c+h) - F(c)}{h} = f(c)$.

	If $c \neq a$, i.e. if $c \in (a,b]$,
	an identical argument shows
	$\lim_{h \nearrow 0}\frac{F(c+h)-F(c)}{h} = f(c)$.
\end{proof}

Question: If $f$ is not continuous at $c$,
is $F$ never differentiable at $c$, maybe differentiable at $c$,
or is always differentiable at $c$ (our trichotomy).
\begin{remark}
	If $f(x) = g(x)$ except at one point,
	then $\int_a^b f(x) dx = \int_a^b g(x) dx$.
	I.e. the Riemann integral does not see the value of the function
	at one point (not true generally for Riemann-Stieltjes,
	we saw $\alpha$ is Heaviside step function only cared about
	the function at a single point).
\end{remark}
When we have a jump discontinuity, $F$ will never be continuous at a point.

\section{January 31!}
Typo from last class:
ff

Last class: If $f \colon [a,b] \to \R$ continuous,
and $F(x) = \int_a^x f(t) dt$, then $F'(x) = f(x)$ for all $x \in [a,b]$.

\begin{theorem}[Fundametnal Theorem of Calculus (Rudin 6.21)]
	Let $f \in \mathcal{R}[a,b]$, let $F \colon [a,b] \to \R$ be differentiable
	and suppose $F'(x) = f(x)$ for $x \in [a,b]$.
	Then $\int_a^b f(x)dx = F(b) - F(a)$.
\end{theorem}

\begin{proof}
	By MVT (``Most Valuable Theorem"),
	for any partition $P = \{x_0, \dots, x_n\}$ there are numbers
$t_i \in [x_{i-1},x_i]$ for $i = 1,\dots,n$ such that
$F'(t_i) = (F(x_i) - F(x_{i-1}))/\Delta x_i$. So
	\begin{align*}
		F(b) - F(a)
		&= \sum_{i=1}^n (F(x_i) - F(x_{i-1}))\\
		&= \sum_{i=1}^n F'(t_i)\Delta x_i\\
		&= ff
	\end{align*}
	Then $\left\lvert \int_a^b fdx - (F(b) - F(a)) \right\rvert
	\leq U(P,f) - L(P,f)$.
	$f \in \mathcal{R}[a,b]$ implies that for all $\ep > 0$,
	there exissts a partiont $P$ such that $U(P,f) - L(P,f) < \ep$
	and hence $\left\lvert \int_a^b fdx - (F(b) - F(a)) \right\rvert< \ep$.
\end{proof}

Now we can go and prove the things we know about integration.
The first one, integration by parts, we kinda alraedy have.

\begin{theorem}[Integration by Parts (Rudin 6.22)]
	Let $F,G \colon [a,b] \to \R$ be differentiable.
	Let $f = F'$, $g = G'$, and suppose $f,g \in \mathcal{R}[a,b]$.
	Then
	\[
		\int_a^b F(x)g(x)dx = F(b)G(b) - F(a)G(a) - \int_a^bf(x)G(x)dx
	\]
\end{theorem}
\begin{proof}
	Let $H(x) = F(x)G(x)$. Then $H'(x) = f(x)G(x) + F(x)g(x) \in \mathcal{R}[a,b]$.
	Apply Theorem 6.21 to $H$, then $H(b) - H(a) = ff$
	ff
\end{proof}

In both of these results, we have this hypothesis that $f,g$
are Riemann integrable.
Question: If $F \colon [a,b] \to \R$ is differentiable and $F' = f$,
do we need to repeat $f \in \mathcal{R}[a,b]$,
or does this hold automatically,
i.e. is $F' \in \mathcal{R}[a,b]$ for every $F \colon [a,b] \to \R$ differentiable?

Does there exist $F \colon [a,b] \to \R$ differentiable,
so that $F'$ is discontinuous at every $x \in [a,b]$?
``We've replaced a hard question with a harder question."
Won't figure this out in this class, but the answer is NO.
It is an interesting question:
which sets can be the set of discontinuities of a derivative?
He gives you $S \subset [0,1]$, can you give a $F'$ discontinuous at $S$
(where $F \colon [0,1] \to \R$ is differentiable).
These are call F-$\sigma$ sets.


Perhaps we want the derivative to blow up, then it isn't Riemann integrable.
Here is a function to keep in your back pocket:
\[
	F(x) = \begin{cases}
		0, & x = 0\\
		x^2\sin\frac{1}{x^2}, & x\neq0
	\end{cases}
\]
This function is differentiable, but it's derivative is unbounded.
On $x_n = \frac{1}{\sqrt{\pi n}}$, $F'(x_n)$ blows up.

Another type of counter-example:
$F'$ is bounded, but $F'$ is discontinuous at
so many places that it is not Riemann integrable.
Uncountable isn't enough: they might still be Riemann integrable.
The condition is that it is discontinuous at points with positive Lebesgue measure:
try to cover all the discontinuities with open intervals,
the smallest we can make the intervals will always add up
to a positive value. But this is for Math 420.

For the end of class, we will get some definitions to
explore when the interval of integration is not a closed interval.
\begin{definition}
	If $f \colon [a,\infty) \to \R$ satisifies
	$f \in \mathcal{R}[a,b]$ for all $b>a$,
	then we define
	\[
		\int_a^b f(x)dx = \lim_{b\to\infty} \in_a^b f(x)dx
	\]
	If the limit exists (as a real number), we say that the $\in_a^\infty f(x)dx$ converges.
	If $\in_a^\infty \lvert f \rvert dx$ exists (as a real number),
	then we say $\int_a^\infty f(x)dx$ converges \emph{absolutely}.
\end{definition}
(This is the same idea as conditional/absolute convergence of a sequence.)
We can make an equivalent definition for $\int_{-\infty}^b f(x)dx$.

If $f \colon \R \to \R$ and both
$\int_0^\infty f(x)dx$ and $\int_{-\infty}^0$ converges (absolutely),
we define $\int_{-\infty}^\infty = \int_{-\infty}^0 f(x)dx + \int_0^\infty f(x)dx$,
and we say $\int_{-\infty}^\infty f(x)dx$ converges (absolutely).

Can we costruct a function that converges but not absolutely?
Taking inspiration from series, we can take a step function
of $\frac{(-1)^n}{n}$.

Next time, we'll finally look sequences of functions.

\section{Feburary 2}
\subsection{Sequences and series of functions}
The setup:
Let $E$ be a set ($E = [a,b]$, $ E = \R$).
Let $\{f_n\}_{n=1}^\infty$ be a sequence of functions $f_n\colon E \to S$
(usually $S = \R$, or $\C$, or some metric space $(M,d)$)
and let $f \colon E \to S$.

What does it mean for $f_n$ to converge to $f$, i.e. $f_n \to f$?
If $f_n \to f$, and the functions $f_n$ all have some property,
must it be true that $f$ also has that property?

\subsubsection{Pointwise convergence}
ff missed a whole chunk late to class

\begin{definition}[Pointwise Convergence (Rudin 7.1)]
	Let $\{f_n\}_{n=1}^\infty$ be a sequence of functions $f_n \colon E \to M$.
	If the sequence $\{f_n(x)\}_{n=1}^\infty$ converges for every $x \in E$.
	Then we say $\{f_n\}$ converges pointwise (on $E$).
	If this happens, define $f(x) = \lim_{n\to\infty}f_n(x)$, $f(x) \colon E \to M$,
	and we say $f_n \to f$ pointwise (on $E$).
\end{definition}

Note that there are two definitions one might use for convergence of functions.
\begin{itemize}
	\item $\forall \ep > 0$, $\exists N$, $\forall n \geq N$,
		$\forall x \in E$, $d(f_n(x) - f(x)) < \ep$.
	\item $\forall x \in E$, $\ep > 0$, $\exists N$, $\forall n \geq N$
		$d(f_n(x) - f(x)) < \ep$.
\end{itemize}
The second is the definition we did above.
The first is called uniform convergence, and we will cover later.

Now, let us consider the second question we asked:
for pointwise convergence, what properties hold?
\begin{itemize}
	\item Continuity: $E = \R$. If each $f_n$ is continuous and $f_n \to f$ pointwise,
		must it be true that $f$ is continuous?
		No: $f_n = e^{-nx^2}$ and $f = \begin{cases} 1, &x=0 \\ 0, &x\neq 0 \end{cases}$,
		but $f_n \to f$.
		Also, $f_n = x^n$ and $f = \begin{cases} 1, & x=1\\ 0, & \text{elsewhere}\end{cases}$
		where $E = [0,1]$, but $f_n \to f$.
		In fact, we can make our points of discontinuity arbitrarily where we want.
	\item Boundedness: If each $f_n$ is bounded and $f_n \to f$ pointwose, must $f$ be bounded?
		The converse doesn't hold ($f_n = \frac{x}{n} \to 0$).
		We have $f_n(x) = \begin{cases} x, &x\in[-n,n]\\n, &x\in(n,\infty)\\
		-n, &x\in(-\infty,n)\end{cases}$
		which pointwise converges to $f(x) = x$.
	\item Quantitative boundedness: If each $f_n$ is bounded by $1$,
		i.e. $|f_n(x)| \leq 1, \forall x \in E$,
		and $f_n \to f$ pointwise, must it be true that $|f| \leq 1,
		\forall x \in E$.
		The answer is yes: contradiction, assume $|f| > 1$,
		so $|f(x)| = 1 + \ep > 1$, there will be some $f_n(x) > 1$.
		
		If we change our previous condition to $<1$, this is no longer true,
		can get $|f| = 1$.
	\item Integrability: $f_n \in \mathcal{R}[0,1]$, $f_n \to f$ pointwise,
		must it be true that $f \in \mathcal{R}[0,1]$.
		We don't know many functions that fail to be integrable,
		but one is $f = \begin{cases} 1, & x \in \Q\\ 0, & x \not\in\Q\end{cases}$.
		Note that $f_n(x) = \begin{cases} 1, & x\in\{q_1,\dots,q_n\}\\0\end{cases}$,
		where $q_n$ is an enumeration of the rationals.
	\item Food for thought: assuming $f_n$ and $f$ are both integrable,
		must the values of their integrals converge?
\end{itemize}


\section{February 5}
Say $f_n \to f$ pointwise.
Recall we found that integrability of $f_n$ does not imply integrability of their limit.
But we asked: If $\{f_n\}$ are Reimann integrable and $f$ Riemann integrable,
must it be true that $\lim_{n\to\infty} \int_0^1 f_ndx \to \int_0^1 fdx$.
My counter-example (inspired by height of $\frac{1}{n}$ and width of $n$):
\[
	g_n(x) = \begin{cases}
		n, & 0 < x < \frac{1}{n}\\
		0, &\text{otherwise}
	\end{cases}
\]
It is clear the sequence converges to the $0$ function,
but the area under the curve remains $1$.
If we wanted to make it so our limit doesn't even exist,
then we could use $n^2$, or something like $(-1)^nn$.
On the homework, we add in some extra conditions that make it true,
that is monotonicity.
However, a monotone sequence of functions doesn't necessarily mean
the integrals converge: $f$ may not be in $\mathcal{R}[a,b]$,
as we saw from the counter-example last time.
But if we assume $f \in \mathcal{R}[a,b]$, then it is enough.

We have seen examples of $f_n \to f$ pointwise,
where $f_n$ are continuous or integrable, and the limit is not.
What is going on?
Suppose $f_n \to f$, $f \colon \R \to \R$.
Let $f_n$ continuous at $c \in \R$, is $f$ continuous at $c$?
Well, we know $\lim_{x \to c}f(x) = f(c)$, so $f$ continuous at $c$ if
\[
	\lim_{x \to c} \lim_{n \to \infty} f_n(x) = \lim_{n\to\infty} \lim_{x \to c} f_n(x)
\]
But we cannot interchange the limit, we know examples where this fails,
and this leads to the failiure of $f$ being continuous.

We can do something similar with Riemann integrability,
but we have to be a bit more careful with it.
Recall, $f$ is Riemann integrable (on $[0,1]$) if
$\lim_{m\to\infty} U(P_m,f) - L(P_m,f) = 0$ (from homework).
$f_n \to f$ pointwise, is it true that
\[
	\lim_{m\to\infty} U(P_m, \lim_{n\to\infty} f_n)
	= \lim_{n\to\infty} \lim_{m\to\infty} U(P_m,f_n)
\]
(and likewise with $L(P_m,f_n)$).
Even though the limit is inside, this is taking the limits in different order.
The examples we have been providing have been using the failiure to
interchange limits.
We saw Lipschitz continuity last term: it also fails to hold in
a pointwise limit, because of this interchange of limits.
.

One final example showing you \emph{cannot} (in general)
interchange limits (an example to keep in mind).
\[
	a_{n,m} = \frac{n}{n+m}, \quad n,m \in \N
\]
Easy to see that
\[
	\lim_{n\to\infty} a_{n,m} = 1
	\implies \lim_{m\to\infty} \lim_{n\to\infty} a_{n,m} = 1
\]
\[
	\lim_{m\to\infty} a_{n,m} = 0 \implies
	\lim_{n\to\infty} \lim_{m\to\infty} a_{n,m} = 0
\]
This is somehow the most simple example.
Most of the examples we gave yesterday were of this flavour.

\subsection{Uniform convergence}
It is harder for functions to converge uniformly,
but many of the properties we want carry over.

\begin{definition}[Uniform convergence (Rudin 7.7)]
	Let $E$ be a set, (i.e. $\R$, or $[a,b]$),
	$(M,d)$ a metric space (i.e. $\R,\C$).
	Let $\{f_n\}$ be a sequence of functions $f_n \colon E \to M$ and $F \colon E \to M$.
	We say $f_n \to f$ \emph{uniformly} if:
	$\forall \ep > 0$, $\exists N$ such that $\forall n > N$, $x \in E$,
	$d(f_n(x), f(x)) < \ep$.
\end{definition}

We talked in 320 about sequences that converge,
and talked about sequences that are Cauchy.
If we have a complete metric space,
our Cauchy sequences and convergent sequences are the same.
Something similar happens here:

\begin{theorem}[Cauchy Criteria for Uniform Convergence (Rudin 7.8)]
	Let $E$ be a set, $(M,d)$ a \emph{complete} metric space.
	Let $\{f_n\}$ be a sequence, $f_n \colon E \to M$.
	Then $\{f_n\}$ converges uniformly (to some $f \colon E \to M$)
	iff $\forall \ep > 0$, $\exists N$ such that
	$\forall n,m \geq  N, x \in E$, $d(f_n(x),f_m(x)) < \ep$.
\end{theorem}

Digression: if we consider the functions as a metric space,
say $(X,\rho)$ where $X$ are the functions $f \colon [0,1] \to \R$
and$\rho(f,g) = \sup\lvert f(x) - g(x) \rvert$,
this in fact is a complete metric space and so we do get that
Cauchy sequences are the convergent sequences.
But this is actually the metric corresponding to uniform convergence:
to prove that this is a complete metric space,
probably use the Cauchy Criteria.
If we had changed $X$ to $f \colon [0,1] \to \Q$, $(X,\rho)$
is no longer a complete metric space, as Cauchy Criteria would predict.

\begin{proof}
	Forwards: Suppose $f_n \to F$ uniformly.
	Let $\ep > 0$, $\exists N$ such that $\forall n \geq N, x \in E$
	$d(f_n(x), f(x)) < \ep/2$.
	So $\forall n,m \geq N, x \in E$,
	\[
		d(f_n(x),f_m(x)) \leq d(f_n(x),f(x)) + d(f_m(x),f(x)) <
		\frac{\ep}{2} + \frac{\ep}{2} = \ep
	\]

	Backwards: Suppose $\{f_n\}$ satisfies the Cauchy Criteria.
	For each $x \in E$, $\{f_n(x)\}$ is a Cauchy sequence in $M$
	and $(M,d)$ is complete, so $\{f_n(x)\}$ converges,
	i.e. $\lim_{n\to\infty} f_n(x)$ exists.
	Define $f(x) = \lim_{n\to\infty} f_n(x)$.
	We have shown pointwise convergence.
	Final goal: show that $f_n \to f$ \emph{uniformly}.
	Let $\ep > 0$. Since $\{f_n\}$ satisfies the Cauchy Criteria,
	$\exists N$ such that $n,m \geq N, x \in E$ we have
	\[
		d(f_n(x), f_m(x)) < \frac{\ep}{2}
	\]
	Hence, $d(f_n(x),f(x)) \leq d(f_n(x), f_m(x)) + d(f_m(x),f(x))
	\leq \frac{\ep}{2} + d(f_m(x),f(x))$.
	Since $f_m(x) \to f(x)$ (for this particular $x$),
	we can select $m \geq N$ such that $d(f_m(x),f(x)) < \ep/2$, hence
	\[
		d(f_n(x), f(x)) < \ep
	\]
\end{proof}
Subtle point: we first use the fact that $M$ is complete to show that
$f_n \to f$ pointwise, and then we choose $m$ for the Cauchy Criteria,
but that is okay because it is for fixed $x$.

There is a cluster of true theorems around this:
i.e. don't need complete metric space if $f_n \to f$ pointwise,
and also an equivalent Cauchy Criteria changing quantifiers for pointwise convergence.

\section{Februrary 7}
\begin{theorem}[Rudin 7.11]
	Let $(M_1,d_1)$ and $(M_2,d_2)$ be metric spaces, with
	$(M_2,d_2)$ complete, i.e. $\R$ or $\C$.
	Let $E \subset M$, let $\{f_n\}$ be a sequence of functions
	$f_n \colon E \to M_2$, and suppose $f_n \to f$ uniformly on $E$.
	Let $E \in M_1$ be a limit point of $E$.
	Suppose $\lim_{t\to x}f_n(t) = y_n$ exists for each $n$.
	Then $\{y_n\}$ is a convergent sequence, i.e. $y_n \to y \in M_2$,
	and $\lim_{t\to x}f(t) = y$, i.e.
	\[
		\lim_{t \to x}\underbrace{\lim_{n \to \infty} f_n(t)}_{f(t)} =
		\lim_{n\to\infty} \underbrace{\lim_{t \to x} f_n(t)}_{y_n}
	\]
\end{theorem}

\begin{corollary}
	Let $(M_1,d_1),(M_2,d_2), \{f_n\}, f, E$ be as before.
	If each $f_n$ is continuous on $E$, and $f_n \to f$ uniformly,
	then $f$ is continuous on $E$.
	``The uniform limit of continuous functions is continuous."
\end{corollary}
We first prove that the theorem implies the corollary.
\begin{proof}
	Isolated points are always continuous,
	so we need only to consider limit points $x \in E \cap E'$.
	For every such $x$, Theorem 7.11 implies
	\[
		f(x) = \lim_{n\to\infty} f_n(x)
		= \lim_{n\to\infty}\underbrace{\lim_{t\to x}f_n(t)}_{f_n \text{ cts}}
		= \lim_{t\to x} \lim_{n \to \infty} f_n(t) = \lim_{t \to x}f(t)
	\]
\end{proof}

We now proceed with the proof of the theorem.
\begin{proof}
	Step 1: Show that $\{y_n\}$ converges (to some $y \in M_2$).
	It sufficies to $\{y_n\}$ is Cauchy.
	Let $\ep > 0$. Choose $N$ such that $\forall n,m \geq2 N, t \in E$,
	$d_2(f_n(t), f_m(t)) < \ep/2$ by the Cauchy Criterion.
	Then $d_2(y_n,y_m) \leq d_2(y_n,f_n(t)) + d_2(f_n(t), y_m)
	\leq d_2(y_n,f_n(t)) + d_2(f_n(t),f_m(t)) + d_2(f_m(t),y_m)$.
	We can choose $t$ such that the above is at most $< \ep/3 + \ep/3 + \ep/3$.
	Now we call this the ``$\ep$ over $3$ trick" (won't show full details in the future).
	Conclusion: $\forall \ep > 0, \exists N$ such that $n,m \geq N$,
	$d_2(y_n,y_m) < \ep$, i.e. $\{y_n\}$ is Cauchy, hence convergent
	($(M_2,d_2)$ complete).

	Step 2: Prove that $f(t) \to y$ as $t \to x$.
	We have $\forall t \in E$ and $n$, $d_2(f(t),y) \leq
	d_2(f(t),f_n(t)) + d_2(f_n(t),y_n) + d_2(y_n,y)$.
	We could first fix $N$ large, and then make $t$ close to $x$,
	or we could do it in the reverse order.
	It will turn out that the former will be more fruitful.
	Let $\ep > 0$. Since $f_n \to f$ uniformly, $\exists N_1$ such that
	$\forall n \geq N_1, t\in E$, $d_2(f(t),f_n(t)) < \ep/3$.
	Since $y_n \to y$, $\exists N_2$ such that $\forall n \geq N_2$,
	$d_2(y_n,y) < \ep/3$.
	Let $n = \max(N_1,N_2)$.
	Applying A1 (with this choice of $n$), we have
	\[
		d_2(f(t),y) < \ep/3 + d_2(f_n(t),y_n) + \ep/3
	\]
	Since $\lim_{t \to x} f_n(t) = y_n$,
	$\exists \delta > 0$ such that $\forall t \in E$, $d_1(t,x) < \delta$,
	we have $d_2(f_n(t),y_n) < \delta$,
	we have $d_2(f(t),y) < \ep$.
\end{proof}

Now, where did we use uniform convergence in this proof?
We used it in Step 1 to prove that our $y_n$ converge (Cauchy criterion).
So what if we add the additional hypothesis that $y_n$ converges,
but we weaken it to pointwise convergence.
Well, we use it by making $N$ large for all $t$,
so Step 2 goes wrong as well.
This proof is not fixable with pointwise convergence.

\subsection{Series of functions}
Series are not as important as sequences of functions,
but it is useful in everyday life.

\begin{definition}
	Let $E$ be a set, let $\{f_n\}$ be a sequence of functions
	$f_n \colon E \to \R$ or $E \to \C$
	and let $g \colon E \to \R$ or $E \to \C$.
	We say that $\sum_{n=1}^\infty f_n$ converges pointwise (resp. uniformly) to $g$ on $E$
	if the sequence $S_n = \sum_{i=1}^n f_i$ converges pointwise (resp. uniformly) to $g$.
\end{definition}
Can't do this in full arbitrary of $f_n$, needs to support addition.
I guess you can do it in some group, but this is abstraction for no good reason.

Example: The series $1 + \sum_{n=1}^\infty \frac{x^n}{n!}$.
Converges to $g(x) = e^x$:
\begin{itemize}
	\item pointwise (on $\R$)
	\item uniformly on any bounded set $E \subset \R$,
		or specifically compact sets $E \subset \R$.
\end{itemize}

\begin{theorem}[Weierstrass M-test (Rudin 7.10)]
	Let $E$ be a set, $E \to \R$ on $E \to \C$.
	If $\lvert f_n(x) \rvert \leq M_n$, $\forall n \geq N_0, x \in E$,
	and if $\sum_{n=1}^\infty M_n < \infty$,
	then $\sum_{n=1}^\infty f_n$ converges uniformly.
\end{theorem}
Proof is an exercise, but useful is Cauchy criterion.
This theorem is almost trivial, but very practical.

\section{February 12}
Housekeeping:
Midterm on Wednesday (bring ID, everything up to and including pointwise convergence).
HW4 is graded, but bonus problem will be graded soon (by him).

Recall last class we talked about $(X,d)$ a metric space,
and $\mathcal{C}(X)$ be the bounded continuous functions $f \colon X \to \C$.
We talked about how $\mathcal{C}(X)$ is a normed vector space that is complete.

What about $\mathcal{C}(X,Y)$,
the bounded continuous functions $f \colon X \to Y$ ($X,Y$ are metric spaces)
(bounded here means $f(X)$ is contained in some $r$-ball in $Y$).
Our metric is $d(f,g) = \sup_{x \in X} d_Y(f(x),g(x))$ (check this is a metric).
Is $\mathcal{C}(X,Y)$ complete?
Yes iff $Y$ is complete (same proof as before;
consider constant function of Cauchy sequence that doesn't converge).

\subsection{Properties of Uniform Convergence}
\begin{theorem}[Rudin 7.16]
	Let $\alpha \colon [a,b] \to \R$ monotone increasing.
	Let $\{f_n\}$ a sequence $f_n \in \mathcal{R}_\alpha[a,b]$.
	Let $f \colon [a,b] \to \R$ and suppose $f_n \to f$ uniformly.
	Then $f \in \mathcal{R}_\alpha[a,b]$ and
	$\lim_{n\to\infty}\int_a^b fd\alpha = \int_a^bfd\alpha$.
\end{theorem}
\begin{proof}
	We first show that $f \in \mathcal{R}_\alpha[a,b]$.
	We will show $\underline{\int_a^b} fd\alpha = \overline{\int_a^b}fd\alpha$.
	Note that we can always assume that these upper and lower integrals exist:
	we just need to show that they are bound, so
	take $\ep = 1$, $\exists N$ such that $|f(x) - f_N(x)| \leq 1$ for all $x$,
	and $|f_N(x)| < K$ since Riemann integrable,
	so $|f(x)| \leq |f(x) - f_N(x)| + |f_N(x)| < K + 1$.

	Now, to show the upper and lower integrals are equal, we could show that
	they difference is less than $\ep$ for all $\ep > 0$.
	Let $\ep > 0$. Since $f_n \to f$ uniformly, there exists $N$ such that
	$\forall n \geq N$, $x \in [a,b]$, $|f(x) - f_n(x)| \leq \ep$.
	Note $f_n(x) - \ep \leq f(x) \leq f_n(x) + \ep$, so
	\[
		\int_a^b (f_n - \ep)d\alpha \leq \underline{\int_a^b} fd\alpha
		\leq \overline{\int_a^b}fd\alpha \leq \int_a^b(f_n + \ep)d\alpha
	\]
	since each $m_i$ is the greatest lower bound,
	it is greater than $f_n-\ep$ on each interval for any partition,
	so $\underline{\int}(f_n - \ep)d\alpha \leq \underline{\int}fd\alpha$
	but the upper and lower converges because its Riemann integrable
	(this is only the first inequality, but the other follows the same).
	Rearranging, we get
	\[
		\overline{\int_a^b}fd\alpha - \underline{\int_a^b} fd\alpha
		\leq \int_a^b(f_n+\ep)d\alpha - \int_a^b(f_n-\ep)d\alpha
		= \int_a^b 2\ep d\alpha = 2\ep(\alpha(b) - \alpha(a))
	\]
	Since $\ep > 0$ is arbitrary, $\underline{\int_a^b} fd\alpha = \overline{\int_a^b}fd\alpha$.

	Now we need to show that the integral agrees with the limit of the integrals of $f_n$
	(which we don't get for free, as we saw with the pointwise case).
	Rearranging our inequality chain, we get
	\[
		\int_a^b f_nd\alpha - \int_a^b \ep d\alpha
		\leq \int_a^b fd\alpha \leq \int_a^b f_nd\alpha + \int_a^b \ep d\alpha
	\]
	And so
	\[
		\left\lvert\int_a^b fd\alpha - \int_a^b f_nd\alpha\right\rvert < \ep(\alpha(b)-\alpha(a))
	\]
\end{proof}
\begin{corollary}
	If $f_n \in \mathcal{R}_\alpha[a,b]$, and $\sum_{n=1}^\infty$
	converges uniformly on $[a,b]$ to $f$, then $f\in \mathcal{R}_\alpha[a,b]$, and
	\[
		\int_a^b fd\alpha = \sum_{n=1}^\infty \int_a^b f_nd\alpha
	\]
\end{corollary}
\begin{proof}
	Let $g_n = \sum_{i=1}^n f_i$ and $g_n \to f$ uniformly.
	Apply Theorem 7.16.
	(Exercise: write this down precisely yourself.)
\end{proof}
You will be moving an infinite sum inside an integral
($\lim_{m\to\infty}\sum_{n=1}^m \int_a^b f_nd\alpha$ and move the sum inside),
which we can't normally do, but the point os Theorem 7.16 is when we can do this.

Our next theorem will take some effort to prove.
Note that we can have a series of differentiable functions
that converge uniformly but is not differentiable.
The functions $f_0 = \sin(x)$, $f_1 = \frac12 \sin(4x)$, $f_n = \frac{1}{2^n} \sin(4^nx)$
are all differentiable,
but  $\sum_{n=1}^\infty f_n$, which uniformly converges
(Weierstrass $M$-test) is not differentiable
(actually not differentiable anywhere, hard to show, Weierstrass function???).
The derivative of $f_n$ is $\frac{4^{n^2}}{2^n}\cos(4^nx))$,
and basically show that the $\cos$ have the conspire and cancel out in such a way
that it just cannot happen.

If we assume a lot, however, we can say some things about differentiability.
\begin{theorem}[Rudin 7.17]
	Lt $\{f_n\}$ be a sequence of functions $f_n \colon [a,b] \to \R$.
	Suppose that
	\begin{enumerate}
		\item each $f_n$ is differentiable on $[a,b]$
		\item $\exists x_0 \in [a,b]$ such that $\{f_n(x_0)\}$ converges
		\item $f_n'$ converge uniformly on $[a,b]$
	\end{enumerate}
	Then $\exists f$ such that $f_n \to f$ uniformly on $[a,b]$,
	and $f'(x)$ exists for all $x\in[a,b]$ and $f'(x) = \lim_{n\to\infty} f'_n(x)$
	(i.e. $f'_n \to f'$ uniformly).
\end{theorem}
We need the last hypothesis, because our example above fails it.
The second is also important, as $f_n = n$ is differentiable and has derivatives,
but it does not converge.

Won't finish proof right now, but we'll get started with an important estimate.
Step 1: $f_n \to f$ uniformly.
Let $\ep > 0$, let $N$ large enough that $\lvert f_n(x_0) - f_n(x_0) \rvert < \ep$
for all $n,m \geq N$ (convergent sequences are Cauchy)
and $\lvert f'_n(x) - f'_m(x)\rvert < \ep$ for all $n,m \geq N$, for all $x \in [a,b]$.
Here is the crucial idea of this proof: apply the Mean Value Theorem
to the difference $(f_n - f_m)$.
For $x,t \in [a,b], x\neq t$, $\exists c$ between $x$ and $t$ such that
\[
	\lvert (f_n(x) - f_m(x)) - (f_n(t) - f_m(t)) \rvert
	= \lvert (f'_n - f'_m)(c)\rvert\lvert x - t\rvert \leq \ep \lvert x -t \rvert
\]
How do we use this inequality?
We have two consequences:
\begin{enumerate}
	\item[(1)] $|((f_n(x) - f_m(x)) - (f_n(t) - f_m(t)) | < \ep |b-a|$
		(useful for uniform convergence)
	\item[(2)] $\frac{|(f_n(x) - f_m(x)) - (f_n(t) - f_m(t))|}{|x-t|} < \ep$
		(useful for differentiability)
\end{enumerate}
(I don't know if this one is right, some power tripping Math 101 TA kicked us out.)


\section{February 16}
Recall what we had last time: in the process of proving theorem 7.17.
\begin{proof}
	What did we do to prove it last time?
	Fixed $\ep > 0$, found $N$ such that $\forall n,m \geq N$,
	\[
		\lvert f_n(x_0) - f_m(x_0) \rvert < \ep
		\quad\text{ and }\quad
		\lvert f'_n(x) - f'_m(x)\rvert < \ep, \forall x
	\]
	We used MVT $x,t \in [a,b]$, $x \neq t$ and got two consequences
	\begin{enumerate}
		\item[(1)] $|((f_n(x) - f_m(x)) - (f_n(t) - f_m(t)) | \leq \ep |b-a|$
			(useful for uniform convergence)
		\item[(2)] $\frac{|(f_n(x) - f_m(x)) - (f_n(t) - f_m(t))|}{|x-t|} \leq \ep$
			(useful for differentiability)
	\end{enumerate}

	We are splitting our step into several proofs.
	We first show that there is some $f$ that $f_n$ converges uniformly to,
	and then prove the statement for the derivative.

	We will try to rearrange (1) to get the Cauchy criterion.
	By (1) for $t = x_0$, for $n,m \geq N$, $x \in [a,b]$, we get
	\[
		\lvert f_n(x) - f_m(x) \rvert
		\leq \lvert (f_n(x) - f_m(x)) - (f_n(x_0) - f_m(x_0))\rvert
		+ \lvert f_n(x_0) - f_m(x_0)) \rvert 
		\leq \ep(b-a) + \ep
	\]
	Since $\ep$ can be arbitrarily small ($\ep'((b-a) + 1) = \ep$),
	$\{f_n\}$ satisfies the Cauchy Criterion for uniform convergence.
	
	Now we prove that $f' = \lim_{n\to\infty} f'_n$.
	Fix $x \in [a,b]$.
	We need to show $f$ is differentiable at $x$ and $f'(x) = \lim_{n\to\infty} f'_n(x)$.
	Since the derivative involves limits,
	this will invovle a careful interchange of limits.
	We write the derivative out explicitly:
	\[
		\phi_n(t) = \frac{f_n(t) - f_n(x)}{t-x}, \quad \phi(t) = \frac{f(t) - f(x)}{t-x}
	\]
	so $f'_n(x) = \lim_{t \to x}\phi_n(t)$, and similarly  for $f'(x)$, if it exists.
	Inequality (2) says for $n,m \geq N$,
	\[
		\lvert \phi_n(t) - \phi_m(t) \rvert \leq \ep
	\]
	i.e. the functions $\{\phi_n\}$ satisfies Cauchy criterion for uniform convergence,
	which implies $\phi_n$ converges uniformly on the domain $[a,b] \setminus\{x\}$
	(but at least $x$ is a limit point; important later).
	This says something about $\phi_n$, but we don't know
	if it converges to $\phi(t)$ at any fixed $t$.
	Showing pointwise is enough.
	Fix $t \in [a,b] \setminus \{x\}$.
	\[
		\phi_n(t) - \phi(t) =
		\left\lvert\frac{(f_n(t) - f_n(x)) - (f(t) - f(x))}{t-x}\right\rvert
		\leq \left\lvert \frac{(f_n(t) - f(t)}{t-x}\right\rvert
		+ \left\lvert \frac{(f_n(x) - f(x)}{t-x}\right\rvert
	\]
	which goes to $0$ as $n \to \infty$.
	Conclusion, $\phi_n \to \phi$ pointwise, hence uniformly on $[a,b] \setminus \{x\}$.

	Theorem 7.11: properties being inherited by uniform limit,
	interchanging limits.
	Since $x$ is a limit point of $[a,b] \setminus \{x\}$,
	and $\phi_n \to \phi$ uniformly on $[a,b] \setminus \{x\}$,
	we apply theorem 7.11 to conclude
	\[
		f'(x) = \lim_{t \to x} \phi(t) =
		\lim_{t \to x} \lim_{n\to\infty} \phi_n(t)
		= \lim_{n\to\infty} \lim_{t \to x} \phi_n(t) = \lim_{n\to\infty} f'_n(x)
	\]
	if the limit exists, and it does.
	And this is a uniform limit???
\end{proof}

\subsection{The Weierstrass function}
\begin{theorem}[Rudin 7.18]
	There exists a continuous function $f \colon \R \to \R$ such that
	$f'(x)$ does not exist for any $x \in \R$.
\end{theorem}
One way to do this is to construct such a function.
Note that if we close our eyes and pick a function at random from $C(\R)$,
it will almost surely be a function that is not differentiable anywhere.
But if we try to write any function down, it is usually differentiable.
``It's like looking for hay in a hay stack, but we never get they hay."
This happens often in math: can prove that almost always functions have a certain property,
but can't even write it out.
\begin{proof}
	Consider the periodization of $|x|$ on $[-1,1]$ to the whole real line,
	call this $\phi(x)$.
	This function is continuous, in fact, it is Lipschitz,
	with Lipschitz constant $1$, or $1$-Lipschitz:
	$\lvert \phi(x) - \phi(y) \rvert \leq 1\lvert x-y \rvert$.
	Note that being Lipschitz is not preserved under pointwise limits,
	but being $k$-Lipschitz, for a fixed $k$, is.

	Let $f(x) = \sum_{n = 0}^\infty \left(\frac{3}{4}\right)^n \phi(4^n x)$.
	This series converges absolutly by the Weierstrass $M$-test.
	Each of these terms are continuous,
	and since absolutely convergent series of continuous functions
	converges to a continuous function, $f$ is continuous.

	For $n$ large, what does $\left(\frac{3}{4}\right)^n \phi(4^n x)$ look like?
	Very small, but very spiky.
	But note that the $4^n$ is getting large faster than
	$\left(\frac{3}{4}\right)^n$ is getting small,
	i.e. if we multiply them, we get $3^n$, which still blows up.
	We get that $\left(\frac{3}{4}\right)^n \phi(4^n x)$ is $3^n$-Lipschitz.
	
	Fix $x \in \R$.
	When to show: $f(x)$ is not differentiable at $x$.
	Suffices to find $\delta_m \searrow 0$ such that
	\[
		\left\lvert \frac{f(x + \delta_m) - f(x)}{\delta_m}\right\rvert \nearrow \infty
	\]
	as $m \to \infty$.
	Trick is finding $\delta_m$ such that
	bigger $m$ cancel out (equal spots on the period),
	and for the smaller values of $m$, the Lipschitz constant is too small
	to make a difference: even if all the other peaks were working against it,
	$3^j - \sum_{n=0}^{j-1} 3^j = \frac{1}{6}3^j$ or something like that.

	Full proof in Rudin, or work through it yourself.
\end{proof}

Will talk about equicontinuity after the break.
Administrative note: two homeworks,
one walks you through the proof of DCT but not to be handed in,
the other is the actual homework.
\end{document}
